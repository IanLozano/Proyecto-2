{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bBhNPkOI_s_p",
    "outputId": "d54fdd4a-deca-4575-e571-0515c26ae4a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.3212\n",
      "Epoch 2 completado. Loss promedio: 0.2285\n",
      "Epoch 3 completado. Loss promedio: 0.1862\n",
      "📈 MCAUC: 0.8778287892393899\n",
      "✅ Archivo 'pred_genres_text_BERT_torch.csv' generado correctamente.\n"
     ]
    }
   ],
   "source": [
    "# ✅ Instalar en Colab (si es necesario)\n",
    "# !pip install transformers scikit-learn torch pandas --quiet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1. Cargar datos\n",
    "df = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', index_col=0)\n",
    "df['genres'] = df['genres'].apply(ast.literal_eval)\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['genres'])\n",
    "\n",
    "# 2. Tokenizador\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 3. Dataset personalizado\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 4. División train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['plot'], y, test_size=0.33, random_state=42)\n",
    "train_dataset = MovieDataset(X_train, y_train)\n",
    "test_dataset = MovieDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# 5. Modelo BERT Multilabel\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.pooler_output\n",
    "        x = self.dropout(cls_output)\n",
    "        return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BERTClassifier(num_labels=y.shape[1]).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 6. Entrenamiento\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# 7. Evaluación con AUC macro\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].cpu().numpy()\n",
    "        outputs = model(input_ids, attention_mask).cpu().numpy()\n",
    "        all_preds.append(outputs)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "y_pred = np.vstack(all_preds)\n",
    "y_true = np.vstack(all_labels)\n",
    "print(\"📈 MCAUC:\", roc_auc_score(y_true, y_pred, average='macro'))\n",
    "\n",
    "# 8. Predicción sobre test real y exportación\n",
    "dataTesting = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTesting.zip', index_col=0)\n",
    "test_encodings = tokenizer(list(dataTesting['plot']), truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
    "test_dataset = DataLoader(torch.utils.data.TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask']), batch_size=32)\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for input_ids_batch, attn_mask_batch in test_dataset:\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        attn_mask_batch = attn_mask_batch.to(device)\n",
    "        pred_batch = model(input_ids_batch, attn_mask_batch).cpu().numpy()\n",
    "        preds.append(pred_batch)\n",
    "\n",
    "y_pred_test_final = np.vstack(preds)\n",
    "cols = ['p_' + genre for genre in mlb.classes_]\n",
    "res = pd.DataFrame(y_pred_test_final, index=dataTesting.index, columns=cols)\n",
    "res.to_csv('pred_genres_text_BERT_torch.csv', index_label='ID')\n",
    "print(\"✅ Archivo 'pred_genres_text_BERT_torch.csv' generado correctamente.\")\n",
    "# 0.87906"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BKn8te_GNh-"
   },
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WEQtNWnrAVXM",
    "outputId": "803a595f-ca23-4206-c552-01b9688bc325"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.3349\n",
      "Epoch 2 completado. Loss promedio: 0.2336\n",
      "Epoch 3 completado. Loss promedio: 0.1913\n",
      "Epoch 4 completado. Loss promedio: 0.1609\n",
      "Epoch 5 completado. Loss promedio: 0.1360\n",
      "Epoch 6 completado. Loss promedio: 0.1152\n",
      "Epoch 7 completado. Loss promedio: 0.0982\n",
      "Epoch 8 completado. Loss promedio: 0.0832\n",
      "Epoch 9 completado. Loss promedio: 0.0719\n",
      "Epoch 10 completado. Loss promedio: 0.0622\n",
      "📈 MCAUC: 0.8993715939939145\n",
      "✅ Archivo 'pred_genres_text_BERT_torch.csv' generado correctamente.\n"
     ]
    }
   ],
   "source": [
    "# ✅ Instalar en Colab (si es necesario)\n",
    "# !pip install transformers scikit-learn torch pandas --quiet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1. Cargar datos\n",
    "df = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', index_col=0)\n",
    "df['genres'] = df['genres'].apply(ast.literal_eval)\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['genres'])\n",
    "\n",
    "# 2. Tokenizador\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 3. Dataset personalizado\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 4. División train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['plot'], y, test_size=0.33, random_state=42)\n",
    "train_dataset = MovieDataset(X_train, y_train)\n",
    "test_dataset = MovieDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# 5. Modelo BERT Multilabel\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.pooler_output\n",
    "        x = self.dropout(cls_output)\n",
    "        return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BERTClassifier(num_labels=y.shape[1]).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 6. Entrenamiento\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# 7. Evaluación con AUC macro\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].cpu().numpy()\n",
    "        outputs = model(input_ids, attention_mask).cpu().numpy()\n",
    "        all_preds.append(outputs)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "y_pred = np.vstack(all_preds)\n",
    "y_true = np.vstack(all_labels)\n",
    "print(\"📈 MCAUC:\", roc_auc_score(y_true, y_pred, average='macro'))\n",
    "\n",
    "# 8. Predicción sobre test real y exportación\n",
    "dataTesting = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTesting.zip', index_col=0)\n",
    "test_encodings = tokenizer(list(dataTesting['plot']), truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
    "test_dataset = DataLoader(torch.utils.data.TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask']), batch_size=32)\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for input_ids_batch, attn_mask_batch in test_dataset:\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        attn_mask_batch = attn_mask_batch.to(device)\n",
    "        pred_batch = model(input_ids_batch, attn_mask_batch).cpu().numpy()\n",
    "        preds.append(pred_batch)\n",
    "\n",
    "y_pred_test_final = np.vstack(preds)\n",
    "cols = ['p_' + genre for genre in mlb.classes_]\n",
    "res = pd.DataFrame(y_pred_test_final, index=dataTesting.index, columns=cols)\n",
    "res.to_csv('pred_genres_text_BERT_torch.csv', index_label='ID')\n",
    "print(\"✅ Archivo 'pred_genres_text_BERT_torch.csv' generado correctamente.\")\n",
    "# 0.89671"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NKOgv8LLItqs",
    "outputId": "4c0e4bee-f40a-46e7-9e54-85e1118bc528"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.3218\n",
      "Epoch 2 completado. Loss promedio: 0.2250\n",
      "Epoch 3 completado. Loss promedio: 0.1842\n",
      "Epoch 4 completado. Loss promedio: 0.1542\n",
      "Epoch 5 completado. Loss promedio: 0.1300\n",
      "Epoch 6 completado. Loss promedio: 0.1102\n",
      "Epoch 7 completado. Loss promedio: 0.0940\n",
      "Epoch 8 completado. Loss promedio: 0.0809\n",
      "Epoch 9 completado. Loss promedio: 0.0698\n",
      "Epoch 10 completado. Loss promedio: 0.0603\n",
      "Epoch 11 completado. Loss promedio: 0.0521\n",
      "Epoch 12 completado. Loss promedio: 0.0452\n",
      "Epoch 13 completado. Loss promedio: 0.0391\n",
      "Epoch 14 completado. Loss promedio: 0.0347\n",
      "Epoch 15 completado. Loss promedio: 0.0294\n",
      "Epoch 16 completado. Loss promedio: 0.0258\n",
      "Epoch 17 completado. Loss promedio: 0.0226\n",
      "Epoch 18 completado. Loss promedio: 0.0198\n",
      "Epoch 19 completado. Loss promedio: 0.0178\n",
      "Epoch 20 completado. Loss promedio: 0.0155\n",
      "📈 MCAUC: 0.8918559901795774\n",
      "✅ Archivo 'pred_genres_text_BERT_torch.csv' generado correctamente.\n"
     ]
    }
   ],
   "source": [
    "# ✅ Instalar en Colab (si es necesario)\n",
    "# !pip install transformers scikit-learn torch pandas --quiet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1. Cargar datos\n",
    "df = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', index_col=0)\n",
    "df['genres'] = df['genres'].apply(ast.literal_eval)\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['genres'])\n",
    "\n",
    "# 2. Tokenizador\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 3. Dataset personalizado\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 4. División train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['plot'], y, test_size=0.33, random_state=42)\n",
    "train_dataset = MovieDataset(X_train, y_train)\n",
    "test_dataset = MovieDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# 5. Modelo BERT Multilabel\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.pooler_output\n",
    "        x = self.dropout(cls_output)\n",
    "        return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BERTClassifier(num_labels=y.shape[1]).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 6. Entrenamiento\n",
    "model.train()\n",
    "for epoch in range(20):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# 7. Evaluación con AUC macro\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].cpu().numpy()\n",
    "        outputs = model(input_ids, attention_mask).cpu().numpy()\n",
    "        all_preds.append(outputs)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "y_pred = np.vstack(all_preds)\n",
    "y_true = np.vstack(all_labels)\n",
    "print(\"📈 MCAUC:\", roc_auc_score(y_true, y_pred, average='macro'))\n",
    "\n",
    "# 8. Predicción sobre test real y exportación\n",
    "dataTesting = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTesting.zip', index_col=0)\n",
    "test_encodings = tokenizer(list(dataTesting['plot']), truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
    "test_dataset = DataLoader(torch.utils.data.TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask']), batch_size=32)\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for input_ids_batch, attn_mask_batch in test_dataset:\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        attn_mask_batch = attn_mask_batch.to(device)\n",
    "        pred_batch = model(input_ids_batch, attn_mask_batch).cpu().numpy()\n",
    "        preds.append(pred_batch)\n",
    "\n",
    "y_pred_test_final = np.vstack(preds)\n",
    "cols = ['p_' + genre for genre in mlb.classes_]\n",
    "res = pd.DataFrame(y_pred_test_final, index=dataTesting.index, columns=cols)\n",
    "res.to_csv('pred_genres_text_BERT_torch.csv', index_label='ID')\n",
    "print(\"✅ Archivo 'pred_genres_text_BERT_torch.csv' generado correctamente.\")\n",
    "# 0.89639"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oTpRIWevSblz",
    "outputId": "de8a5f9f-9335-4f7d-ae37-4a24adc615ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m126.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m107.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hEpoch 1 completado. Loss promedio: 0.3195\n",
      "Epoch 2 completado. Loss promedio: 0.2207\n",
      "Epoch 3 completado. Loss promedio: 0.1785\n",
      "📈 MCAUC: 0.883273000679806\n",
      "✅ Archivo generado: pred_genres_text_BERT_concat.csv\n"
     ]
    }
   ],
   "source": [
    "# 🛠️ Instala las librerías necesarias (si aún no están)\n",
    "!pip install transformers scikit-learn torch pandas --quiet\n",
    "\n",
    "# 📚 Importar librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1. Cargar y preparar datos\n",
    "df = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', index_col=0)\n",
    "df['genres'] = df['genres'].apply(ast.literal_eval)\n",
    "df[\"input_text\"] = df[\"title\"] + \" (\" + df[\"year\"].astype(str) + \"): \" + df[\"plot\"]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['genres'])\n",
    "\n",
    "# 2. Tokenizador y Dataset\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 3. División train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['input_text'], y, test_size=0.33, random_state=42)\n",
    "train_dataset = MovieDataset(X_train, y_train)\n",
    "test_dataset = MovieDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# 4. Modelo BERT Multilabel\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.pooler_output\n",
    "        x = self.dropout(cls_output)\n",
    "        return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BERTClassifier(num_labels=y.shape[1]).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 5. Entrenamiento\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# 6. Evaluación\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].cpu().numpy()\n",
    "        outputs = model(input_ids, attention_mask).cpu().numpy()\n",
    "        all_preds.append(outputs)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "y_pred = np.vstack(all_preds)\n",
    "y_true = np.vstack(all_labels)\n",
    "print(\"📈 MCAUC:\", roc_auc_score(y_true, y_pred, average='macro'))\n",
    "\n",
    "# 7. Predicción final y exportación\n",
    "dataTesting = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTesting.zip', index_col=0)\n",
    "dataTesting[\"input_text\"] = dataTesting[\"title\"] + \" (\" + dataTesting[\"year\"].astype(str) + \"): \" + dataTesting[\"plot\"]\n",
    "test_enc = tokenizer(list(dataTesting[\"input_text\"]), truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
    "test_dataset = DataLoader(torch.utils.data.TensorDataset(test_enc['input_ids'], test_enc['attention_mask']), batch_size=32)\n",
    "\n",
    "preds = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for input_ids_batch, attn_mask_batch in test_dataset:\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        attn_mask_batch = attn_mask_batch.to(device)\n",
    "        pred_batch = model(input_ids_batch, attn_mask_batch).cpu().numpy()\n",
    "        preds.append(pred_batch)\n",
    "\n",
    "y_pred_test_final = np.vstack(preds)\n",
    "cols = ['p_' + genre for genre in mlb.classes_]\n",
    "res = pd.DataFrame(y_pred_test_final, index=dataTesting.index, columns=cols)\n",
    "res.to_csv('pred_genres_text_BERT_concat.csv', index_label='ID')\n",
    "print(\"✅ Archivo generado: pred_genres_text_BERT_concat.csv\")\n",
    "# 0.89671"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X-tGlHy7lywn",
    "outputId": "ba2ff783-2e98-4af8-abb6-d9047eb3faaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.3219\n",
      "Epoch 2 completado. Loss promedio: 0.2221\n",
      "Epoch 3 completado. Loss promedio: 0.1800\n",
      "Epoch 4 completado. Loss promedio: 0.1488\n",
      "Epoch 5 completado. Loss promedio: 0.1245\n",
      "Epoch 6 completado. Loss promedio: 0.1050\n",
      "Epoch 7 completado. Loss promedio: 0.0897\n",
      "Epoch 8 completado. Loss promedio: 0.0758\n",
      "Epoch 9 completado. Loss promedio: 0.0650\n",
      "Epoch 10 completado. Loss promedio: 0.0550\n",
      "📈 MCAUC: 0.8997952115373078\n",
      "✅ Archivo generado: pred_genres_text_BERT_concat.csv\n"
     ]
    }
   ],
   "source": [
    "# 🛠️ Instala las librerías necesarias (si aún no están)\n",
    "!pip install transformers scikit-learn torch pandas --quiet\n",
    "\n",
    "# 📚 Importar librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1. Cargar y preparar datos\n",
    "df = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', index_col=0)\n",
    "df['genres'] = df['genres'].apply(ast.literal_eval)\n",
    "df[\"input_text\"] = df[\"title\"] + \" (\" + df[\"year\"].astype(str) + \"): \" + df[\"plot\"]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['genres'])\n",
    "\n",
    "# 2. Tokenizador y Dataset\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 3. División train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['input_text'], y, test_size=0.33, random_state=42)\n",
    "train_dataset = MovieDataset(X_train, y_train)\n",
    "test_dataset = MovieDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# 4. Modelo BERT Multilabel\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.pooler_output\n",
    "        x = self.dropout(cls_output)\n",
    "        return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BERTClassifier(num_labels=y.shape[1]).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 5. Entrenamiento\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# 6. Evaluación\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].cpu().numpy()\n",
    "        outputs = model(input_ids, attention_mask).cpu().numpy()\n",
    "        all_preds.append(outputs)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "y_pred = np.vstack(all_preds)\n",
    "y_true = np.vstack(all_labels)\n",
    "print(\"📈 MCAUC:\", roc_auc_score(y_true, y_pred, average='macro'))\n",
    "\n",
    "# 7. Predicción final y exportación\n",
    "dataTesting = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTesting.zip', index_col=0)\n",
    "dataTesting[\"input_text\"] = dataTesting[\"title\"] + \" (\" + dataTesting[\"year\"].astype(str) + \"): \" + dataTesting[\"plot\"]\n",
    "test_enc = tokenizer(list(dataTesting[\"input_text\"]), truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
    "test_dataset = DataLoader(torch.utils.data.TensorDataset(test_enc['input_ids'], test_enc['attention_mask']), batch_size=32)\n",
    "\n",
    "preds = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for input_ids_batch, attn_mask_batch in test_dataset:\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        attn_mask_batch = attn_mask_batch.to(device)\n",
    "        pred_batch = model(input_ids_batch, attn_mask_batch).cpu().numpy()\n",
    "        preds.append(pred_batch)\n",
    "\n",
    "y_pred_test_final = np.vstack(preds)\n",
    "cols = ['p_' + genre for genre in mlb.classes_]\n",
    "res = pd.DataFrame(y_pred_test_final, index=dataTesting.index, columns=cols)\n",
    "res.to_csv('pred_genres_text_BERT_concat.csv', index_label='ID')\n",
    "print(\"✅ Archivo generado: pred_genres_text_BERT_concat.csv\")\n",
    "# 0.90159"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 750,
     "referenced_widgets": [
      "aad863907e03477fa5c2c3894cd5c734",
      "673087abd49a415fa3e0ad5be65278c3",
      "fd27c8846ce148f396c1c7fe1b1d91a5",
      "ceaa8469219b43b68b88b60eb4f6726b",
      "3057a8a7ef544b40b7dbdcae700c96ac",
      "25fe50c1f8fc4a1daef15057b53dec45",
      "dfb9dc5593024f568d27b5ecd8aef0a3",
      "c96e6557644a456089d6d27e4b6cc739",
      "c8735994a5f84f20aa7f3a72ed26a516",
      "1ade1fa1760341639690506302c7843d",
      "5090ac8acc4744e19396326611fb5634",
      "503c15a09ca747b9908b09205e6121df",
      "f88360f2a3484766bbce7a660370b3df",
      "dc13f90b9fab4f9c9c679a49621ef426",
      "fe47e2364e084493ae21702b080e6c1f",
      "45faae9f0427494b8e87c8422f0296c1",
      "c2566541c49b493dbdc5277b09db6d92",
      "167ea55cbacc4bd995eaa7c2fe56c768",
      "29c8dd1942fd46a488f321c0c55ce723",
      "9c40787c1602476580316c19e9e561cb",
      "925604491e7b4de5a2ac820d46533e6f",
      "05836015313e4a17a0de1ade0169a00f",
      "5440eb0e62014e30b42c2affb896338b",
      "c96703e9f485422cbfede22bd1e7105f",
      "1dbc6cac08cc4a89a283203a9fa10698",
      "44fedbbf87c444d196aa3c87a465430b",
      "29be059359e945428d0e8bc06adb480c",
      "e076140030634365b1b968d0d8dab55f",
      "30ef3d4da1e74b1f83d66b741e6766ae",
      "717edd8ae9b147f2954c3d5a83db9b25",
      "d55e44e904704f869fce336587e7f8c8",
      "eb27bb5b001a4cd1a42676f72f27f20d",
      "2a7ce8ef97f84063a4216c098b64de30",
      "b28c9387d2124dcea14829fca4d5e375",
      "a1db305edcd64adcb069b69387e09069",
      "bd2d8ff944f147ef998fc6f5faaafada",
      "0e6b5b561fe64951ada31015e86ace47",
      "31eb037f983b49a08fe74d34b563a074",
      "515f7942b4654f12b95ae6d2f3dc8ad3",
      "1ec2a41f8ec84545b0923df3ef10d902",
      "e808af6cfd8b4e598f340b5b92997041",
      "655997d6d9db4b71a44d31768c1729a9",
      "c2c06e916e604002a8b90e847f9baee0",
      "dab4eff0ba114ec188604a27a73e2867",
      "66f4f13f4f4f4cad8ee2d8cee957c34b",
      "9a834e70faf24beb95e2c7fc036d8ce0",
      "f8eede2d568f4e679295c55b439e6323",
      "9bfa67fe6bef4c7493c36986349b361a",
      "748c88ca7a4a4bff9ab49147ffed08af",
      "a9d2a3d1438d41c286edad0cfca2e3d5",
      "794d4c0c95894a4d96de79d12b6b3834",
      "b09bc5461e594cccaa522fb6b87bf305",
      "7843a8c3b8cd47abb75fd877a8d6632f",
      "7bc230439c124a458e9ee6d8e81e6955",
      "3746ef2bcba7483a9a3f2642d528872c"
     ]
    },
    "id": "8SCPDJyxpNMV",
    "outputId": "fd204606-d3b4-4f36-f496-934950bf36cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aad863907e03477fa5c2c3894cd5c734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "503c15a09ca747b9908b09205e6121df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5440eb0e62014e30b42c2affb896338b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28c9387d2124dcea14829fca4d5e375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66f4f13f4f4f4cad8ee2d8cee957c34b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.3018\n",
      "Epoch 2 completado. Loss promedio: 0.2033\n",
      "Epoch 3 completado. Loss promedio: 0.1634\n",
      "Epoch 4 completado. Loss promedio: 0.1336\n",
      "Epoch 5 completado. Loss promedio: 0.1113\n",
      "Epoch 6 completado. Loss promedio: 0.0933\n",
      "Epoch 7 completado. Loss promedio: 0.0774\n",
      "Epoch 8 completado. Loss promedio: 0.0654\n",
      "Epoch 9 completado. Loss promedio: 0.0550\n",
      "Epoch 10 completado. Loss promedio: 0.0462\n",
      "Epoch 11 completado. Loss promedio: 0.0390\n",
      "Epoch 12 completado. Loss promedio: 0.0329\n",
      "Epoch 13 completado. Loss promedio: 0.0284\n",
      "Epoch 14 completado. Loss promedio: 0.0239\n",
      "Epoch 15 completado. Loss promedio: 0.0203\n",
      "📈 MCAUC (referencia, mismo set de entrenamiento): 0.9996660574926551\n",
      "✅ Archivo generado para Kaggle: pred_genres_text_BERT_full.csv\n"
     ]
    }
   ],
   "source": [
    "# 🛠️ Instalar librerías necesarias\n",
    "!pip install transformers scikit-learn torch pandas --quiet\n",
    "\n",
    "# 📚 Importar librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1. Cargar y preparar datos\n",
    "df = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', index_col=0)\n",
    "df['genres'] = df['genres'].apply(ast.literal_eval)\n",
    "df[\"input_text\"] = df[\"title\"] + \" (\" + df[\"year\"].astype(str) + \"): \" + df[\"plot\"]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['genres'])\n",
    "\n",
    "# 2. Tokenizador y Dataset\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 3. Usar todo el dataset para entrenamiento y validación\n",
    "X_all, y_all = df['input_text'], y\n",
    "full_dataset = MovieDataset(X_all, y_all)\n",
    "full_loader = DataLoader(full_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# 4. Modelo BERT Multilabel\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.pooler_output\n",
    "        x = self.dropout(cls_output)\n",
    "        return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BERTClassifier(num_labels=y.shape[1]).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 5. Entrenamiento con todo el dataset\n",
    "model.train()\n",
    "for epoch in range(15):\n",
    "    total_loss = 0\n",
    "    for batch in full_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(full_loader):.4f}\")\n",
    "\n",
    "# 6. Evaluación sobre los mismos datos (solo referencia interna)\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in full_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].cpu().numpy()\n",
    "        outputs = model(input_ids, attention_mask).cpu().numpy()\n",
    "        all_preds.append(outputs)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "y_pred = np.vstack(all_preds)\n",
    "y_true = np.vstack(all_labels)\n",
    "print(\"📈 MCAUC (referencia, mismo set de entrenamiento):\", roc_auc_score(y_true, y_pred, average='macro'))\n",
    "\n",
    "# 7. Predicción final para Kaggle y exportación\n",
    "dataTesting = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTesting.zip', index_col=0)\n",
    "dataTesting[\"input_text\"] = dataTesting[\"title\"] + \" (\" + dataTesting[\"year\"].astype(str) + \"): \" + dataTesting[\"plot\"]\n",
    "test_enc = tokenizer(list(dataTesting[\"input_text\"]), truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
    "test_dataset = DataLoader(torch.utils.data.TensorDataset(test_enc['input_ids'], test_enc['attention_mask']), batch_size=32)\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for input_ids_batch, attn_mask_batch in test_dataset:\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        attn_mask_batch = attn_mask_batch.to(device)\n",
    "        pred_batch = model(input_ids_batch, attn_mask_batch).cpu().numpy()\n",
    "        preds.append(pred_batch)\n",
    "\n",
    "y_pred_test_final = np.vstack(preds)\n",
    "cols = ['p_' + genre for genre in mlb.classes_]\n",
    "res = pd.DataFrame(y_pred_test_final, index=dataTesting.index, columns=cols)\n",
    "res.to_csv('pred_genres_text_BERT_full.csv', index_label='ID')\n",
    "print(\"✅ Archivo generado para Kaggle: pred_genres_text_BERT_full.csv\")\n",
    "# 0.91335\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "90d3183eda22422b913168d891e0bfa3",
      "2201360c5f054bc99f462861acc47460",
      "273024bf1f8a4bd48b4616f48489b07d",
      "ee6c4d8d98c24607aee2a8a80c4de140",
      "6462430ee77e49f0917ea99ebba91d7f",
      "b201184b4cce494db6583168241db471",
      "c0a59f31fd644a3d95577cded9e1b57c",
      "f2412ace3cd8425d8b5317337ed195ee",
      "90182036dd294aa9a1fdc4aacc6f7ab6",
      "108e938d4e6b40888828d4e048682030",
      "09010e6628ba41a8904c5510ea218b56",
      "6e04c16e2e834d5da22fc2a6029bce08",
      "0885b6bb9da9428486841ea913924371",
      "6b6dd6f009254a2eb4c6d8663cb792e6",
      "565f33d3301b438f80453d2dcabb43b6",
      "51496665db0c4213af1ba266d8bc399f",
      "af87f7f5da6245c099d4f3801b1c938f",
      "3dcc4e1af6994611b8dd780cd0893739",
      "3e731fb831bc4885a75ce34a2f43c58c",
      "71496f162dc9423f95d9a48a4b14bfa2",
      "9a44d0ec20ce43808e2e3bc4bbe5b4e0",
      "0462f3fa33054d62b906b9a5f6888da6",
      "f8d9f15bedb64c83b869fb7fee93f973",
      "9bf95c35455b473e94fef094eef1f20e",
      "3232fdcb2fd043e0bc381e234c967a2a",
      "d599cb3f3b3342719696e8dd731589ab",
      "256f203eea3b44519e05269042b2267a",
      "290c02d326014f60a0a8a2a9db79810c",
      "e8ee80e1e1e842c99623e3268cff39ed",
      "4f06dcfc72c448818f7d995a6d9d9410",
      "2e29243facd341269f678a11175314a4",
      "494bfd63e747450da087e57d62ebb063",
      "7c9555f19532402a9d4f36df1dea0c85",
      "7f60ab40305d4c30b5108ce1841675fd",
      "b9c4ce2f45a44e9595f1e4b71d8699e1",
      "b9cd61f9d39143848393db19244f7fb6",
      "f588e0e5bf564a61acfd5d29fc685d89",
      "3ef32dd7b4ab46e99ab8ef10171ac948",
      "15e3c9772d5c45a9ae60ba77202bc296",
      "fc4a7188dbbd4f2f97c0bac5a1e991ed",
      "0057c34b10eb46908bfc6d82eeeefb71",
      "08a81849ae7f422084da87aa83d0996e",
      "2e53172483be445eb835d9b26e27e975",
      "74637213095c40ee801d1f5c70a5be81",
      "9521041bc02d42809d3ceb25ba1b6cec",
      "1d14b93427464bfdaa22f8752e169594",
      "22e5a55e016b4417ba0856d2f474a5aa",
      "fa97297323a74b50a34d21da927f2664",
      "77ed52692c574f43933e2ac7b22364e9",
      "ee5cdb9d9a15417984ec672773d47871",
      "806ff584546d4d83bc587aee09edb0bd",
      "769c155e05d64177b6ad01df3f2dfa82",
      "6f69a67939504f34b1492324ef99879b",
      "d6365947454147e49cae1ea28c192f61",
      "db28e70fe45d4488bd3dadeec4b5ad6e",
      "f66dbac5564d459a9f8dc0159cc497a0",
      "5a2f66fe0b00471185a284788c0ee724",
      "d3a5a5515c954db38eb844628425e1fc",
      "38dc21dc8dbf4fe9a8830fa55b96992e",
      "24bba917344544b7a1cb099ba53e314c",
      "cc32bdb842ea4769ad12c5bed296982e",
      "8e8a17a03b3241a8ae3d0450ff8d2c3e",
      "79a24b5985da4dca99e8e8ab8e00c950",
      "1cd223147c8642ac821bf4d60b3279d1",
      "236853a538054c649c1095caefa50712",
      "5843c6ebbf9347e893368c88cd7a006d",
      "1a6857a0df864f56ac26e413aa1525bd",
      "8697ff609d544fc6acbc1c0d541952d0",
      "161d15ebe7b1445e9062d3766a2d1265",
      "1a6bd6e1be05483dbe443f79ffb25c3f",
      "3902799c4a1c4913be9c814905f9aa03",
      "1ecb8529054d4a759abed187d509a826",
      "3e7dd9685620426d9e8edf05aec92662",
      "c4ea2f08928146ca8f13504639594f33",
      "00983c873825470e91c77c83a176e4a1",
      "795766542e3541dcb10f54453af4941e",
      "56cbd0f64ea4450a95a2b36314a23806",
      "381e480fe5ff4bdc982727d5902b929f",
      "818e6cc6c424437cbb56e645a68fede0",
      "5dbf14cc8bb54c3e945ec51484ee94ce",
      "2eb00f1b0471408ebc4a96d9534d9abe",
      "d774d4f3428044ec801ea6ad76f8b13f",
      "52eb012128164e81aa710dfa208d8efb",
      "b96d895b3f4a42f496529b5e3193642e",
      "3ad0dbe3334f4f22bec2d08c19ef1e0b",
      "568ee08d5047479eb8a3a691e2c3dd35",
      "6465be7b46824d84a8bf024c0057bb32",
      "a7dd6353a3cc41ec942fe20acb1791b6",
      "df74004c5fc646498965d1edb4c12e12",
      "143f9f7f8e744bbc802308e4256f9463",
      "5c180bb78ead45178471eddaccb3667d",
      "e11bc886bb4640daad02bc1ac5dbe65c",
      "aaa672f79bf1464ca336dbdf7202420a",
      "bcae526174814241b7b3f295331b258e",
      "c0bf70c2e02e460da9561c75747ce345",
      "db39437188de4bb8bbc95c87dc51c479",
      "e16e4eab33a74b01b9eb385b40c2f833",
      "44f6dcdc4ee341818fa50deb8c5e3576",
      "5c4a59e9d317466386792856d04b0216",
      "7b67e3d5d05c4a2ba0481b72207df970",
      "0fe90a527caa4e3f9c74c9156f0d2e92",
      "ae6afb2a95304862b6dbee1b4bda915e",
      "5c856d78ae0d476e8615c39a11c7d8c5",
      "806a0baa28fe4ed6a88d96712ca9af48",
      "989242a510244d45966932a75a7fe66f",
      "98367d9ea65c4151aa2b4329ea1ca12b",
      "63bad2ab46f34ab29818f49bd444708a",
      "672df5c5bfc14e119ef867dadb8caa8a",
      "a39fbc1b21b2456ca8c7579c02457d5f",
      "a493eb72dc3140d3872a2cddad11637f",
      "f19c143cbaa74b80b924155520565baf",
      "99cff5f3ff694c0896e88ae0ac1f0f52",
      "60c09cfabbfb43ceafbf5d3a8fb7e4df",
      "247415c4744e49ae95454f3132a41ac1",
      "1059cb0ca90d4f5a83b17a65fcfc0bf3",
      "dbd8d3c33b194121acfaf6671024da9f",
      "0a6c4893fde045408a605384e91b8d0f",
      "b3585f76a16c43e5ae9fe8a61dec0715",
      "4ad24b4345c841bfbec8df2c9a5a4489",
      "5293ff69b5294f8cae5a21a0aaef8161",
      "8ee9da5963e7466c9bc81dcddb07f5b2",
      "eda78d931755423badbceef9129d4656",
      "f1ee76a3d3014e688febd04b5f73eb91",
      "a13911e33cd34040b256a2454704ec61",
      "9522db2c0b134afcab2dfd716adb465a",
      "dcd49591f54b436c88b50a3dd0a370eb",
      "7f955d715f6c411d82d6c44557ee2ea7",
      "57eda1512e8442c1b545efb765ef28a6",
      "211cc7e325bd4e318f213bcc851eedbd",
      "b55aa3f26f124dc88b9b4f2a1c02c7e9",
      "b0469d4b90e2467ab97d77254d8c8afa",
      "df03392583504080bdd22a632152e683",
      "6fa3e00e2aef4af4a8108a6b0c26fbff",
      "85ad8787d584442486c03db834915c46",
      "4a31a195a47e455b9c70eef94a8710b5",
      "239d0a0d39e7457aa630e436957f7482",
      "a22b27fba41d40d387e7e346c2e409a0",
      "29492866c77244158d0460d91fda9800",
      "6a99c10dc97341d38fce8e02a736a486",
      "d6a7805e483c45fd898c97e81f2b2138",
      "e4ff6e5500e0431fbdc9cf7de6b62c53",
      "0494d0d3a05e44509be7219ea354931a",
      "6a6c600a5314406cb59e3b897b880d07",
      "7a489fd68f4645d9bfc520e818847d17",
      "0d8641b0a2ac427dac458f4180bdfb1f",
      "0cd03a927e2c4bd4a74a36d4ddf99b3c",
      "3b9d36355854444a9578639cdc7adfb8",
      "8da8ab0c4af94299ae5ec934237e519b",
      "341ef0f0dc7d4e42a1255eb51df9a97b",
      "dd9d2fb960e4480fb50129aa474b68a7",
      "412c7b0c63e24c169770467c4f4144df",
      "50abba1b635446b4b745477b9c2ebec6",
      "ad9516e61c194d83907577efeb5c2523",
      "1372d04e741b4038af4faaed8943fb2f",
      "d71f319940ee4675a76698bf46985f3c",
      "a8f8a94200034e39b1145bd79160c0c3",
      "676c312a930a4547a603d978371f52a1",
      "fd7f28b2d16d4d578381734cbf9cc2f7",
      "be8a9cb6863f4137b89c859acf6d8677",
      "169b0336da88439096afb81205cc552a",
      "990945ed8ab548e9872335c390e1676a",
      "89aa14e65cb4490fb0b3d4c5746c3f6a",
      "442fe7f4380d4abdb97f3ecfe1b417ee",
      "85f764c999ff48eb932706c16cc8387a",
      "a7562e50e22149b49f1e9e1aad2d518e",
      "0e8b568461b14a7597d7eda9e65e6da2",
      "d187958a7c0c41c8acbfd157931fcf46",
      "5aa5cde4836e49319203bda21791fa21",
      "21d994ac08eb4e3c9d28a1f4b571675f",
      "a2bba43f323447c193b2993a71485e83",
      "2b22a0f5eadf421c847023c3b3509ae8",
      "bedb27d73ac7481783ecf6d41a1cfd5f",
      "8968f10e7c3447fda5dbcacc6e039e60",
      "9f820e2179024245914552e94e554955",
      "414c39d8330b469ea001fa25026bed3c",
      "af6c3ab3df854b39a551bbcad569c37a",
      "4d650bfbb39d4787bde40a76d8af1d22",
      "02b7c329b3be47edaaddafd2b6136c7d",
      "58c59ab84e7340999382a18b4631c3e0",
      "ff97614fc04646e481345b1438ce20b7",
      "0351d57fe48f4788ad8fd620f061d18f",
      "35a6679c310441ed89f78a5d435f2cf9",
      "22ae259b77b048dc85020acb8a6adfbd",
      "6c83650be8c640e4a4e377836fa57b59",
      "83611cb430b24ab68be6823406eba068",
      "7f350e3688f64759aaaa1345925b09c3",
      "7dac3a50d5044fdf90056df6115a0a72",
      "37cde47a57d64f27a83225ffd833f410",
      "6f92bf348fcb42f2aa205cd1e0d98cba",
      "fe8e356058d7405f95fed66a524ee337",
      "1ab8283784fd465f92d91212fe4df1af",
      "b62ab95b7e4b406b8b8fb5a7e47cee0a",
      "b645125e6c114e7d8144ab63d9b15ea8",
      "349cb3b05be34de4b6975138614f79eb",
      "4555b642de114e7082c89baf18162ba7",
      "e1b37d322ab8433a94e44360d7548e7e",
      "101efdf905a145478e14f286d00bc041",
      "08943435999d43dabc6dea068b68a846",
      "471907cd70bf4cc1a86c3a8fa21b3f9c",
      "c8dfcfd1508841de989362646fef0899",
      "7ea5fbb50f8b407abbadd3061cc49971",
      "2536d7f149fe4752ab1155e8dfce13a0",
      "eb83ed2b087846e09a215dc842ce49ec",
      "2ec20d9ef945411d9c4da9054fba5816",
      "8797db70bc154bf2afeb430fd6faa3bf",
      "7ce75aa4a1544d8f9daa984a407f1ddf",
      "7993a43912b048e881f557ff1a2c27dc",
      "bba4aa50a9b74706a9f54005dbf2c490",
      "09c9c9052a5d44b1befc05c038c663ac",
      "26e6f641fca943e1954a5b3fd117b998",
      "43eed418f936490db792c709e0745038",
      "69051bd68f064de1954d92b6b714e7b8",
      "035105ec0fac4681a96753e969e0c5e3",
      "e6a642e7698f4f669bde0e05ead29bc2",
      "edc5cbd040ab436f840be38f81bd57df",
      "0e0aae9c01504ba3940ca45fceff96f4",
      "0c5489c53f794c54acb73e1cfad38f94",
      "730773778e8541de842ff71663be59bb",
      "d776913c42c241a28058aee860442434",
      "805bb40760194c0a900e4f18bb339930",
      "9e94082fee39441d9e7bc8a3ed8b5c89",
      "38e112157f624c13b9790b7d27ed2e76",
      "8df7b3e97e764c0b98a544294d018aac",
      "c0df8c3bf2f44df4bb10757af1483984",
      "a1ec1a8281c84ee7b73440812cb63979",
      "799339e71c5449aeab8cfbd520a023f6",
      "069480535b094f5b8282d04891adc3e7",
      "df4882aa17de4293b24f019e4837dff6",
      "3b3232d834934b3fa91ce2773e80dc1f",
      "24e4a3929f6746ae9cb055ec6dd34b9c",
      "50a1f69dca2b42ffa098a6255076ceba",
      "9ce026fd62814faa9166e8eecd109b08",
      "c65e88fc560241709714ad46125bc752",
      "cad76ac159f94cd08746ad6f7cdb4d50",
      "3e27648beeac4e07809bc074657a17f8",
      "9290277720fd4994b740f716e89d9723",
      "4fa3e85605d0449682dfe68e45761e87",
      "2b23962033c74c3789fcd5194a2353f4",
      "34ac9f12f0d44ca18a6cde068e5608c2",
      "11ba6ec81af44c0f815eeecc1171638a",
      "3269a13fe1954fb5bb627dc08043d715",
      "f6ce8f47637d4a4e891184017a7165ba",
      "f139c2dafa7e40eca32252099cad2c83",
      "7560fa0df2c944b691b1f80f38c5a017",
      "d8da178c4981461aa613d27672a993b3",
      "6e81a8c13fc94156b42cc91a27b004a4",
      "11ddc9c6b9c84ff699f80be8791721ae",
      "24894b520d6148f38a10c9ca4e1a330f",
      "ece3bee871de4966bcbef983fecaf5dd",
      "50527d16d67a4880b54cc6408a07563d",
      "e5adc38464504446a3f0e54a68422323",
      "a54cf2e120fd4fab929eb623e2b22f5a",
      "e72c08df8bec4e4cb28e745540e22174",
      "50eef01921fb4c24afaffd65ea1071cf",
      "7ba4a8a3d8e7425ea6f438d6825c39b5",
      "d00c3bdc05884b13846e486e269c6f50",
      "0fe57712376442e6ae29672e411b5431",
      "e9131ac14bb94f869870ffd7e1315e5d",
      "432eb05823a443849883ddd33bdcfa8c",
      "3e384e4d88e0446a95d2a807fd1958b7",
      "9dcbb042f7c644dcb1a16fa760b0be9c",
      "b9809da8ba0645d7af48d684fd1dfe6f",
      "3b393a416f9544ce873623d02e22fbed",
      "c1c5a2945f5d4f789348b8892ae66c6f",
      "d8835d67ebf248509e06e3eebc3ce317",
      "d60bf06c057344dea513896b2e77ce7d",
      "3093f3c0f9c84201ae464a0e5620a37b",
      "42d5770e3c384ccd8be1127a181b4ce0",
      "dd70a1e78d8f4888bdb24b49e2e862bc",
      "3b8836aab5c84c7aafc67413318e415c",
      "7b3a2a1750ce4fa6ad03e67a15aa8a1d",
      "de741547de7841568177665a7682b28a",
      "30af595c66fc44bea860f37d823060c2",
      "d54e00054c8e49009e5796f7bfaf6c3e",
      "7eba4dbe2a55404f8542a78b20fc83f2",
      "12635e0b43dc4036a8f094ca50e12bb8",
      "f3138e1cfa8647b19fc0f43d97c73d95",
      "4e78b47a0d9d4de5ad2e236021344cf2",
      "20726896b36446ce9883fd938dcfd59d",
      "3ca2d9eb412b40e19a3d83135e385ee9",
      "a1be98a4d2c14290a772d4a7ab68e70d",
      "2c7d07bb60fc43908ed0610612c51f3b",
      "97f9f1e557264bb5902cc08fb8105086",
      "291ff8b18a3c4325af25891ea025cf19",
      "23e55da3cd49492f9f7c46789dec1529",
      "bea735aae5d54435a50c892487f8db52",
      "a83fecb2fae54475ae1d8ccba4593aa0",
      "dc04a1767d934df0aa48a7b5ad5ab2ec",
      "0995af5af84e444d8684b6bd46720b38",
      "50e3d26246b843b09c7d250bac3b6aa2",
      "4316414b7aef4f30bebe9a17e104139f",
      "bb55c8a993d0419bb183e00735f0a550",
      "5f11f2fcccf246749c877cdd470459bf",
      "2add81775b494fcaa25d08b9222ab983",
      "039b77d237ad42c3b21930ce62f47c9d",
      "b90433f4d2044e2ca7095eadbed3e8f4",
      "98cf565343ba457bb23eedf9a9fc7940",
      "2e3d3da36093490d88fd4d700a3f28c4",
      "715704941e7746a69f34c55e5c2c5b27",
      "d62fa3f9cca1410f9cda3a4d67059f04",
      "f31161247bcc41bb8b5e3f0ebbb86bbe",
      "dc4383258e2e4bc680655abc1d650989",
      "bd7d56adbacd41bab7493d34b0994230",
      "d99245479cf84f90a174129ec7878bea",
      "025e2978582a41ac8a63bdbcc035c845",
      "ed738bc178d045bdbdd6d3b99bdc7ceb",
      "2caaf4985d70412fa353aae0654db503",
      "8e73cc04015a44858f549eaa372f787a",
      "1d47f1c3f8054504ab7363074ed0ffcf",
      "3639a7b1cc3a4c2184496ce81eb6bc10",
      "9ba2e7c4b77d4b3499a5f276b968710f",
      "6d9e0f7873eb4c5f91018e10df412120",
      "370fef849c704fc683478645c061cd70",
      "64820750843b49f08d6058d801e12c58",
      "9f5d68ee5ac4424883b8b8f946054893",
      "8ef2d893259d473dbdc43e8f96c5ef05",
      "ab95ce1190ed4b8d864df423e448678b",
      "a5079e815f4c4577af44dcc82a1faaca",
      "4485c4d7cbb84b8e9ca010429024a6bf",
      "28619f35375d48b7b6e09f78f3502df9",
      "5a8fc754e5034458bf3bb38b0dcd49fa",
      "06e6f22612784ea4922f7231678a8b65",
      "685c12c1f87a4b759cdbdc9d19a90723",
      "a8d6341b98aa422fb90a581e72f5edb3",
      "9d598ba4ccea4166a947803632b7d1cc",
      "4670ee8a503143a3beb91e51a5bab2ff",
      "732cbb5079aa43bd8db1d90a21ff57f2",
      "1b52b969e772473fb21dd2527ad77984",
      "0c14e5ecca8e4f9dbc56ab4dfae3a6a1",
      "8e92ed6676b749b185182ac172121f5d",
      "1d25af85228748bb9b7809483fc4e4c0",
      "54ecb1b561f44e4ba01d49815b625166",
      "0a95aac4bd7f49c49343e99f56087380",
      "7106e487eb0247cd83afe8c85f4765ae",
      "bfd05f3ee61248a3b1951abcb553475d",
      "f5633b7f5c2d4726b47fce3e8ab40a7a",
      "6c2118fd32a44f0e94b0ae2ddd2acdbd",
      "afc9c87c62904c41bebb90d7cafb6b0b",
      "f783acdc08e3462f8cf43fbd4d8bfd2b",
      "6424c69febb34b13b8da5b63ca39db38",
      "44dd1383358d4a9a96e16741b7807060",
      "29dfbc4f161247a9ba0d64b0ab8faf49",
      "51f1257db2b3429490f4dc5bd30898b6",
      "0ca21828aefc4942a0bb3e8a6c65c586",
      "1a155ce23eb240d3b3f8ada7005d4a62",
      "a7c1a9838a504fb8a3d0134567beb306",
      "401d8cc6ac414a289d19d2fb047a730d",
      "15ebfb2eb4a2466bac9ad2e6e6edc5ce",
      "9cd97ab3d8d2400b83541768d622031f",
      "d678b481552241028cdfaf8a369b2f86",
      "be4c403cad67424c9bf4c1fb6d1cdd51",
      "46a788832cb24b0a9e842ab25e173a9d",
      "34c5a7d91d8341dfb20a6f6308fb7f3e",
      "cad080b7c8a845adad313256fa1174e0",
      "70610201bc2f429883ec1eede22c10de",
      "bcc283cd78c3485e8236a8321056ff2a",
      "37d56f5ee09b4a5fa9bb51a60b7f2cab",
      "495273493673408da33e261bf044cad5",
      "dadcfcd770d64f5d819109f9264497bd",
      "fcbe7a1566144fb9a586bcb0c22a820b",
      "283804adc47c4f5bb09a1274bbcd0d98",
      "711335ab743d4fcc916764dc0a1c4acb",
      "d00d2ed53cfc4bf58d6d233de942127a"
     ]
    },
    "id": "g4RNToKN1dqp",
    "outputId": "301d0b19-11af-4ae3-9242-2469e6480aff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Entrenando modelo: BERT (bert-base-uncased)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90d3183eda22422b913168d891e0bfa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e04c16e2e834d5da22fc2a6029bce08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d9f15bedb64c83b869fb7fee93f973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f60ab40305d4c30b5108ce1841675fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9521041bc02d42809d3ceb25ba1b6cec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.3325\n",
      "Epoch 2 completado. Loss promedio: 0.2338\n",
      "Epoch 3 completado. Loss promedio: 0.1922\n",
      "📈 MCAUC (BERT): 0.87729\n",
      "\n",
      "🔄 Entrenando modelo: RoBERTa (roberta-base)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f66dbac5564d459a9f8dc0159cc497a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a6857a0df864f56ac26e413aa1525bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "381e480fe5ff4bdc982727d5902b929f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df74004c5fc646498965d1edb4c12e12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b67e3d5d05c4a2ba0481b72207df970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f19c143cbaa74b80b924155520565baf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.3003\n",
      "Epoch 2 completado. Loss promedio: 0.2172\n",
      "Epoch 3 completado. Loss promedio: 0.1842\n",
      "📈 MCAUC (RoBERTa): 0.88614\n",
      "\n",
      "🔄 Entrenando modelo: DistilBERT (distilbert-base-uncased)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eda78d931755423badbceef9129d4656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fa3e00e2aef4af4a8108a6b0c26fbff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a489fd68f4645d9bfc520e818847d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d71f319940ee4675a76698bf46985f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e8b568461b14a7597d7eda9e65e6da2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.2829\n",
      "Epoch 2 completado. Loss promedio: 0.2066\n",
      "Epoch 3 completado. Loss promedio: 0.1734\n",
      "📈 MCAUC (DistilBERT): 0.89603\n",
      "\n",
      "🔄 Entrenando modelo: ALBERT (albert-base-v2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d650bfbb39d4787bde40a76d8af1d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37cde47a57d64f27a83225ffd833f410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "471907cd70bf4cc1a86c3a8fa21b3f9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26e6f641fca943e1954a5b3fd117b998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e94082fee39441d9e7bc8a3ed8b5c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/47.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.3195\n",
      "Epoch 2 completado. Loss promedio: 0.2892\n",
      "Epoch 3 completado. Loss promedio: 0.2729\n",
      "📈 MCAUC (ALBERT): 0.65739\n",
      "\n",
      "🔄 Entrenando modelo: ELECTRA (google/electra-base-discriminator)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ce026fd62814faa9166e8eecd109b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f139c2dafa7e40eca32252099cad2c83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50eef01921fb4c24afaffd65ea1071cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8835d67ebf248509e06e3eebc3ce317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12635e0b43dc4036a8f094ca50e12bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a83fecb2fae54475ae1d8ccba4593aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.3119\n",
      "Epoch 2 completado. Loss promedio: 0.2294\n",
      "Epoch 3 completado. Loss promedio: 0.1944\n",
      "📈 MCAUC (ELECTRA): 0.87506\n",
      "\n",
      "🔄 Entrenando modelo: MiniLM (microsoft/MiniLM-L12-H384-uncased)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e3d3da36093490d88fd4d700a3f28c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d47f1c3f8054504ab7363074ed0ffcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28619f35375d48b7b6e09f78f3502df9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d25af85228748bb9b7809483fc4e4c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29dfbc4f161247a9ba0d64b0ab8faf49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34c5a7d91d8341dfb20a6f6308fb7f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.4016\n",
      "Epoch 2 completado. Loss promedio: 0.2930\n",
      "Epoch 3 completado. Loss promedio: 0.2740\n",
      "📈 MCAUC (MiniLM): 0.66168\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers scikit-learn torch pandas --quiet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1. Cargar datos\n",
    "df = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', index_col=0)\n",
    "df['genres'] = df['genres'].apply(ast.literal_eval)\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['genres'])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 2. Modelos a probar\n",
    "modelos = [\n",
    "    (\"bert-base-uncased\", \"BERT\"),\n",
    "    (\"roberta-base\", \"RoBERTa\"),\n",
    "    (\"distilbert-base-uncased\", \"DistilBERT\"),\n",
    "    (\"albert-base-v2\", \"ALBERT\"),\n",
    "    (\"google/electra-base-discriminator\", \"ELECTRA\"),\n",
    "    (\"microsoft/MiniLM-L12-H384-uncased\", \"MiniLM\")\n",
    "]\n",
    "\n",
    "# 3. Loop para evaluar cada modelo\n",
    "for model_name, display_name in modelos:\n",
    "    print(f\"\\n🔄 Entrenando modelo: {display_name} ({model_name})\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    class MovieDataset(Dataset):\n",
    "        def __init__(self, texts, labels):\n",
    "            self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
    "            self.labels = torch.tensor(labels).float()\n",
    "        def __getitem__(self, idx):\n",
    "            item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "            item['labels'] = self.labels[idx]\n",
    "            return item\n",
    "        def __len__(self):\n",
    "            return len(self.labels)\n",
    "\n",
    "    # 4. Partición de datos\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df['plot'], y, test_size=0.33, random_state=42)\n",
    "    train_dataset = MovieDataset(X_train, y_train)\n",
    "    test_dataset = MovieDataset(X_test, y_test)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "    # 5. Modelo base exacto\n",
    "    class Classifier(nn.Module):\n",
    "        def __init__(self, num_labels):\n",
    "            super().__init__()\n",
    "            self.encoder = AutoModel.from_pretrained(model_name)\n",
    "            hidden_size = self.encoder.config.hidden_size\n",
    "            self.dropout = nn.Dropout(0.3)\n",
    "            self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "        def forward(self, input_ids, attention_mask):\n",
    "            outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            pooled = outputs.pooler_output if hasattr(outputs, 'pooler_output') else outputs.last_hidden_state[:, 0, :]\n",
    "            x = self.dropout(pooled)\n",
    "            return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "    model = Classifier(y.shape[1]).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # 6. Entrenamiento (3 épocas)\n",
    "    model.train()\n",
    "    for epoch in range(3):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # 7. Evaluación con AUC macro\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].cpu().numpy()\n",
    "            outputs = model(input_ids, attention_mask).cpu().numpy()\n",
    "            all_preds.append(outputs)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    y_pred = np.vstack(all_preds)\n",
    "    y_true = np.vstack(all_labels)\n",
    "    print(f\"📈 MCAUC ({display_name}): {roc_auc_score(y_true, y_pred, average='macro'):.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "24d6f435613a4f7eb5ea6980dedaf5c3",
      "981b347d9a5142749489cd26c1f4dc4f",
      "652232da840e45e085bdb010e014a965",
      "0923ca2cb9154be98ae428b61648e156",
      "2e84e6e15d59418a9fbb4d9745d0ab43",
      "5349c08a61be45d7826b95b7afb0aa0e",
      "fbfbf1729cd14d99935cf067e3919230",
      "be580b4be96e41baa79ea6c35e7e479b",
      "3168a7940d7142bda7b798f2d51de525",
      "5e9e9f083139488bbd280a660867b34d",
      "ddf603722963440aabdb2c7e3303772c",
      "6d602129e71243b1b80e678e113ea4f4",
      "6e98a1071300460285f5db0323c0c77e",
      "e2fad6f39e4b479fb4c7f31ed4b98e20",
      "cb0d0649b92b4e1596fba33765a0afdb",
      "e070b647bddc43ab907469defcbcea17",
      "faf295b44b9a43099aa8ae6f6b19cb72",
      "e19d8443fd384422aa6de1a3067dbc0e",
      "8f3b41e978884e708d0803a6fb8680e2",
      "4a1d201d231640ed9d937be50b84f25a",
      "e6a11fac7b5245ff9e9383b4066f7fff",
      "8dd2822407e846e595e54786f59476ea",
      "a5ecf7d4311843ee86b2e42c269f47c5",
      "bd31a1a8b4c14ca4bd4095462a394c46",
      "adf9cf9b3cc34952b43eabb3694e183a",
      "f0371828de1145f48a1533e74550a256",
      "aa3d10fbbe104231aa720e6e16b1af63",
      "0cebcf0862c147218260f77768662f7b",
      "7832c0f82b584866af84173a19e20780",
      "b94517c9acd841afb444ceac9e8fc0e9",
      "db6e8134b27145faa77fd5cc0e30f24d",
      "33cc576e555d4613b5e6f78fdf20993a",
      "70b68d3c2b2a48b4851c0c5a11c90c09",
      "d379fb6ad1c64a6ebdf3facd76cbf939",
      "893af66c3b964598818ce92f7c9eea7d",
      "21043f495ef04c269a51ce709cdd5072",
      "3529fada5b6442e6ba3b100d2990de95",
      "6882857eeec4498d9958981a20337dc7",
      "d31780e1ab2440c89e87a498123f2f47",
      "96a1562018374800b705560018d79593",
      "d8db055bf0b94db89b1b17efd74c3379",
      "59ba6d68553d42b8a3e1425489a8bd00",
      "9be6065c64104f8d9640893772eab20b",
      "a3b552868e6c474b8b345949fb847ebe",
      "884221a1096f465c892cfbb7388f48a7",
      "737f11a63e5b4875a737e4408617bedb",
      "4e0bb3bc4c7b4dd7a3e51d04375b9f77",
      "8aac7f3e0b254edb83c2cf6964d57fcc",
      "015d3b0d0a004277b287f0b766356118",
      "622fdfeed12342c98800a846339d3bfc",
      "61d7ba35d84248a1909be1c408bd7080",
      "5f7c9b188bfc4cf981dfb22e9d87411e",
      "6cb56fc678024ca2b1e21493002e1320",
      "1bf4538216fc4ff48a4513f810888941",
      "d56f8c8c8b0c4e72987c918c8bfec2f6",
      "31d07988bcd045c2afe09cd99fc06bd7",
      "165cee12b0304cf9ad83c139204fb1dc",
      "400f72299c814a3691d5ff6c106f85ab",
      "1792aa14d8f644449d50ef16a1822f16",
      "8e0df18239d843108a2d41592c5f1db1",
      "57a77c69a5ee49e58536978bc0c28a0a",
      "e1149e35b4b14e0799ba93f9b3b360b2",
      "ccd58212b6d7405fbe630bf51e75016f",
      "1c9847171cd149c5bc83f2289745a76a",
      "6d83f385b74a4c2397dc97b4932c3a99",
      "75cfba8cd5484a47a9e83eebf76b008c",
      "f89e877f19d0407596eeb555c564c7be",
      "4d8fea52370b43dfae695818a561781e",
      "d98f32b22133434da5ec5df896bcb574",
      "dd84b72752dc4ebf8232677309db31ab",
      "cade2c1cd73c44edb1414853428c2e5e",
      "00167a58cd1e454782ff02c721585f6e",
      "6de26f6e19f84971aae4196833165f54",
      "0feca238f0d94a2a938c6ad1e2f1c1f5",
      "24989cc37f0e46b9a147387dbbd0a9eb",
      "9717f8415a654054a27903fe0f9de7bd",
      "62395b53d42d494589caf813dfbe7503",
      "cf717d823101422db05bf90ceb5318ba",
      "30e2d7dfae39422a996296b0cb2748c9",
      "9a911fb76af642e1bf882503f12fc6f1",
      "a8798fad147f4138a3dbcbb4a0c6cc2b",
      "6472d54d62694f719e4c936535a327c9",
      "8958d93b57bc4751b25aaf16b62ace42",
      "cd05c0aeca654f3e91e01a2380acea64",
      "89b5d22dd15641d1a740e5fea46ea95b",
      "d7e5ab05fd4e441c91d37ffe3e732471",
      "a094d68754c74970b6e3323098d17ee8",
      "cbe374c0fcba446488d854a1ce228803",
      "c06ba1653292477e93e57ffe5b044683",
      "a3fc35b95e884a139f7110f583e428c4",
      "453ecac19efc4f548b028a474c7c9f8d",
      "4828d9b0cf2b4182b647a7ad00e84124",
      "1e1a91530b9f436089923e85d97395e6",
      "9fdde75cd1834a6fa6e0bcafaa258ea5",
      "0b30a9293eb54451a66726945a4a97f7",
      "f8cf5f822fa64f50804dc83e43be7595",
      "29830685283a413ab3f83b886c14a6fa",
      "d0c0a152e25b4e3abf5a23a0e3f159e3",
      "eeb981f203dc48b584171fc96fb70053",
      "baa4b3dc0698426a92c17ad985f04ae2",
      "9c10dcc4e81b40218a71e39f2c99105b",
      "3049ad663d9f46fcb23be1616d083e06",
      "49b528cd132d40abbc0cc8a15cf5fa4b",
      "ce00f8c790f241b092f1458a51117d87",
      "5ec09b6d6a7d45abafbfe6608adffb9e",
      "1615978917d8475c9ee9af8c6e1cbcdb",
      "ec7449c4289640efb537d61e4ec9e41a",
      "2e16b71edd234cb7a67e659b8dbbf07c",
      "4edcb4771869481da17881415b0a2c92",
      "62db863f08a841c7aaf99dd3a970fff0",
      "90e0c7a8ffa54aa7acf548b938a940ac",
      "aa8e6dcb425d407fae39911ded59f371",
      "d60bc2bc85084f7a954602feda3aa86d",
      "a447e065923b4ae792ed7156fae38dc9",
      "254657dc2e3c433eba64eb117fd682fb",
      "3b4b5c274757405e896b7b42731ffd72",
      "74fb5beda3164faf90c1dd1b37eb12dd",
      "a91fba1c39c94508bbcd53595c20d571",
      "074811db2f5848a3acc1fa9e96d70752",
      "6b2feeec3fdf4f36bd8318c62e9f6e94",
      "549a714932724c5dad85c17ddb741360",
      "01b25f08ea704f3dbb8821b377cb5f05",
      "d697c6caba4e4a8192997ef9e338e178",
      "bd696b80199940baa404746197d5c744",
      "34856672f85245ad88036525d5b1225c",
      "a9bb6d79c9714ec8a440e7a47d63dc13",
      "b7ed50ef0b1d4c968b71539eccc4a24f",
      "6eb71bbbd56843d0b74fe9a688d64ffb",
      "042b4b52c0e54906b5b34bf7e4aa1d5f",
      "ccdbd5068ee048f4990af6cf66ccac25",
      "647dc0396212487dbfa1959bb1c4a90e",
      "bf0c6fcf44ce4245be32be4a5a15247a",
      "c34612c469c046e291f4786b8c874b48",
      "2292a92c3a1f4c339c21ace2ef6358fd",
      "73dca4864a8d4af7beec1cffc47c1201",
      "63b064a4604f4a60b4b7d4a47adc6f94",
      "4404792eddd343c2bf5fe773c09214f5",
      "d47d740b6386449e9d382069cbc9b2b8",
      "46b9bd0172ed4d9da7ec2297a37c06dc",
      "15c3282778e94670840d384646126c38",
      "987dcec3c921400ba57b6e2641e339dd",
      "f3bc77a6dbe840338b2406ae21dd6bab",
      "26f2ae000b08433699d2d313b9238533",
      "39b0eaca79ce46e0bcf55b705c9b81c6",
      "4847294163bf4dc296b89462608a6e12",
      "ca1582fd02cc4972b78e693783e4464b",
      "2eb06d00dc71457fa1edb8ee25106e31",
      "d25d49312fcc4ad4915d5ba5b4c109f7",
      "c841858ecf274b56ab1fcadc35025abf",
      "4e79f7981312480786b0812bd9ad7a6b",
      "c919d283753c4db488ab8df6d8e3c783",
      "6db45e0e932f44ef8cf5969cd87e474c",
      "431f060f93f348d4a9081f9092d30a42",
      "6f5814e6a3f54fdeacc9f01ea0e73355",
      "d3726b41334b4550be345dd6a5bb725f",
      "0134ca3111cb42809a94e107c560f3e4",
      "06670240eadb43fabecb1540c487ba59",
      "f481093782c94847958be6148e148db2",
      "16670653e11247a89c272e9f57a247ab",
      "e887992693b848c29cd8451f6b6e69e4",
      "b52c7041a1f846d48451681eb118bcc1",
      "6ba6ed863ab94da48c2c026b5fde11e3",
      "798bcf8c4b0e471a9b9a51c62b62a9e3",
      "749ac191c24a4e79ae94c7b00f70af5a",
      "89ec8e8a36e44b1094a10fba5cb04414",
      "6f5906313774492a94448be19b642495",
      "5bc129f903354fc88e18469c378e3daa",
      "036fc7e5d1594c84bf7bba31e3df0a14",
      "8ea51f7425ca4024bf14b59a24fb5d9f",
      "1f26167a0003438893b3970f11ce14ee",
      "482726669f864ff8b358d1f898830f10",
      "4937d2cac4ee4887a32f806b1dd4885a",
      "5a2064d0d9a244bd89ba36f0c31e27a1",
      "37b1e650b60f4e0589542b7045f493bc",
      "df0d974478f84b53802d66a615475b1c",
      "6f8e31773a5848f89383149a7557dca6",
      "3b110044cf274128af326231f9ff4e85",
      "051b6ded3b4c4d9aa8a45e415455a7a6",
      "13d8981bc8184923ad5753ac1db83a0c",
      "503cb67b2ef84e25aa49b852e6af7746",
      "16ba12ed77b349178fb9ce41741b17ea",
      "2d9f8f93671049629bc1e6073d8a9c11",
      "44a52a16d402463ba010ff72a9fe8267",
      "10896bfc680b4d0fa5e1b5efad55ff70",
      "87f77018ff8a42f2b2071a146fbaa22c",
      "5844a68d5c5549d7a17c95755e158b33",
      "17416cc8795941379b6a27e395ffaeae",
      "03dfd07641e94b7db0a276fdc20e63e8",
      "6b3adb18d6114f41abdd60b27f80f213",
      "77fcc9c6886e43b7bdb380d6e4ae04f1",
      "972016ffdedb439ca50d6b066d335b94",
      "7e7d48fd39e242e682b2fc79cbda4fe0",
      "e7fe426ddecd486e947092b02ab3bf60",
      "ca84187887704393a3df3ce81a52bcb5",
      "e90ac730a86841eea89686d3a6e222bb",
      "7571c0d3b672425c9891c603861d2857",
      "8fb4904bc9c34ca5ab338588e4779aaa",
      "cd3de98252984092b1f4aaedc1c13a00",
      "445c55e33dce4a3b8784bb749d88073e",
      "acff51557ebc4c299c31629c78ed192c",
      "4c9e2d2d1a2247dfbbd3cb7396916594",
      "5ebae78ef3cf4d7ca883df12f12d7a3f",
      "d5c4ecde860648c0aec5fc0897f442e3",
      "5218cbaa14b348b49888cc9ea03efeb6",
      "90375c719efa4e829288ca403d158dc1",
      "256cf4cd365749b3a20d6d6dbc5b7de5",
      "33d3ef0d38094cfe8ec3a1e90d09394d",
      "aab98c4dec0d44c2bfff644e56848f33",
      "495a5e3dc7de44b3a6dc56d73f896e7b",
      "cf922be0cd754b9eb7f539f7c134873d",
      "8f49b77c3d8740deb9508a23671b08ba",
      "d09915a571fa422caff564c146a5d10c",
      "b712ccd2f5824d28958713dc83a08e8b",
      "5b417ac5bbc7446bab2244b07bbdc41e",
      "33c9b24717b84d0e8171af958423821e",
      "e14b8de9be5b4cf4a2d38f80fe7a2b09",
      "ab2ab2f15081497ba6098e7527d233d8",
      "ac02ea6d97b042beacf1f1473c68ef2c",
      "3de35369fdf048d2af65d8678d6940d5",
      "e05c415f45fa4fffa3645780dcbbbea5",
      "58f48be17a334a79ad586f2a9303eb3b",
      "4782ba5d4a804d4fbe85405ddace311a",
      "25f5ffd0868342db998928c76baf0263",
      "df95f67542f24b9b99b119d5093f56c3",
      "24932658aa6a4eecbef823709d14b884",
      "cb103c2fe0514c45b17590676249f557",
      "d99b2377f23c4dfab9acbbb0cead087e",
      "81a9b3e950034197acd522bca5d6a090",
      "5981c99fc91848709de89df0dfd18237",
      "7aa6b0e2e3c24f1489aaebf65027d7be",
      "1f284d740ced412fb49415c00d8c403a",
      "23990f25aa3548be87873d415de0022c",
      "e71181bd5df448afbafd90fb0fb471bf",
      "97343cba6879401c8420890b8175c370",
      "d87cd29a2c584e8baa5f238914076cf2",
      "dc00ddd71de84da2a9bf47235ec6afc1",
      "a1076dc2fe7e4e31bd0b89946f222795",
      "74c0432302894c3ab77b1af437fb53d8",
      "fe4da8320bfa4a738ca5e131d8fb42eb",
      "2a887e43347a4403aca23c2bdbbfb091",
      "4a3e565e5d5d4c3c97424c802476d545",
      "771a6acf1c7145c3b0302ba43f4af049",
      "658e2b565b574ed68730c70b6ed8fac0",
      "b87f896829d34286932eca7a0302b7aa",
      "475935f3f69141bba3bd4215f7fa3d1a",
      "37f27056f08b4b5f8eb092c7c84045e2",
      "a2c5abdd490e450a9975590098fb2322",
      "459f230aea5143989f5241da83e68f5b",
      "e796f3ce5d9d4bb8a443c0b28cdc882c",
      "e23560033c8e4a068c4e89dd009ac50a",
      "b7dc4af9bc6f421c84c7d44d211672ec",
      "509b8e95c0a7480b94c74b70947b84da",
      "6b19c9e4979c401abd7ee73dde3005ea",
      "af4713926c044c469c3a044a4f5a4da3",
      "2ad3ab90bd2b404585550cc656c5e9c3",
      "5359a206dbd043c3a19a70d745100677",
      "404a99f5c998487c9c2c7ea057f67af5",
      "e54bef3c68504e44b8168f029438e8df",
      "e135263620ab4ca1829d301c9b071d79",
      "c2e9d5cbd95b457f88492dc37a2e5f02",
      "b3e6e5f206e64c8c946c6fa4bbfb0c28",
      "0fbbdea10ca347858f6ac527bf6dea1f",
      "92547659f5a14b58bc5d6eb3f0d34227",
      "87f068a8b819497aae656993ca28fe92",
      "f2159abad2b847859a808d9eaa5e8318",
      "64615c196f514ef081419629c0db6c18",
      "643b891b61bd489682c719db6d8cd4bd",
      "23ef7775aacd44cdb980e1faf85861c3",
      "4bfd9bbb555c4ad39301161f4f556dd9",
      "06f25607351145239b75f770242aea84",
      "98f67a69c2fd49a8b50697a8b1fc7288",
      "c19f4e9a847547a980d9686e48934967",
      "b624c2f39c334aaa8224d6271c468f2a",
      "ab89886243484c51b395118011a9003e",
      "92bfb2f2b8964ebf94740538d2f71605",
      "e3098820658a4366bf7c69b87193f318",
      "f3fc0a1b4ca045d4848f3b459776581c",
      "cb312381b1ca4835aa6e8fb99d90d214",
      "e290a1409ec34e14947a23ba184a03bd",
      "baa1700b4ffb4138b430fed1046f9d51",
      "52f50da6666d48438d4ae04c830027ff",
      "5f2e756cfcf148259ce5a6037948a137",
      "04c9d25d8ad64cb3ba8a1ee643cb2aaf",
      "40488aa488444a92b17c27dec7d53042",
      "30df1b2e63f84c03beeb2aa6adaaa480",
      "4b0081d912ec486ab74a57df953989be",
      "931a1dae94ee49fe939902e2d67b8aa8",
      "34613f9eeaa740ccaaf1d219fc66217a",
      "0434e13fb15741dea865f3ff8a2087f9",
      "710f45e7569848f3a567fcebdd0b9d30",
      "943018d5066946ec815f8d56da04a418",
      "456172cd6b5a4adeafd793f95cac0782",
      "6d0d8686778f40b3a8beea060edd3f0a",
      "95de24732f484cf4be5902b49a265b15",
      "ee279bafd0d44c619e6d2be67f91815f",
      "3dd4d43de97d477780d8da38d9a2d169",
      "77d0860e2506479889b5ae85f508faf6",
      "449cc1db687348f9b7fd2ee1bd8d6874",
      "d423ffce91914ee582e568c9b9250be9",
      "9b388d90afff460e8d70b6a5efa520df",
      "e565455540fc41bca8d156e5dd509605",
      "5f6ef781c4f74220ba4c135946032158",
      "72c81fa2cf434f67a3570608aeb335d1",
      "969ec68920e244b2a2211b68136385be",
      "ce887c8fa8714732ab13ea5dab2fa930",
      "a89deaa655184ebb93ff78b26ed80bb4",
      "861f508ef0a841d49bb042dbee1c82e0",
      "3292b236f0bf47e9844e596658a06461",
      "b2b5c677b81d42b2801c5516220b16e9",
      "e6bb491e27414451a3cf785e1f8e7d83",
      "1f842c0131414d7e90765a88322ff182",
      "293607e28c374e0ebd930936168edf1e",
      "ca2a796c24bc49a9a6f734143dd1c6a1",
      "ba4c1f98120d4de58f3b243d8610d5f4",
      "b9ad277fc84846d783a866b675e95ba2",
      "17d1fe21788d41ffabbd3371f3fb2182",
      "61e1e82d1c4845cb870cb21f76cfd30c",
      "cb9d5e0f5637435494018fbb7e9cac62",
      "b76db011a208465a95c1f7ae2cb9cd61",
      "b5de7df9c13642e79280022202d08955",
      "f43e0e145dcd4e1ca5ac75c7ac7fcf08",
      "b793af48195d4e799a96cb2827917761",
      "e04acab0e1b741eea9e93ebdbebe49ec",
      "0ca7a12322d44abcb675203c42c7874f",
      "0be7a06fdef84669bf28c2f963da4c00",
      "1b7376dcbaae4812b81031b8d3ea870e",
      "dd842ede4d9c4cafb8150267dfeb136b",
      "4b5c720c893b4187a5bc859cf02fb378",
      "5256530389b3429f99fc7b847165147f",
      "4efbfbe46e2c4e719837369242bdbb77",
      "3eabc0b7336b483c93ddb7929c6eb7c0",
      "3da28a8ef84f48e3833402d21b03721d",
      "900f859a9307499ea8f6f0a3faf304f0",
      "c23e8f72d4d34e4f9170f5ea035e7c1e",
      "b48e4dd434984544838dca9fda6f92c7",
      "22e4f223cfc94edb9eae0c0acebbe381",
      "eca4f4db5b494097b5707283884e34ad",
      "3e40160bd9c34f36ae6efec0a19b3f04",
      "d65cab519d71434b8a0111bed1653856",
      "51976f173b354cf2be98a109278ead03",
      "dbd0c51b6b7e463c9c57154ee6c17baa",
      "86647564c53143eabf7ce511a2d73f19",
      "040cbf4e66f94f16990cf99a3a51855c",
      "a905659530c14cacaa25f6cf1056cb7d",
      "0ed41d6799184a30b1781601ee54aee6",
      "9fb4478729f94bfa9fe4da3f321f604f",
      "cb6c6d5aa5fa4ae9a3f75811591e7d4e",
      "26d99b316e7d480886134fd8442ee9ca",
      "65ab5f52d70a488e92d8079632429cb4",
      "c52bdb44883e4acb88c4f78ce8a8cca7",
      "61ad96f2c4d040b3b4b31f0cf58de17f",
      "a6c20d454ce246d6ad4bea202ebba728",
      "6cfed2e9e5d34e42885bd32cfcb7e695",
      "852e8b03a6e9455e8e8bce2d2c38f5f5",
      "b960b33851064f02b18479a4d23fd479",
      "05890fc03fdc413a908d0a6cb4bea4fb",
      "518d99b98c8142c89b835c375a71df04",
      "991c66b7e62c4b47b39e5cb788aa36f3",
      "e09003bfb48048cd9ef2ed4200311847",
      "dd98a2e2a37e432da2e3e9a922d103d1",
      "e4c4ce1b7cca479a90c0733e7069115e",
      "024db5632f77452bbdb442eedad7792c",
      "b3937e2b2d4242b8b0bb53c608863922"
     ]
    },
    "id": "6K7l53CNcYJs",
    "outputId": "dab1a95e-fc95-4a6b-da64-65c53738bdb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Entrenando modelo: BERT (bert-base-uncased)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d6f435613a4f7eb5ea6980dedaf5c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d602129e71243b1b80e678e113ea4f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5ecf7d4311843ee86b2e42c269f47c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d379fb6ad1c64a6ebdf3facd76cbf939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "884221a1096f465c892cfbb7388f48a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.3243\n",
      "Epoch 2 completado. Loss promedio: 0.2275\n",
      "Epoch 3 completado. Loss promedio: 0.1861\n",
      "📈 MCAUC (BERT): 0.88187\n",
      "\n",
      "🔄 Entrenando modelo: RoBERTa (roberta-base)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d07988bcd045c2afe09cd99fc06bd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f89e877f19d0407596eeb555c564c7be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf717d823101422db05bf90ceb5318ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c06ba1653292477e93e57ffe5b044683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baa4b3dc0698426a92c17ad985f04ae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e0c7a8ffa54aa7acf548b938a940ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.2962\n",
      "Epoch 2 completado. Loss promedio: 0.2104\n",
      "Epoch 3 completado. Loss promedio: 0.1772\n",
      "📈 MCAUC (RoBERTa): 0.88725\n",
      "\n",
      "🔄 Entrenando modelo: DistilBERT (distilbert-base-uncased)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b25f08ea704f3dbb8821b377cb5f05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c34612c469c046e291f4786b8c874b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b0eaca79ce46e0bcf55b705c9b81c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3726b41334b4550be345dd6a5bb725f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f5906313774492a94448be19b642495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.2903\n",
      "Epoch 2 completado. Loss promedio: 0.2078\n",
      "Epoch 3 completado. Loss promedio: 0.1739\n",
      "📈 MCAUC (DistilBERT): 0.89506\n",
      "\n",
      "🔄 Entrenando modelo: ALBERT (albert-base-v2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b110044cf274128af326231f9ff4e85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03dfd07641e94b7db0a276fdc20e63e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "445c55e33dce4a3b8784bb749d88073e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf922be0cd754b9eb7f539f7c134873d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58f48be17a334a79ad586f2a9303eb3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/47.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.3014\n",
      "Epoch 2 completado. Loss promedio: 0.2268\n",
      "Epoch 3 completado. Loss promedio: 0.1930\n",
      "📈 MCAUC (ALBERT): 0.87236\n",
      "\n",
      "🔄 Entrenando modelo: ELECTRA (google/electra-base-discriminator)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23990f25aa3548be87873d415de0022c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "658e2b565b574ed68730c70b6ed8fac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af4713926c044c469c3a044a4f5a4da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2159abad2b847859a808d9eaa5e8318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3098820658a4366bf7c69b87193f318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "931a1dae94ee49fe939902e2d67b8aa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.3110\n",
      "Epoch 2 completado. Loss promedio: 0.2256\n",
      "Epoch 3 completado. Loss promedio: 0.1906\n",
      "📈 MCAUC (ELECTRA): 0.88203\n",
      "\n",
      "🔄 Entrenando modelo: MiniLM (microsoft/MiniLM-L12-H384-uncased)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "449cc1db687348f9b7fd2ee1bd8d6874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b5c677b81d42b2801c5516220b16e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5de7df9c13642e79280022202d08955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eabc0b7336b483c93ddb7929c6eb7c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86647564c53143eabf7ce511a2d73f19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cfed2e9e5d34e42885bd32cfcb7e695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.4008\n",
      "Epoch 2 completado. Loss promedio: 0.2924\n",
      "Epoch 3 completado. Loss promedio: 0.2730\n",
      "📈 MCAUC (MiniLM): 0.66701\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers scikit-learn torch pandas --quiet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1. Cargar datos\n",
    "df = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', index_col=0)\n",
    "df['genres'] = df['genres'].apply(ast.literal_eval)\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['genres'])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 2. Modelos a probar\n",
    "modelos = [\n",
    "    (\"bert-base-uncased\", \"BERT\"),\n",
    "    (\"roberta-base\", \"RoBERTa\"),\n",
    "    (\"distilbert-base-uncased\", \"DistilBERT\"),\n",
    "    (\"albert-base-v2\", \"ALBERT\"),\n",
    "    (\"google/electra-base-discriminator\", \"ELECTRA\"),\n",
    "    (\"microsoft/MiniLM-L12-H384-uncased\", \"MiniLM\")\n",
    "]\n",
    "\n",
    "# 3. Loop para evaluar cada modelo\n",
    "for model_name, display_name in modelos:\n",
    "    print(f\"\\n🔄 Entrenando modelo: {display_name} ({model_name})\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    class MovieDataset(Dataset):\n",
    "        def __init__(self, texts, labels):\n",
    "            self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=384, return_tensors='pt')\n",
    "            self.labels = torch.tensor(labels).float()\n",
    "        def __getitem__(self, idx):\n",
    "            item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "            item['labels'] = self.labels[idx]\n",
    "            return item\n",
    "        def __len__(self):\n",
    "            return len(self.labels)\n",
    "\n",
    "    # 4. Partición de datos\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df['plot'], y, test_size=0.33, random_state=42)\n",
    "    train_dataset = MovieDataset(X_train, y_train)\n",
    "    test_dataset = MovieDataset(X_test, y_test)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "    # 5. Modelo base exacto\n",
    "    class Classifier(nn.Module):\n",
    "        def __init__(self, num_labels):\n",
    "            super().__init__()\n",
    "            self.encoder = AutoModel.from_pretrained(model_name)\n",
    "            hidden_size = self.encoder.config.hidden_size\n",
    "            self.dropout = nn.Dropout(0.3)\n",
    "            self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "        def forward(self, input_ids, attention_mask):\n",
    "            outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            pooled = outputs.pooler_output if hasattr(outputs, 'pooler_output') else outputs.last_hidden_state[:, 0, :]\n",
    "            x = self.dropout(pooled)\n",
    "            return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "    model = Classifier(y.shape[1]).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # 6. Entrenamiento (3 épocas)\n",
    "    model.train()\n",
    "    for epoch in range(3):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # 7. Evaluación con AUC macro\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].cpu().numpy()\n",
    "            outputs = model(input_ids, attention_mask).cpu().numpy()\n",
    "            all_preds.append(outputs)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    y_pred = np.vstack(all_preds)\n",
    "    y_true = np.vstack(all_labels)\n",
    "    print(f\"📈 MCAUC ({display_name}): {roc_auc_score(y_true, y_pred, average='macro'):.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596,
     "referenced_widgets": [
      "6b6788c6d10849469627e7625d05671f",
      "2f52a6be7cc44b70875eff3acd6b00ce",
      "72250012fd9d449caebbf580851760d3",
      "a140f487c6f34a4b91191c45b42fbb93",
      "af2c7ed4aba943e1b964ce97e660a0f3",
      "a869410092c64df7ac2334f34e7d8c29",
      "b342fbf061664978b0131b02f0f53fc3",
      "c69d26edbab94102ba8d279f73896b66",
      "5cc0f67e0f8f4d41a7ccf9ad67bd7b88",
      "c66d478486a94a0285d03ee2045288b9",
      "57158c53731743bc94b101bd6655bb80",
      "95367624c39a4ae29ea97741f60e9278",
      "204ae697c81547aa833f4433dec221c3",
      "a25b88c353d5438198c6385f1bb967cf",
      "bbfa17a7fd594e5db79e39cbab23157c",
      "dfc1dee1b01b465cbf4d7f2a783ae61d",
      "9cccd3c1eb2d4aa6917315809d581e2b",
      "7e3479e9b24648d4b34467d8d7335c39",
      "bf389233f1184248a3de608d1abfc732",
      "49f8a5e7623d444481ad14ea552bcced",
      "4a17b3b371844d44a1cfff3822cece58",
      "23df12804d0e413890c0d927c574ce4a",
      "06a0905816074f618eca915e62a08679",
      "87fd4dca3ec743988e4b21673916e58c",
      "4dcac7f0432f43ca850dfde4037e24d5",
      "5a4a89cbe1d547f99393db4f6a31d954",
      "1c2c4e1c885a4af3a2b37a97fcd3527c",
      "4f9d9f6211354f739dfc46dd695593da",
      "8c80ee9fb69b4871acd400d0570a9455",
      "5fa9cadaebf542089b49599f96f91cda",
      "856e70f1375c4a8e8aa52144ce517974",
      "5ee0b9bad11c464699e13ca15979cc2a",
      "b8b0de83434440ee85070272482a378c",
      "cc31a8c1fa7b44dfa52a7c2e91353f99",
      "775b9f5800b14a8a8144c50d140d8dc6",
      "e948ac43b1c34dcf9457cf97c4dde912",
      "29e416861f934b18a49cfad4172bdb6e",
      "b852a870fae444438f0f49cef06619ba",
      "578abe848f7140d39b675ef78b0aeaf6",
      "29fded181f1d43799e2ea8c63aa0fdba",
      "f70b0e2d72b5402db6ff259722509aea",
      "72901a2b62754faa84a6763fb8b6d8fe",
      "5bd6f01703174ca592baa5511c20837b",
      "1af05739957a49e68cee7fecf19d8985",
      "11927f703b2b416798ecf1155f0fcb34",
      "40b8a002b49e49b4b4db39f8d0787b64",
      "dfa49fc2622b48dca0119b2f2d5edd94",
      "a9a8230eca4e412a91a8d34e687a317e",
      "49b1b7e3706b42fa9aca9ebb5716db55",
      "d81686a164994444b16ac69083bfeebb",
      "4b12e3733c3141c084284fe603b5f692",
      "a5e0d1b8b2ec4db288a0b7feec7fb385",
      "ac2a07c45b154a90a87fed37f43662cf",
      "ba762f367a2d49ff8fe6662b0b8f0187",
      "2dada993c06e4d3b95cc22d9233d8eed"
     ]
    },
    "id": "dJ89RkBvccvL",
    "outputId": "5559d0a9-3da0-4d9a-d65d-6609daa4c201"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b6788c6d10849469627e7625d05671f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95367624c39a4ae29ea97741f60e9278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06a0905816074f618eca915e62a08679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc31a8c1fa7b44dfa52a7c2e91353f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11927f703b2b416798ecf1155f0fcb34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.2863\n",
      "Epoch 2 completado. Loss promedio: 0.2048\n",
      "Epoch 3 completado. Loss promedio: 0.1707\n",
      "Epoch 4 completado. Loss promedio: 0.1450\n",
      "Epoch 5 completado. Loss promedio: 0.1220\n",
      "Epoch 6 completado. Loss promedio: 0.1030\n",
      "Epoch 7 completado. Loss promedio: 0.0865\n",
      "Epoch 8 completado. Loss promedio: 0.0730\n",
      "Epoch 9 completado. Loss promedio: 0.0617\n",
      "Epoch 10 completado. Loss promedio: 0.0517\n",
      "Epoch 11 completado. Loss promedio: 0.0443\n",
      "Epoch 12 completado. Loss promedio: 0.0374\n",
      "Epoch 13 completado. Loss promedio: 0.0314\n",
      "Epoch 14 completado. Loss promedio: 0.0270\n",
      "Epoch 15 completado. Loss promedio: 0.0227\n",
      "📈 MCAUC: 0.8937670420967231\n",
      "✅ Archivo 'pred_genres_text_DistilBERT.csv' generado correctamente.\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers scikit-learn torch pandas --quiet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1. Cargar datos\n",
    "df = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', index_col=0)\n",
    "df['genres'] = df['genres'].apply(ast.literal_eval)\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['genres'])\n",
    "\n",
    "# 2. Tokenizador\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# 3. Dataset personalizado\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=384, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 4. División train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['plot'], y, test_size=0.33, random_state=42)\n",
    "train_dataset = MovieDataset(X_train, y_train)\n",
    "test_dataset = MovieDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# 5. Modelo DistilBERT Multilabel\n",
    "class DistilBERTClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # usar token [CLS]\n",
    "        x = self.dropout(cls_output)\n",
    "        return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DistilBERTClassifier(num_labels=y.shape[1]).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 6. Entrenamiento\n",
    "model.train()\n",
    "for epoch in range(15):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# 7. Evaluación con AUC macro\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].cpu().numpy()\n",
    "        outputs = model(input_ids, attention_mask).cpu().numpy()\n",
    "        all_preds.append(outputs)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "y_pred = np.vstack(all_preds)\n",
    "y_true = np.vstack(all_labels)\n",
    "print(\"📈 MCAUC:\", roc_auc_score(y_true, y_pred, average='macro'))\n",
    "\n",
    "# 8. Predicción sobre test real y exportación\n",
    "dataTesting = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTesting.zip', index_col=0)\n",
    "test_encodings = tokenizer(list(dataTesting['plot']), truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
    "test_dataset = DataLoader(torch.utils.data.TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask']), batch_size=32)\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for input_ids_batch, attn_mask_batch in test_dataset:\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        attn_mask_batch = attn_mask_batch.to(device)\n",
    "        pred_batch = model(input_ids_batch, attn_mask_batch).cpu().numpy()\n",
    "        preds.append(pred_batch)\n",
    "\n",
    "y_pred_test_final = np.vstack(preds)\n",
    "cols = ['p_' + genre for genre in mlb.classes_]\n",
    "res = pd.DataFrame(y_pred_test_final, index=dataTesting.index, columns=cols)\n",
    "res.to_csv('pred_genres_text_DistilBERT.csv', index_label='ID')\n",
    "print(\"✅ Archivo 'pred_genres_text_DistilBERT.csv' generado correctamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646,
     "referenced_widgets": [
      "ac4636a53fd7436e800315aea0cc03f5",
      "fefd7022e35549b2928edb817f58c5c3",
      "415a1d1e9c034f3eb61d8060e5cf737d",
      "ba075d4f03694ae69894a9eb76f9b262",
      "78e0c49b42054695ad252ad17c1c7e22",
      "14f2b37f31124e45a6d1daeaa256ef27",
      "27474f198f1b4d8bab70e92bf58d7f2c",
      "80e2ee5450d94394812be7016ff34fd9",
      "edc6120d5df2443881438a881bf3d9b9",
      "12a8caa1c961471aa57b45156e37e9d0",
      "675630750d8742c9894a6cc94e666db6",
      "cc2719c77ce94f439de498cb720a24a0",
      "6a9befbb017747ce9359969b8b626c41",
      "fc2a5588de8d424daa20559db4c5192a",
      "e3c86e994d8643dda257bd02c11c57a0",
      "3fd8e12b10a543e2b97af9e24919159c",
      "51b7a519bfd64f06a743259351a72406",
      "a925a34f4a9e429595cc606e9b10a30e",
      "25becf3931e943588b0ecfe02b59ff2b",
      "46c3375a17ca400eb909992f154411e3",
      "388027d1b9f44e62b5704b5b5d671624",
      "4efc1cc6e8e54d8fa392e0ea7ff5401f",
      "a93de775bf8a472f84cb962832b482e6",
      "24e934fbd8154adf9af28a31207fc294",
      "32dc7f6b428b415fa8564cc8df19313a",
      "f1fa88432fb24283898a278d8bec6322",
      "d7da60f2303c43b6bcd9c220d136328f",
      "c410d04783f7488d945a602c4f6911ad",
      "02855093e06445ea87fe6ff9503c87e8",
      "c54df46fd2d94569a39d640bac7d92f6",
      "86c6288e46b54b94a0588c332aa31153",
      "d86b4141a3db484db6495e367f7062ea",
      "c093b52e86d44f95a5bde93ea813188e",
      "1152e305023b41269c8b976cc303ee8f",
      "ac0a1f3e0e774bddb26aacc1cbc115f0",
      "c982e13f055749819362925e345f85c0",
      "6c654b6091534c748a7574e9ae0d4d5c",
      "e45ce4025a0149ba8ac193b7fd106155",
      "fd77db28aed54d9b97556e694bb7952b",
      "b4444f6d1ff84a1a968ca2bdad27de3f",
      "8d177258b1ed413e902b3441ce33548a",
      "a5ad44a3f5ef4d958a500a60a70314ec",
      "88bcc822eaf2457794229d4d777bccb7",
      "d258dc294a82453db468bdae25b5605f",
      "f945bcda24784ecda71675390ccd8a2e",
      "0a9ad5abad23491ebd43d61b1925575b",
      "980a714f4c964646b8cac052472e67eb",
      "b5115f453b704757abff50a4e72c3283",
      "eee2e7f83d714859bcb1d007c47448d7",
      "5863721d5a3f4f3e9363ab50d6d1a69f",
      "c3a0486d30b846e5814b802bb4b0fc30",
      "ab64e2be2a264a63b386785e6ccdaea4",
      "5e21866241e74b91b30a5149ab86662c",
      "ca0a775af39e4ea9840f1508b7d108b6",
      "96e1894530ab42f1bed78d99b63b534c",
      "8f76c3a70e3a498192c33d9ea3ed8782",
      "f92b6f63ab694c94b607e9f037925243",
      "1062c3d27d3c48cdba51177684f83dbc",
      "1a96086959aa4ff6afde843822518efe",
      "daa226632c8e4093ad884b47fb70e2ea",
      "ce21c6617be744e8865a002ddea0d05d",
      "360e59328eaa478da24e894eed43a77b",
      "f55b72b7e03f4d3c89d6b7ed4840331a",
      "ab82ee8e853f4e7cad0bb76f11072d55",
      "68b2ef31ff4a4c1b861108dd97ca5239",
      "1620b6cb738248b9b1ca828a95861c86"
     ]
    },
    "id": "YfC3cXIpMhaA",
    "outputId": "9f3502ad-719d-4c12-c8cb-27cef34fe739"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac4636a53fd7436e800315aea0cc03f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc2719c77ce94f439de498cb720a24a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a93de775bf8a472f84cb962832b482e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1152e305023b41269c8b976cc303ee8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f945bcda24784ecda71675390ccd8a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f76c3a70e3a498192c33d9ea3ed8782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.2564\n",
      "Epoch 2 completado. Loss promedio: 0.1834\n",
      "Epoch 3 completado. Loss promedio: 0.1540\n",
      "Epoch 4 completado. Loss promedio: 0.1309\n",
      "Epoch 5 completado. Loss promedio: 0.1124\n",
      "Epoch 6 completado. Loss promedio: 0.0961\n",
      "Epoch 7 completado. Loss promedio: 0.0829\n",
      "Epoch 8 completado. Loss promedio: 0.0704\n",
      "Epoch 9 completado. Loss promedio: 0.0585\n",
      "Epoch 10 completado. Loss promedio: 0.0491\n",
      "Epoch 11 completado. Loss promedio: 0.0415\n",
      "Epoch 12 completado. Loss promedio: 0.0342\n",
      "Epoch 13 completado. Loss promedio: 0.0293\n",
      "Epoch 14 completado. Loss promedio: 0.0250\n",
      "Epoch 15 completado. Loss promedio: 0.0217\n",
      "Epoch 16 completado. Loss promedio: 0.0196\n",
      "Epoch 17 completado. Loss promedio: 0.0165\n",
      "Epoch 18 completado. Loss promedio: 0.0148\n",
      "Epoch 19 completado. Loss promedio: 0.0140\n",
      "Epoch 20 completado. Loss promedio: 0.0128\n",
      "📈 MCAUC (referencia, mismo set de entrenamiento): 0.9999551270132044\n",
      "✅ Archivo generado para Kaggle: pred_genres_text_RoBERTa_full.csv\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers scikit-learn torch pandas --quiet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1. Cargar y preparar datos\n",
    "df = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', index_col=0)\n",
    "df['genres'] = df['genres'].apply(ast.literal_eval)\n",
    "df[\"input_text\"] = df[\"title\"] + \" (\" + df[\"year\"].astype(str) + \"): \" + df[\"plot\"]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['genres'])\n",
    "\n",
    "# 2. Tokenizador y Dataset\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=384, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 3. Usar todo el dataset para entrenamiento y validación\n",
    "X_all, y_all = df['input_text'], y\n",
    "full_dataset = MovieDataset(X_all, y_all)\n",
    "full_loader = DataLoader(full_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# 4. Modelo RoBERTa Multilabel (igual estructura)\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # primer token\n",
    "        x = self.dropout(cls_output)\n",
    "        return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BERTClassifier(num_labels=y.shape[1]).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 5. Entrenamiento con todo el dataset\n",
    "model.train()\n",
    "for epoch in range(20):\n",
    "    total_loss = 0\n",
    "    for batch in full_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(full_loader):.4f}\")\n",
    "\n",
    "# 6. Evaluación sobre los mismos datos (solo referencia)\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in full_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].cpu().numpy()\n",
    "        outputs = model(input_ids, attention_mask).cpu().numpy()\n",
    "        all_preds.append(outputs)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "y_pred = np.vstack(all_preds)\n",
    "y_true = np.vstack(all_labels)\n",
    "print(\"📈 MCAUC (referencia, mismo set de entrenamiento):\", roc_auc_score(y_true, y_pred, average='macro'))\n",
    "\n",
    "# 7. Predicción final para Kaggle y exportación\n",
    "dataTesting = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTesting.zip', index_col=0)\n",
    "dataTesting[\"input_text\"] = dataTesting[\"title\"] + \" (\" + dataTesting[\"year\"].astype(str) + \"): \" + dataTesting[\"plot\"]\n",
    "test_enc = tokenizer(list(dataTesting[\"input_text\"]), truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
    "test_dataset = DataLoader(torch.utils.data.TensorDataset(test_enc['input_ids'], test_enc['attention_mask']), batch_size=32)\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for input_ids_batch, attn_mask_batch in test_dataset:\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        attn_mask_batch = attn_mask_batch.to(device)\n",
    "        pred_batch = model(input_ids_batch, attn_mask_batch).cpu().numpy()\n",
    "        preds.append(pred_batch)\n",
    "\n",
    "y_pred_test_final = np.vstack(preds)\n",
    "cols = ['p_' + genre for genre in mlb.classes_]\n",
    "res = pd.DataFrame(y_pred_test_final, index=dataTesting.index, columns=cols)\n",
    "res.to_csv('pred_genres_text_RoBERTa_full.csv', index_label='ID')\n",
    "print(\"✅ Archivo generado para Kaggle: pred_genres_text_RoBERTa_full.csv\")\n",
    "# 0.90437"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663,
     "referenced_widgets": [
      "1fef0bb422f543c2bfd28b52ecf9bf06",
      "9bc6ac2e4e764ab090084b27d26edef7",
      "6ba49871d24149b1bcbd3967467eec95",
      "ff9e192e0c784a2ab246cf3139f19d72",
      "661958bfd1b54b9194bfe612a505ce36",
      "42b6bd874d394490ad39a4ba8b4e8fa7",
      "53388f20330049c1b7a300feae758324",
      "50e239b95e884546a8dc6d915e3a2b2b",
      "79a17e9aa05f4514ada49615979116a0",
      "cade1f36f9734a5fb5bdb74ab79edb96",
      "4990b27d6366483295c6e00084b51370",
      "67e90bfcb2324bc9b181596f7df1450e",
      "338c7d09dc69486dbc31bf66c428690f",
      "47edc0cd3bb247fcb6959bdc72485266",
      "e89de422d9cc4a1ebfe6d7d7afbc07b8",
      "c3b06d3fe8d646a4b16f57c397cb37f0",
      "5abc4cc8b114487e958ccadf38063e5e",
      "e79c8a1dfd8f4748a68c7a6015ed28fb",
      "db704f9a875b430aa692e62ba5acba0d",
      "e43e60dd5df84a82a51469678eb31495",
      "20fe568ab7854ea6b541a09969411cf4",
      "c29e21bdf0cd48e6b2d2bdf99c2f411f",
      "db6a527aaf6b4ec0b5896b29c1f176ae",
      "3453d04015c9423f9349b607b54c1fe1",
      "04ad95d17e954562b7772354559224b5",
      "7075c01c6030418c840beee04f1c1074",
      "11a7389e22f04db0810073683fa4bdbc",
      "1aa73a95138c498e98ce31e8b914bcea",
      "489085a5870342caa7dc76259585a5f8",
      "a026e8091e6b4106b2e16b8798585102",
      "86a970a7c5744571af63146c0d546b08",
      "f371830841dd4c81a08d52b8e2006488",
      "6bbafbee65f84309ad7282b5080aed0e",
      "6743e034d9c945e28d5ff7e59c4a9ba2",
      "0a9f1341d74c42c0bf6cc8c922f614f1",
      "119e21a5f1aa416a9c45116cd4ecbd66",
      "2e87cec45abd43a1bc45d48b9e1e9390",
      "bc8f9a039fc941f681b1d75928661697",
      "ffc1147efb9a4c9ea22d2b13f7936d64",
      "9b597cd736dd4b9881b129dfda831771",
      "6b5baf90a38b49e4acc7d4f5d8c803e4",
      "9bc43ea8646e40db8061d937dc65ed35",
      "2eb32a4b4efb4ad7a34b303ac2a90200",
      "f141fb406b204b23931fc06f20eb01ee",
      "678c19ff8453445489ff672a0adee583",
      "ffd059d2fd2f4986818fc99387933ab1",
      "7b6ce2042bf54807b6a87f3f41088945",
      "69f9ec012f3e4c52adad01a3f71e263f",
      "c0f0cc174f4f486f8635cf0147d79b02",
      "4455c6d60a5e49f98ec636adbee5aeb8",
      "8f622fccbda04f81b85892169efb767f",
      "86b1cd69298f43b98b97aad96ceda396",
      "382c4baba1ed404cb554a98bd24fed6b",
      "0233b229bf4641c8b6fd3ee38fcbf12d",
      "00bd7a583a7d403b9ed712c744b1babb",
      "e7b23175f9454f79b7b3b02d06cbc407",
      "1a11dd0fec564a21a1c2746f5085b816",
      "9610cbcba8ae42058e1afedb62282ae9",
      "9a9d1c9666144a10b15a3bee7e8b3d37",
      "5211ecd47a7f4096abb9414f0ebb2588",
      "753763e72a9a4cfa89e05ae3da3340f7",
      "c47247bd8ab04e409ae9f0bb62b6aecb",
      "d634f815e2a74e98964fcf57da48456d",
      "b95c37818c154809a3877e0e9a14de87",
      "cd2a9f304b0d412e906ad2de2748dfc9",
      "f75948354c294f31b2818b6eb946ddd3"
     ]
    },
    "id": "WEFvtPIqZI3_",
    "outputId": "f9954352-4962-4910-a64d-3bfc19af57dc"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fef0bb422f543c2bfd28b52ecf9bf06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e90bfcb2324bc9b181596f7df1450e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db6a527aaf6b4ec0b5896b29c1f176ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6743e034d9c945e28d5ff7e59c4a9ba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "678c19ff8453445489ff672a0adee583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b23175f9454f79b7b3b02d06cbc407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.2543\n",
      "Epoch 2 completado. Loss promedio: 0.1818\n",
      "Epoch 3 completado. Loss promedio: 0.1514\n",
      "Epoch 4 completado. Loss promedio: 0.1301\n",
      "Epoch 5 completado. Loss promedio: 0.1104\n",
      "Epoch 6 completado. Loss promedio: 0.0943\n",
      "Epoch 7 completado. Loss promedio: 0.0810\n",
      "Epoch 8 completado. Loss promedio: 0.0689\n",
      "Epoch 9 completado. Loss promedio: 0.0575\n",
      "Epoch 10 completado. Loss promedio: 0.0478\n",
      "Epoch 11 completado. Loss promedio: 0.0407\n",
      "Epoch 12 completado. Loss promedio: 0.0332\n",
      "Epoch 13 completado. Loss promedio: 0.0275\n",
      "Epoch 14 completado. Loss promedio: 0.0242\n",
      "Epoch 15 completado. Loss promedio: 0.0208\n",
      "📈 MCAUC (referencia, mismo set de entrenamiento): 0.9999594742812302\n",
      "✅ Archivo generado para Kaggle: pred_genres_text_RoBERTa_full.csv\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers scikit-learn torch pandas --quiet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1. Cargar y preparar datos\n",
    "df = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', index_col=0)\n",
    "df['genres'] = df['genres'].apply(ast.literal_eval)\n",
    "df[\"input_text\"] = df[\"title\"] + \" (\" + df[\"year\"].astype(str) + \"): \" + df[\"plot\"]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['genres'])\n",
    "\n",
    "# 2. Tokenizador y Dataset\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=384, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 3. Usar todo el dataset para entrenamiento y validación\n",
    "X_all, y_all = df['input_text'], y\n",
    "full_dataset = MovieDataset(X_all, y_all)\n",
    "full_loader = DataLoader(full_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# 4. Modelo RoBERTa Multilabel (igual estructura)\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # primer token\n",
    "        x = self.dropout(cls_output)\n",
    "        return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BERTClassifier(num_labels=y.shape[1]).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 5. Entrenamiento con todo el dataset\n",
    "model.train()\n",
    "for epoch in range(15):\n",
    "    total_loss = 0\n",
    "    for batch in full_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(full_loader):.4f}\")\n",
    "\n",
    "# 6. Evaluación sobre los mismos datos (solo referencia)\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in full_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].cpu().numpy()\n",
    "        outputs = model(input_ids, attention_mask).cpu().numpy()\n",
    "        all_preds.append(outputs)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "y_pred = np.vstack(all_preds)\n",
    "y_true = np.vstack(all_labels)\n",
    "print(\"📈 MCAUC (referencia, mismo set de entrenamiento):\", roc_auc_score(y_true, y_pred, average='macro'))\n",
    "\n",
    "# 7. Predicción final para Kaggle y exportación\n",
    "dataTesting = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTesting.zip', index_col=0)\n",
    "dataTesting[\"input_text\"] = dataTesting[\"title\"] + \" (\" + dataTesting[\"year\"].astype(str) + \"): \" + dataTesting[\"plot\"]\n",
    "test_enc = tokenizer(list(dataTesting[\"input_text\"]), truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
    "test_dataset = DataLoader(torch.utils.data.TensorDataset(test_enc['input_ids'], test_enc['attention_mask']), batch_size=32)\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for input_ids_batch, attn_mask_batch in test_dataset:\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        attn_mask_batch = attn_mask_batch.to(device)\n",
    "        pred_batch = model(input_ids_batch, attn_mask_batch).cpu().numpy()\n",
    "        preds.append(pred_batch)\n",
    "\n",
    "y_pred_test_final = np.vstack(preds)\n",
    "cols = ['p_' + genre for genre in mlb.classes_]\n",
    "res = pd.DataFrame(y_pred_test_final, index=dataTesting.index, columns=cols)\n",
    "res.to_csv('pred_genres_text_RoBERTa_full.csv', index_label='ID')\n",
    "print(\"✅ Archivo generado para Kaggle: pred_genres_text_RoBERTa_full.csv\")\n",
    "# 0.91238"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472,
     "referenced_widgets": [
      "3da7fb80420848bc9a0b4b7e228fc2ce",
      "2ad70bc8b2434f52a9c4e14409b52c09",
      "fcf4da5cf0b343cf8811f85e3fe2e366",
      "4f4f48eb35cf4536b8790a7da8972228",
      "d8210d0b5b484b49af7edb9a531d0039",
      "4d19b27870dd43a8a8c562175b158e68",
      "d8b66f476a0b4eb88440cb8a5e1d115b",
      "9cad2d5375764f7795bda6ce58447e4c",
      "54cf46709d21494b886a8b20e532ee05",
      "4a8078b9bc4645f9a80848e3e8916ea5",
      "48e48511e86048478f6f6d37cfb65c73",
      "e473b331906049bba6df98fbb56496c4",
      "96cf576540c243aa991fa6d9f3e7e420",
      "fc01128e7db24cee9303a16e242a0457",
      "778be5c4a27c4aabb4bd94847a84a3e0",
      "d4155e5a915f40e9a4d0db20d340be0e",
      "fc6fe6c02463400d8f54db86e4936e55",
      "ebcaab467b5442cbb20e08dd9d6aaea5",
      "7cab844e154e4bf0afc18d35db4d995f",
      "67a1fa8bb79a4e3fa481d468e35a108c",
      "83f55a97cecd41f9ad7ad9d967ca6030",
      "a6cccd199e5f476eb7dd555696fff59b",
      "9e3537d54b9645119ecdf019d912041d",
      "7c8229384a2d438fbc40eb2217beccaf",
      "e7c6b516c8e04a72a9a8a0394d7f20d0",
      "c3000f1618394ceea7b5f98f1ad53979",
      "5c5e883e9c244993b06196bef050d686",
      "e7cb9d40ecee4f0a92262904bd2fe509",
      "90ca96e7761f4828aae85af71ba56b8e",
      "1ed1cce2662d47f5ab0b56fc21d6a560",
      "b7038ae2fdc44f4b81d90d5d4e3abb50",
      "bff88fce70c24cc2aaebc3d9dde78e10",
      "a82036a3660f45a48e737478812a973e",
      "5f94930607a349a595af5392fdd132cf",
      "2b76186d35034c47b53536a2980d8cf1",
      "1a9b84bd84db452d9d88a98139e0d2a3",
      "5ced90a8d8864d78ab60501672ad6656",
      "f4cacbc608c14f98b7304772702c11ae",
      "a3810af4a3ae46e79d9efdfbad07a295",
      "7f604b1001d1479da182f2130983f42f",
      "40be28cea3b247348edb7caf2652346e",
      "368a0077ba5f42378e3164adc9f8cdf3",
      "56f098cf1a2b40a4bd2ce43e50f8cb3c",
      "84e2f82001c7450b8675aabfd981cac7",
      "f21dc31996394a0591e813dd18502268",
      "446367c5ad5944e1b66073b8642b015f",
      "6596c96ade6742c690a1c52265e2ed50",
      "18fd166931c24cad95a0585938bdfb2c",
      "086da91e3d2944c49e47665645d57211",
      "8f3b52c1466647c492fbd948f8e2c775",
      "9211804307454bfa8a57824dfd9b2bcb",
      "5023c856d4c84659af7e812794b5437a",
      "0821da65c66b4f8dbe7549608962675f",
      "5227af1a821f4299aef058bf782a3c3a",
      "045ff107a98041bba540b644918e3d2e"
     ]
    },
    "id": "sR-7C2DZLE0O",
    "outputId": "a751894e-e109-4d9d-ec35-f59acb7da152"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3da7fb80420848bc9a0b4b7e228fc2ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e473b331906049bba6df98fbb56496c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e3537d54b9645119ecdf019d912041d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f94930607a349a595af5392fdd132cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f21dc31996394a0591e813dd18502268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.2923\n",
      "Epoch 2 completado. Loss promedio: 0.2025\n",
      "Epoch 3 completado. Loss promedio: 0.1634\n",
      "Epoch 4 completado. Loss promedio: 0.1336\n",
      "Epoch 5 completado. Loss promedio: 0.1103\n",
      "Epoch 6 completado. Loss promedio: 0.0919\n",
      "Epoch 7 completado. Loss promedio: 0.0763\n",
      "Epoch 8 completado. Loss promedio: 0.0644\n",
      "Epoch 9 completado. Loss promedio: 0.0536\n",
      "Epoch 10 completado. Loss promedio: 0.0453\n",
      "Epoch 11 completado. Loss promedio: 0.0381\n",
      "Epoch 12 completado. Loss promedio: 0.0321\n",
      "Epoch 13 completado. Loss promedio: 0.0271\n",
      "Epoch 14 completado. Loss promedio: 0.0231\n",
      "Epoch 15 completado. Loss promedio: 0.0194\n",
      "📈 MCAUC (referencia, mismo set de entrenamiento): 0.9997665672670131\n",
      "✅ Archivo generado sin año: pred_genres_text_BERT_no_year.csv\n"
     ]
    }
   ],
   "source": [
    "# 🛠️ Instalar librerías necesarias\n",
    "# !pip install transformers scikit-learn torch pandas --quiet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1. Cargar y preparar datos\n",
    "df = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', index_col=0)\n",
    "df['genres'] = df['genres'].apply(ast.literal_eval)\n",
    "df[\"input_text\"] = df[\"title\"] + \": \" + df[\"plot\"]  # 🔧 AÑO ELIMINADO\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['genres'])\n",
    "\n",
    "# 2. Tokenizador y Dataset\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 3. Usar todo el dataset para entrenamiento y validación\n",
    "X_all, y_all = df['input_text'], y\n",
    "full_dataset = MovieDataset(X_all, y_all)\n",
    "full_loader = DataLoader(full_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# 4. Modelo BERT Multilabel\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.pooler_output\n",
    "        x = self.dropout(cls_output)\n",
    "        return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BERTClassifier(num_labels=y.shape[1]).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 5. Entrenamiento con todo el dataset\n",
    "model.train()\n",
    "for epoch in range(15):\n",
    "    total_loss = 0\n",
    "    for batch in full_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(full_loader):.4f}\")\n",
    "\n",
    "# 6. Evaluación sobre los mismos datos (solo referencia interna)\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in full_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].cpu().numpy()\n",
    "        outputs = model(input_ids, attention_mask).cpu().numpy()\n",
    "        all_preds.append(outputs)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "y_pred = np.vstack(all_preds)\n",
    "y_true = np.vstack(all_labels)\n",
    "print(\"📈 MCAUC (referencia, mismo set de entrenamiento):\", roc_auc_score(y_true, y_pred, average='macro'))\n",
    "\n",
    "# 7. Predicción final para Kaggle y exportación\n",
    "dataTesting = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTesting.zip', index_col=0)\n",
    "dataTesting[\"input_text\"] = dataTesting[\"title\"] + \": \" + dataTesting[\"plot\"]  # 🔧 AÑO ELIMINADO\n",
    "test_enc = tokenizer(list(dataTesting[\"input_text\"]), truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
    "test_dataset = DataLoader(torch.utils.data.TensorDataset(test_enc['input_ids'], test_enc['attention_mask']), batch_size=32)\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for input_ids_batch, attn_mask_batch in test_dataset:\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        attn_mask_batch = attn_mask_batch.to(device)\n",
    "        pred_batch = model(input_ids_batch, attn_mask_batch).cpu().numpy()\n",
    "        preds.append(pred_batch)\n",
    "\n",
    "y_pred_test_final = np.vstack(preds)\n",
    "cols = ['p_' + genre for genre in mlb.classes_]\n",
    "res = pd.DataFrame(y_pred_test_final, index=dataTesting.index, columns=cols)\n",
    "res.to_csv('pred_genres_text_BERT_no_year.csv', index_label='ID')\n",
    "print(\"✅ Archivo generado sin año: pred_genres_text_BERT_no_year.csv\")\n",
    "# 0.90346\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0WjtmZ6qfIE",
    "outputId": "9a19c643-60e4-47fd-883d-5c151b921d23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m113.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m102.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hEpoch 1 completado. Loss promedio: 0.2988\n",
      "Epoch 2 completado. Loss promedio: 0.2061\n",
      "Epoch 3 completado. Loss promedio: 0.1655\n",
      "Epoch 4 completado. Loss promedio: 0.1364\n",
      "Epoch 5 completado. Loss promedio: 0.1127\n",
      "Epoch 6 completado. Loss promedio: 0.0944\n",
      "Epoch 7 completado. Loss promedio: 0.0784\n",
      "Epoch 8 completado. Loss promedio: 0.0654\n",
      "Epoch 9 completado. Loss promedio: 0.0551\n",
      "Epoch 10 completado. Loss promedio: 0.0461\n",
      "Epoch 11 completado. Loss promedio: 0.0385\n",
      "Epoch 12 completado. Loss promedio: 0.0324\n",
      "📈 MCAUC (referencia, mismo set de entrenamiento): 0.998991750669143\n",
      "✅ Archivo generado para Kaggle: pred_genres_text_BERT_full.csv\n"
     ]
    }
   ],
   "source": [
    "# 🛠️ Instalar librerías necesarias\n",
    "!pip install transformers scikit-learn torch pandas --quiet\n",
    "\n",
    "# 📚 Importar librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1. Cargar y preparar datos\n",
    "df = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', index_col=0)\n",
    "df['genres'] = df['genres'].apply(ast.literal_eval)\n",
    "df[\"input_text\"] = df[\"title\"] + \" (\" + df[\"year\"].astype(str) + \"): \" + df[\"plot\"]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['genres'])\n",
    "\n",
    "# 2. Tokenizador y Dataset\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 3. Usar todo el dataset para entrenamiento y validación\n",
    "X_all, y_all = df['input_text'], y\n",
    "full_dataset = MovieDataset(X_all, y_all)\n",
    "full_loader = DataLoader(full_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# 4. Modelo BERT Multilabel\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.pooler_output\n",
    "        x = self.dropout(cls_output)\n",
    "        return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BERTClassifier(num_labels=y.shape[1]).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 5. Entrenamiento con todo el dataset\n",
    "model.train()\n",
    "for epoch in range(12):\n",
    "    total_loss = 0\n",
    "    for batch in full_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(full_loader):.4f}\")\n",
    "\n",
    "# 6. Evaluación sobre los mismos datos (solo referencia interna)\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in full_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].cpu().numpy()\n",
    "        outputs = model(input_ids, attention_mask).cpu().numpy()\n",
    "        all_preds.append(outputs)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "y_pred = np.vstack(all_preds)\n",
    "y_true = np.vstack(all_labels)\n",
    "print(\"📈 MCAUC (referencia, mismo set de entrenamiento):\", roc_auc_score(y_true, y_pred, average='macro'))\n",
    "\n",
    "# 7. Predicción final para Kaggle y exportación\n",
    "dataTesting = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTesting.zip', index_col=0)\n",
    "dataTesting[\"input_text\"] = dataTesting[\"title\"] + \" (\" + dataTesting[\"year\"].astype(str) + \"): \" + dataTesting[\"plot\"]\n",
    "test_enc = tokenizer(list(dataTesting[\"input_text\"]), truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
    "test_dataset = DataLoader(torch.utils.data.TensorDataset(test_enc['input_ids'], test_enc['attention_mask']), batch_size=32)\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for input_ids_batch, attn_mask_batch in test_dataset:\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        attn_mask_batch = attn_mask_batch.to(device)\n",
    "        pred_batch = model(input_ids_batch, attn_mask_batch).cpu().numpy()\n",
    "        preds.append(pred_batch)\n",
    "\n",
    "y_pred_test_final = np.vstack(preds)\n",
    "cols = ['p_' + genre for genre in mlb.classes_]\n",
    "res = pd.DataFrame(y_pred_test_final, index=dataTesting.index, columns=cols)\n",
    "res.to_csv('pred_genres_text_BERT_full.csv', index_label='ID')\n",
    "print(\"✅ Archivo generado para Kaggle: pred_genres_text_BERT_full.csv\")\n",
    "# 0.91257\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 854,
     "referenced_widgets": [
      "7c700b1974664994b67014253c28f63f",
      "bad870e3581641e296e494a84bdfc09f",
      "374a6b5b38da492e996a2d829bcd694d",
      "92400859991b49f098c06a17d9f5b0d8",
      "92750b13d9574c608c82b07705125a44",
      "3e02b438acd44e7b8e72103d137f51c3",
      "cda02223d4ab4abcba3bef5786c1cfb6",
      "14b0547d48a1450e8d0fad4186e05de4",
      "b59d2252bfa949ce84ecf2663b258e2a",
      "3d142d2ac3a34f3fb1bca9fe0f5f95b2",
      "18088a09654a4413998362890ba6e5b9",
      "65b03d9bfaeb4f8f870ea3001025f273",
      "dc5cfdb164cd47928d2e592ebaa04686",
      "16318832482e49a0ac8ad0cb8e799b1f",
      "59775bd91316422983a59c7c9f5ad4be",
      "e1bb0b40395a4933a656b0af697e725e",
      "0dfd6459cccb48158f7239323bb2d5c4",
      "5bcf6a42bfde4cbb956da923c0765b6d",
      "02098b399ed1475a9465fd4dd9213be2",
      "dc9ebd31ea5f4dbdb5f5c4c2230bb294",
      "ce387bf4460b4c528c331038d9bcf70b",
      "ee2a3e3a31d84a0288b9f3727d28b0f1",
      "75975266c7e049d2ac98bac0b0254565",
      "aca510b2b9044995adc28915ac0bf411",
      "95ffdc4e7a1043bda13cbb5f97da144f",
      "e8e5d2e60dbe41bf828d058326c3e79e",
      "407dbc35f58f40f08914ac31a525248c",
      "44a70b6414fb4f6f8018773e344ea6db",
      "f1913eb6b1624d39800c6565f2a4bb88",
      "669652a06ddb43a3a55d6cdbe89dabf2",
      "26b72c3d576547c19b12f53a83642304",
      "b43511f5aa5a4f39947c46b55548c03a",
      "c7860d6a9b2d43039d7743730a9d7b68",
      "7d6b58839a3c4dd19199b4deeaae64da",
      "d85e7217eed0475aacdd11c78dfc6d42",
      "7e009f56b86443a59a2845e1421334b0",
      "d1c853ed6d4c4a788333d1e99ed953a7",
      "f8315b941d6a407aa5813a971f8528ea",
      "5aebefe5211a43e8b863dbada56e9f5b",
      "4e599cbdc45844a788220aa78ce076e6",
      "206763ace1e143a0a4a3ea9ae8f003ea",
      "8875ac8a959145688921b332ff4cb635",
      "7707d39fd6e2437fbf79b468569c7e74",
      "7a731756afea4f76a1658a452d959765",
      "f36d2e40aa5b44919261f262b9132cfc",
      "bffd7a3766ab45bc9d0cb15382e60db2",
      "b8ea8b8f240944f5a98b8dfd365629d9",
      "af7006cd6a394830bb8ea6e6b838f273",
      "4baeaf9f37204330a79d364ab676d020",
      "421778814a5c46a586f93f9180280009",
      "479cf7a8f6bc472f92f53b2eb8c4acc9",
      "fc299e4be2094bf5a40c596e13653993",
      "bb759c439ee74bbb9946f87cc0e44676",
      "bebd42253656443797039243cdb4a56a",
      "b59ac1e0899f458cac826bd213166135"
     ]
    },
    "id": "F8nl9N1eBNee",
    "outputId": "8507c6f1-442a-45cb-b4f6-7b2dcfbc211d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c700b1974664994b67014253c28f63f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b03d9bfaeb4f8f870ea3001025f273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75975266c7e049d2ac98bac0b0254565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d6b58839a3c4dd19199b4deeaae64da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f36d2e40aa5b44919261f262b9132cfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.2986\n",
      "Epoch 2 completado. Loss promedio: 0.2044\n",
      "Epoch 3 completado. Loss promedio: 0.1639\n",
      "Epoch 4 completado. Loss promedio: 0.1349\n",
      "Epoch 5 completado. Loss promedio: 0.1113\n",
      "Epoch 6 completado. Loss promedio: 0.0927\n",
      "Epoch 7 completado. Loss promedio: 0.0779\n",
      "Epoch 8 completado. Loss promedio: 0.0643\n",
      "Epoch 9 completado. Loss promedio: 0.0537\n",
      "Epoch 10 completado. Loss promedio: 0.0457\n",
      "Epoch 11 completado. Loss promedio: 0.0379\n",
      "Epoch 12 completado. Loss promedio: 0.0317\n",
      "Epoch 13 completado. Loss promedio: 0.0266\n",
      "Epoch 14 completado. Loss promedio: 0.0227\n",
      "Epoch 15 completado. Loss promedio: 0.0198\n",
      "Epoch 16 completado. Loss promedio: 0.0169\n",
      "Epoch 17 completado. Loss promedio: 0.0150\n",
      "Epoch 18 completado. Loss promedio: 0.0137\n",
      "📈 MCAUC (referencia, mismo set de entrenamiento): 0.9996191970935708\n",
      "✅ Archivo generado para Kaggle: pred_genres_text_BERT_full.csv\n"
     ]
    }
   ],
   "source": [
    "# 🛠️ Instalar librerías necesarias\n",
    "!pip install transformers scikit-learn torch pandas --quiet\n",
    "\n",
    "# 📚 Importar librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1. Cargar y preparar datos\n",
    "df = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', index_col=0)\n",
    "df['genres'] = df['genres'].apply(ast.literal_eval)\n",
    "df[\"input_text\"] = df[\"title\"] + \" (\" + df[\"year\"].astype(str) + \"): \" + df[\"plot\"]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['genres'])\n",
    "\n",
    "# 2. Tokenizador y Dataset\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 3. Usar todo el dataset para entrenamiento y validación\n",
    "X_all, y_all = df['input_text'], y\n",
    "full_dataset = MovieDataset(X_all, y_all)\n",
    "full_loader = DataLoader(full_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# 4. Modelo BERT Multilabel\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.pooler_output\n",
    "        x = self.dropout(cls_output)\n",
    "        return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BERTClassifier(num_labels=y.shape[1]).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 5. Entrenamiento con todo el dataset\n",
    "model.train()\n",
    "for epoch in range(18):\n",
    "    total_loss = 0\n",
    "    for batch in full_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(full_loader):.4f}\")\n",
    "\n",
    "# 6. Evaluación sobre los mismos datos (solo referencia interna)\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in full_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].cpu().numpy()\n",
    "        outputs = model(input_ids, attention_mask).cpu().numpy()\n",
    "        all_preds.append(outputs)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "y_pred = np.vstack(all_preds)\n",
    "y_true = np.vstack(all_labels)\n",
    "print(\"📈 MCAUC (referencia, mismo set de entrenamiento):\", roc_auc_score(y_true, y_pred, average='macro'))\n",
    "\n",
    "# 7. Predicción final para Kaggle y exportación\n",
    "dataTesting = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTesting.zip', index_col=0)\n",
    "dataTesting[\"input_text\"] = dataTesting[\"title\"] + \" (\" + dataTesting[\"year\"].astype(str) + \"): \" + dataTesting[\"plot\"]\n",
    "test_enc = tokenizer(list(dataTesting[\"input_text\"]), truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
    "test_dataset = DataLoader(torch.utils.data.TensorDataset(test_enc['input_ids'], test_enc['attention_mask']), batch_size=32)\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for input_ids_batch, attn_mask_batch in test_dataset:\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        attn_mask_batch = attn_mask_batch.to(device)\n",
    "        pred_batch = model(input_ids_batch, attn_mask_batch).cpu().numpy()\n",
    "        preds.append(pred_batch)\n",
    "\n",
    "y_pred_test_final = np.vstack(preds)\n",
    "cols = ['p_' + genre for genre in mlb.classes_]\n",
    "res = pd.DataFrame(y_pred_test_final, index=dataTesting.index, columns=cols)\n",
    "res.to_csv('pred_genres_text_BERT_full.csv', index_label='ID')\n",
    "print(\"✅ Archivo generado para Kaggle: pred_genres_text_BERT_full.csv\")\n",
    "# 0.91304\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UXIhnYFhTxL9",
    "outputId": "19b128ba-b5c9-4723-dba5-1d691c4128cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.2936\n",
      "Epoch 2 completado. Loss promedio: 0.1966\n",
      "Epoch 3 completado. Loss promedio: 0.1569\n",
      "Epoch 4 completado. Loss promedio: 0.1291\n",
      "Epoch 5 completado. Loss promedio: 0.1064\n",
      "Epoch 6 completado. Loss promedio: 0.0884\n",
      "Epoch 7 completado. Loss promedio: 0.0737\n",
      "Epoch 8 completado. Loss promedio: 0.0611\n",
      "Epoch 9 completado. Loss promedio: 0.0512\n",
      "Epoch 10 completado. Loss promedio: 0.0426\n",
      "Epoch 11 completado. Loss promedio: 0.0360\n",
      "Epoch 12 completado. Loss promedio: 0.0301\n",
      "Epoch 13 completado. Loss promedio: 0.0257\n",
      "Epoch 14 completado. Loss promedio: 0.0222\n",
      "Epoch 15 completado. Loss promedio: 0.0187\n",
      "📈 MCAUC (referencia, mismo set de entrenamiento): 0.9998319396345208\n",
      "✅ Archivo generado para Kaggle: pred_genres_text_BERT_year_first.csv\n"
     ]
    }
   ],
   "source": [
    "# 🛠️ Instalar librerías necesarias\n",
    "!pip install transformers scikit-learn torch pandas --quiet\n",
    "\n",
    "# 📚 Importar librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1. Cargar y preparar datos\n",
    "df = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', index_col=0)\n",
    "df['genres'] = df['genres'].apply(ast.literal_eval)\n",
    "\n",
    "# ✅ Año al comienzo\n",
    "df[\"input_text\"] = df[\"year\"].astype(str) + \" \" + df[\"title\"] + \": \" + df[\"plot\"]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['genres'])\n",
    "\n",
    "# 2. Tokenizador y Dataset\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=384, return_tensors='pt')  # ✅ max_length aumentado\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 3. Usar todo el dataset para entrenamiento y validación\n",
    "X_all, y_all = df['input_text'], y\n",
    "full_dataset = MovieDataset(X_all, y_all)\n",
    "full_loader = DataLoader(full_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# 4. Modelo BERT Multilabel\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.pooler_output\n",
    "        x = self.dropout(cls_output)\n",
    "        return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BERTClassifier(num_labels=y.shape[1]).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 5. Entrenamiento con todo el dataset\n",
    "model.train()\n",
    "for epoch in range(15):\n",
    "    total_loss = 0\n",
    "    for batch in full_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(full_loader):.4f}\")\n",
    "\n",
    "# 6. Evaluación sobre los mismos datos (solo referencia interna)\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in full_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].cpu().numpy()\n",
    "        outputs = model(input_ids, attention_mask).cpu().numpy()\n",
    "        all_preds.append(outputs)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "y_pred = np.vstack(all_preds)\n",
    "y_true = np.vstack(all_labels)\n",
    "print(\"📈 MCAUC (referencia, mismo set de entrenamiento):\", roc_auc_score(y_true, y_pred, average='macro'))\n",
    "\n",
    "# 7. Predicción final para Kaggle y exportación\n",
    "dataTesting = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTesting.zip', index_col=0)\n",
    "dataTesting[\"input_text\"] = dataTesting[\"year\"].astype(str) + \" \" + dataTesting[\"title\"] + \": \" + dataTesting[\"plot\"]\n",
    "test_enc = tokenizer(list(dataTesting[\"input_text\"]), truncation=True, padding=True, max_length=384, return_tensors='pt')  # ✅ también aquí\n",
    "test_dataset = DataLoader(torch.utils.data.TensorDataset(test_enc['input_ids'], test_enc['attention_mask']), batch_size=32)\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for input_ids_batch, attn_mask_batch in test_dataset:\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        attn_mask_batch = attn_mask_batch.to(device)\n",
    "        pred_batch = model(input_ids_batch, attn_mask_batch).cpu().numpy()\n",
    "        preds.append(pred_batch)\n",
    "\n",
    "y_pred_test_final = np.vstack(preds)\n",
    "cols = ['p_' + genre for genre in mlb.classes_]\n",
    "res = pd.DataFrame(y_pred_test_final, index=dataTesting.index, columns=cols)\n",
    "res.to_csv('pred_genres_text_BERT_year_first.csv', index_label='ID')\n",
    "print(\"✅ Archivo generado para Kaggle: pred_genres_text_BERT_year_first.csv\")\n",
    "# 0.91258\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "854e31cfc40242c1bda54f5d17cfb939",
      "ff5bcd18717f4009b597177595ebf2d0",
      "53ab49ffebcf4b459ba3ead804a5c0f2",
      "4afd602522f6471bb2c0552b6ec4a5b5",
      "db101df139a64a5e9957b35bef2a2104",
      "8436614fcc1d4894b82890c16c735132",
      "454a281d67754f328d7683446305e6e3",
      "1e7d8b232f814808922c74bbebf527e6",
      "c156aa0679dd438eb75f5197a6d10fb8",
      "e1e183d1d7b1465ca64d076d31c5fda7",
      "ad22e332039841ba84bed266e3a36458",
      "93c0417e1d5246edb2dc03c1e30f020b",
      "3c4c8896cdfb428fa8f4d499fe882e97",
      "4a7e7d63f2604c20b181f4df9c723752",
      "c57b5e7b9dc648819080991ebdb7d541",
      "38b8ea6980824695b4b689c87f1bb1db",
      "5f9b50fd001d45ff9d1f7bb3c3ca6b7a",
      "00131fcdaa0c479681fee9a92faa2a14",
      "d84c93012f5048a8ad72f302c3b5c96d",
      "e19022e9b0414c6996dcde79b37fdc4e",
      "6e0ea2c64cbf44058d428aa1b68f5a48",
      "219c3b10607247f699882d25ca21da43",
      "f7688d266a74453b88d29d835958e553",
      "e29c2eb3f9fc4d21bb335d8f0dfe6d34",
      "7a294853b5104377a924e003d3a49a97",
      "0073120b9b884196af736fd2ba635a85",
      "57c27c32c43c4c15ac3d92d529c1ba4b",
      "fdf77ae5d7c946b4a90bfefa9211e0dd",
      "8790d7fa551e4785b63ca7632fbe07e6",
      "5353583163e34e4c939805b416f82ff0",
      "17d16091599a4394b9bb6cdf84dd1160",
      "cda58314911a4066a329b326882c9b2c",
      "f45d27763dd044fd9708dee10ff65217",
      "a6ce27881d9d480e85fa0edbb1b676a7",
      "1fd45531e39b4a1a99da8ba084652eca",
      "215da2d163ca4db198b23392196046c0",
      "fe1b073bb7f0422b987179ad7df5f9a1",
      "f4f284b7ada14f4ba35f190198910be2",
      "6acb6a06b0414b809f468d3e3dd02ea7",
      "b0eba37d523843e78fd6a3d524b5cd24",
      "7c62aa4cbfc148f19231bb2becbb2d6d",
      "c24544bc73924e938eff5ab7cb969675",
      "ce1235b11b424d80a1db0c0eb5e3528a",
      "972ca0ed844b40a98ee527832493821d",
      "1871670d8dad4e28b907612d13550d5f",
      "02904b3e08944754813b24e4d9bbf9f7",
      "3a1cfe4722fe4425a86cc860299f0dd2",
      "d7e4951ba00d41a7ad86c621b25350d9",
      "9080b2fb5d4041b3a6941a0a234e3655",
      "0ca00e36facf40c4abffdb3a1fcecec2",
      "48e585a8ab5d409381e33dd21580a20e",
      "022f4f3c974a4eaab9c58b71a544940d",
      "3aa9844e887c46ca9e9c6d9ab5edef0f",
      "9eef35e5029f4868b800e7b3867b26e1",
      "03609e8d3afe49978e9cd999bdaa4c8b",
      "753ce926c27c45209e8534806e3571b1",
      "e9bf85cf375b49d8aaeed9eb9c18342b",
      "1f0281577ee346958abc291ba2063e6c",
      "786f1389dc1a4584aebda268eddaff20",
      "fff739b6e66c446484a3a81299799c50",
      "ee4ff502faab4c64a1f9ba4f4d4fd5d2",
      "33a8361caad74338b3149901e0c5e05d",
      "cfcf48cb9995452baedfe921e2626a88",
      "c582cdb0873f4c498d85b4831e5046db",
      "cbdb21bd07e2473cba707de2517a3878",
      "ff46db4e13a245598be54db825a6aade",
      "ca3846eb477841638d21e70b70477188",
      "491b2624fb464692b6430fed4b0a5194",
      "839811f6d3814918af0a118e81d59287",
      "f8a2297f504d4c378f6c7d716de46aac",
      "e79df51d76d74e74b51ab7d83d88ac0d",
      "1a273675102f49e8878bdd7c764a52bf",
      "6095bd74659c436389b59ab7a11d0276",
      "da7ac34aab064fb7a15ac96d711383b8",
      "2384640cdf454e0d8bd52d881114ac2a",
      "d1bc13078b4248d1a2fb18cfa1114667",
      "05f4680438054ac1a637b8c986490094",
      "adc617caf21b44e2846e307de3417ac7",
      "5bae8375d20b4626be5f7463f75c89d9",
      "4515d7b4e59c4963a98ba013ee0b9d5b",
      "f2bc4ee580064b73b0c2295850a3aea1",
      "4681441ca6654a0c87a1b58c3402771f",
      "1ecf4663c1a142d59afcda949bae8ebc",
      "83a3281c039c4a1e8b03294d89f37e2c",
      "5a93636d0be848cd8deb88cfb806183f",
      "11d31b12ef204fafa7a80a4e6b80e028",
      "dd579aa3c848405c92a90a394f9ab459",
      "8e54052f825e4735a28f9ecc77b09b5f",
      "afd2d1b7b1024a89952b9561cf9a49ec",
      "3816dc3aa61244efb6e2cfba637bbc43",
      "3a68780b4feb4ab8b087fcbb31fb88b5",
      "966c6025cca043ef9af7a85d1a002249",
      "cf9d742280ff404380c63d92fbf9fddb",
      "2a034b5a4c6a402c8583551fd1f06d1c",
      "cf9ed26734f64cfbbe2a336e26650415",
      "354538f840844275bfa265382ab17257",
      "d7c40f8360dc4eec846af822edde5b9c",
      "a5c4f70fce5a45a68ae55f8d2534679a",
      "87b502f398cb40e98fddbe0829473e43",
      "caabbf8a90d043df9026b80c524ff02f",
      "d8a747c468b4481a9fd855dc1cc68f78",
      "7e8b02095ed4445594f9a472306716ca",
      "7806ad18dfa749189655f7fc88c5d62c",
      "1b41079cacdf4ab1b9c9cc09e69b0756",
      "b13fbbcbe42f43a0b05aef1a512c6a0a",
      "783dd66fec5743b68043819ea5ea4b6f",
      "011f6b4e2ef849289e4b9a9b108c00e3",
      "55765f3ea74146a096fb54b218158ef5",
      "c445b135fbc74c12b6d3fc774fb423e3",
      "86da948e3699408da39fd2e87f24c74b",
      "12d791fdb569482faf6b80a9e2587e7d",
      "674e6dfbca384fae80437e4d7f86ef35",
      "baed2bb6ea264305bf0a80bdd3161f84",
      "9c125af73ff24c71940b08b4135bbc9c",
      "1f3a132a338642c3b03866ae16626d2f",
      "120da1d1d0a045559c61e3d6392acf7e",
      "710790451a304f01aa4c06bf992010b8",
      "84327afc76f04beab94124a251a842dd",
      "4c7642b609434e4dab138ce1b911a986",
      "e5d25246ca1a4d07a85d24ed75d0fcb6",
      "ebdbfc6075e049889a12b0a760aff693",
      "8de62e8fcb4c4db787d99d28d9f216f4",
      "7c1082d8f1cd4f1db60274852c2191c3",
      "24adc81789234224ae13c16669c6ec79",
      "e03d956596e64935b4dde06e5f119b3e",
      "032617bafb034fc0bd610b459cf87771",
      "918f80fc64a04d02a2d808b55b354636",
      "4507bd8968d446eb95eadd01ef00e043",
      "eacfc73a0c044a358a29828041d479e1",
      "40cdc5ed5fe94b20a1b985308bcde427",
      "4521efcb93ef4e3587d62626cf5eff80",
      "eb2f1a26e2b645e78927e852852c8cf2",
      "e6a47b2b5a364c82a15f3de60ed6545b",
      "52f2e8b8f3d746d5b65dffc0036e2e9d",
      "1113155eccdf4cd88f14d13a619c16db",
      "73fb4e433ac94d46a61f8f774e6bd5c3",
      "cc4afe64828e4e889df465bd663f5a7e",
      "bb5ab6207ea64384a5a753953029b06f",
      "e53fa71fef8447019df31c575ecbfef9",
      "6de589763aac4481811c5d676ff56bc6",
      "501c107d97914f45b81856b1578b1524",
      "2d297357af8942aab22577fad576da29",
      "75b147030cab4046ae81c5b81386990d",
      "836569d4597f48eabec3dd47b24ff63d",
      "a42482d171a14f7d82af6fe03a7c0ce7",
      "dd218a84cd884ec9abbba9e0425598b5",
      "cc85332e55da4ddbbd1d3a367efc8e75",
      "a6bb569f6fe040fe8615a0d16982e279",
      "e0e6ac98acf649db89701733506bcbe2",
      "e79b2f245fb04782a6f2857a03a8abfb",
      "9a2c041dfb094896bcf7b69b79616574",
      "527c695646d54c1d8f1356ea1931c06d",
      "7ad11a3422004416b0d86e3315fd9d6e",
      "f2a2ba0c98184e1eb5a86fde0246d440",
      "6d06a702c4ba430fadd14a680449d3a9",
      "2130ac998a364c29b5053e5e0013cc83",
      "e5c1044bd3a4453a97f4277cc8b91bd6",
      "1103e3c4091447749f9bf6fde17d9034",
      "7a2475e2768a4a099f1c59aa16474a93",
      "4f27a96475154214a77713a76f197b28",
      "34bb54a0a9e447088ddfe99a619c06dc",
      "8d0793afd3404dda82b2bea3d0035aa8",
      "17b595dfb66b4a6cad0997c4dfa03ba7",
      "3a8d371540874f33829615a7e43cfb1f",
      "c9f5a9c81dc54e64823d59af50008acd",
      "b54d0f5b0ea042dd804bae17fe61d1e1",
      "44b0512af905459c88a0be1ed81f0bda",
      "b1f58f84a3124d278f8e60cc687700b6",
      "6b44090365e34f878153c31057626050",
      "e9f4d9bc2cc64b069b68f77a77ca1001",
      "4c4a554787eb4a53a44c5cd7401f069c",
      "0cee3e17cc48431495679d35ec0f3236",
      "da28d38706464af9bd131980bdd09ee0",
      "ee307b5b65fe45e39e4d4bb4bb657dc9",
      "091bcb64cfdb4a5c9c60430e16ab684a",
      "aa376dfc87514a40a3b843a11f9c3886",
      "7327a2c27a2b47d698d568fa172e9dd1",
      "bd742ab3be6b42648ec07c93ac9d5f83",
      "a6bd8aed28b4480eabcae119a635a9ab",
      "f84e09d810ee484f87a20affeab10b07",
      "5e490922db2745a98aecf2338b4ca72b",
      "3b6bbaee80a948d3a5f6d64b408577b4",
      "4c5bc90d401946fabdbf3ac6efea253a",
      "299b5f405c32491a884e0ae3fd7374ed",
      "af2072c27e5c420b9f8db980ded33ae8",
      "c022ae5c85ea4b799dc370e927837b9f",
      "f0a080371d8440438bd005bef7d2696b",
      "7218afba04384b568066d57eee9d4a70",
      "bedce0b30cb643d19c2d3ae5d52096b8",
      "67f747da201445fdb4929aba2482a840",
      "998b4de699594cc79a4b07388473889a",
      "3f0e12810b33465085de71c94a6394c1",
      "564c5cfe4b3e405a84fe41c916a5bc4a",
      "df1a9b2d19ec4571b41cf7cbf6a25d16",
      "e15d929a5f6940dea523c3069b69dce6",
      "f76629922c4f475590d43add2fbdd795",
      "af5e60b1d5b34bd7971591167db2acbf",
      "f02f6073b2a04f6fb5e5fc1bed4cdb73",
      "83638a1244214712bf73e51c3c9f5739",
      "e280055aa8b246febaa6bdd27f9827f1",
      "ae0a9c23986b459ba50f557deee57237",
      "82f8b90d690e498d94b0c2070b855ad8",
      "dea0a4eae49d4f26bcdb00c1671a7902",
      "74f845b7c94e49fb800220f48c963ed0",
      "9a1f8d1bc50546f5bb5e006b61342088",
      "cc8f464736984b8da5d8bc79d325890f",
      "3aa31f2fa17f478f8a8c67419cf28e2c",
      "622975d4e86c4ddbb5dbc6d9476462c2",
      "ed6191be4b9f495ca8fd07125f6b298e",
      "c3fbd64df8c8456781fad0e4e44ae247",
      "727fde2315154736a8f38df7cbe2d351",
      "e23ed2a59a544dcc9cf953903de1032a",
      "c81f9eba361e40fbaa6aea028b7e8bba",
      "e65c6a2fbd6e4928a1c0d9109bcbd8e8",
      "4d055e0486bd4020813ed93e3df66708",
      "8bfaa6846460436d9859ab0bfdc12e32",
      "a21d730b6a4d4768bcf9db8f05e29124",
      "34f0d5b82d874f45b36870ffe66303f8",
      "f8fe24d70a024df0b4e703f5c7d80142",
      "48a55356e0c847299f99878a7d95b416",
      "653aee4988f449ef93f49036922ff72a",
      "dad7b4d4687c4a74b77abb078eb5cc2f",
      "fab77d80b91f47c794f5d8a667a8cd5c",
      "438654d4f7bd414498768e08ed0a8e6b",
      "4aa650000f7a49608311b01907671b9a",
      "01778e3d3ff74edbbe0d0d705d4caf3e",
      "0dee75808f584fe996b4f379e67e4547",
      "24f677970b694a06bd276e144146d304",
      "c53b139bd55c4061ba05e2476d9db64a",
      "843824d0367f49539eff39a59b0a0d67",
      "553e644e85274434a7b617319316e162",
      "67464017f77943099500e8f8fbc366be",
      "68e74f8beb664bc09d83fdcf2e8ae278",
      "f3ba66f3390f470f99dc349c10dd82bd",
      "de7ee0d46e744de5a88de6925fde9ef6",
      "9f2aef403c474b47b095fa0e76900061",
      "03595c175f8e4faaa8af40b4a02a8816",
      "ab32319b1a614654bebb20cd83a49a99",
      "96735b53a36f42c095bf8f677442248a",
      "d8f2854e4ebf4197be661be9aa0ade8a",
      "c709f50743ef40e191a1e60fa8514f5d",
      "34810d7e72444806a437a2a3da3db3eb",
      "bfdc13df9cd24192aaa824b9f830e6ae",
      "cb7921eb01b944b98b798814c229125d",
      "951e0d3a18764076804870e9396cf701",
      "8ca762ef2de24095a4bc69b4065281a6",
      "d811ae9fc53c4344865e056492d6eb99",
      "79a441e7c1d9450baa4f32bddf408d64",
      "77a3788c30204cb1898776ff1fe38d19",
      "dd79352c30454e55bcbe48de6a81465a",
      "1db448e5e6974104bd6a4cf8967e6d5d",
      "137dc3d9be5041848a282e38d8e8eee8",
      "ae037fd83bce4388bf0142f98a8a9cd7",
      "a011cc1f411749efaa063a8e233b48e9",
      "f09654bf37cb4b1789ab0e7f6b4fda07",
      "a52b9cef2ae94ab19262dc403c945192",
      "133e638667424bd2813d00e987f8e01f",
      "03d58a00b2294d1cbdce75de4700aadf",
      "899d819448774b9ea35443533931c8b1",
      "2dade3beb90e49a791f265bcdbb2c5b7",
      "73df18598cf1437ea6a8adc870be65c3",
      "0e657bef56cf45f6b3325c57816bdb93",
      "26d5a7c8d08a498aa211841b30dfe40a",
      "9a8723ca0af349b1ad6cfb80ee649156",
      "1c53f8ea96d04378b778b72d9b98e568",
      "9d3ce88825c846b99d99c46c9cdc03a5",
      "cc795fb442d54eee881b896d4b193fd0",
      "95b5fa334316491d8a72944f532e4627",
      "e8193ef7f6ea43c5be712d3e7ea2f3b2",
      "fcfda24a04324a20a39dba1493fd7b8c",
      "69894d91315d4ed0a707493e159b3eea",
      "139d40139bf34193b2c1811084cf0855",
      "0d90d2b354d04931b3d46f5c28235863",
      "5b09e0318b734aa6b4ae7261bf824ae5",
      "3df5acd24cca4a49b167760a58f1d4dc"
     ]
    },
    "id": "A7TIbaprvjf7",
    "outputId": "ca771f75-c7a9-4730-8621-3fd92690a904"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\n",
      "🔄 Entrenando modelo: DeBERTa v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "854e31cfc40242c1bda54f5d17cfb939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93c0417e1d5246edb2dc03c1e30f020b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7688d266a74453b88d29d835958e553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6ce27881d9d480e85fa0edbb1b676a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1871670d8dad4e28b907612d13550d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/371M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.2951\n",
      "Epoch 2 completado. Loss promedio: 0.2161\n",
      "Epoch 3 completado. Loss promedio: 0.1850\n",
      "📈 MCAUC (DeBERTa v3): 0.89357\n",
      "\n",
      "🔄 Entrenando modelo: XLM-RoBERTa\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "753ce926c27c45209e8534806e3571b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca3846eb477841638d21e70b70477188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adc617caf21b44e2846e307de3417ac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afd2d1b7b1024a89952b9561cf9a49ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caabbf8a90d043df9026b80c524ff02f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.3284\n",
      "Epoch 2 completado. Loss promedio: 0.2678\n",
      "Epoch 3 completado. Loss promedio: 0.2315\n",
      "📈 MCAUC (XLM-RoBERTa): 0.82531\n",
      "\n",
      "🔄 Entrenando modelo: BERT cased\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12d791fdb569482faf6b80a9e2587e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8de62e8fcb4c4db787d99d28d9f216f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a47b2b5a364c82a15f3de60ed6545b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "836569d4597f48eabec3dd47b24ff63d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d06a702c4ba430fadd14a680449d3a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.3140\n",
      "Epoch 2 completado. Loss promedio: 0.2211\n",
      "Epoch 3 completado. Loss promedio: 0.1810\n",
      "📈 MCAUC (BERT cased): 0.88235\n",
      "\n",
      "🔄 Entrenando modelo: Google BERT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b54d0f5b0ea042dd804bae17fe61d1e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7327a2c27a2b47d698d568fa172e9dd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7218afba04384b568066d57eee9d4a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83638a1244214712bf73e51c3c9f5739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3fbd64df8c8456781fad0e4e44ae247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.3234\n",
      "Epoch 2 completado. Loss promedio: 0.2272\n",
      "Epoch 3 completado. Loss promedio: 0.1843\n",
      "📈 MCAUC (Google BERT): 0.88813\n",
      "\n",
      "🔄 Entrenando modelo: MobileBERT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653aee4988f449ef93f49036922ff72a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/847 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67464017f77943099500e8f8fbc366be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfdc13df9cd24192aaa824b9f830e6ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a011cc1f411749efaa063a8e233b48e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/147M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c53f8ea96d04378b778b72d9b98e568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/147M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 45.9943\n",
      "Epoch 2 completado. Loss promedio: 45.5143\n",
      "Epoch 3 completado. Loss promedio: 45.3794\n",
      "📈 MCAUC (MobileBERT): 0.50000\n",
      "\n",
      "🏁 Resultados finales:\n",
      "DeBERTa v3          : 0.89357\n",
      "Google BERT         : 0.88813\n",
      "BERT cased          : 0.88235\n",
      "XLM-RoBERTa         : 0.82531\n",
      "MobileBERT          : 0.50000\n"
     ]
    }
   ],
   "source": [
    "# ✅ Instalar librerías necesarias\n",
    "!pip install transformers scikit-learn torch pandas --quiet\n",
    "\n",
    "# 📚 Importar librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ⚙️ Modelos a evaluar\n",
    "model_list = [\n",
    "    (\"microsoft/deberta-v3-base\", \"DeBERTa v3\"),\n",
    "    (\"xlm-roberta-base\", \"XLM-RoBERTa\"),\n",
    "    (\"bert-base-cased\", \"BERT cased\"),\n",
    "    (\"google-bert/bert-base-uncased\", \"Google BERT\"),\n",
    "    (\"google/mobilebert-uncased\", \"MobileBERT\")\n",
    "]\n",
    "\n",
    "# 📄 Cargar y preparar datos\n",
    "df = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', index_col=0)\n",
    "df['genres'] = df['genres'].apply(ast.literal_eval)\n",
    "df[\"input_text\"] = df[\"title\"] + \" (\" + df[\"year\"].astype(str) + \"): \" + df[\"plot\"]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['genres'])\n",
    "\n",
    "# 🧾 Dataset personalizado\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 🧪 Dividir en train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['input_text'], y, test_size=0.33, random_state=42)\n",
    "\n",
    "# 🧠 Clase para cada modelo\n",
    "class CustomClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        if hasattr(outputs, \"pooler_output\"):\n",
    "            x = outputs.pooler_output\n",
    "        else:\n",
    "            x = outputs.last_hidden_state[:, 0]  # CLS token\n",
    "        x = self.dropout(x)\n",
    "        return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "# 🚀 Entrenar y evaluar cada modelo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "results = []\n",
    "\n",
    "for model_name, display_name in model_list:\n",
    "    print(f\"\\n🔄 Entrenando modelo: {display_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    train_dataset = MovieDataset(X_train, y_train, tokenizer)\n",
    "    test_dataset = MovieDataset(X_test, y_test, tokenizer)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "    model = CustomClassifier(model_name, y.shape[1]).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(3):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # 🎯 Evaluación\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].cpu().numpy()\n",
    "            outputs = model(input_ids, attention_mask).cpu().numpy()\n",
    "            all_preds.append(outputs)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    y_pred = np.vstack(all_preds)\n",
    "    y_true = np.vstack(all_labels)\n",
    "    score = roc_auc_score(y_true, y_pred, average='macro')\n",
    "    results.append((display_name, score))\n",
    "    print(f\"📈 MCAUC ({display_name}): {score:.5f}\")\n",
    "\n",
    "# 📊 Mostrar resumen final\n",
    "results.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"\\n🏁 Resultados finales:\")\n",
    "for name, score in results:\n",
    "    print(f\"{name:20s}: {score:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 654,
     "referenced_widgets": [
      "908b4d4b98a94990a274cc05f2c6de53",
      "816bb64df5c54ef99c5c64f0738e952c",
      "325febc440a04b87b7ff6d7a7deaf23d",
      "459582663bdd4289a606249913c4a446",
      "fd9fda2415d344c99cd333b79cbdb64e",
      "97375db3a8ef4b0cafc99bb0e69dc401",
      "8c4b849a4ba14e54b5070a4b1c2f3102",
      "41aefd7ab2cf4b91b0f9d9f4d6445333",
      "e23109be295647abb1bbcd25921899bf",
      "18f426ffad4c4d8a898b0d8f2de161e7",
      "49d97f5a2b8442bfa8214fe0314cdede",
      "d3ae0768136449e4a528419dc6d74799",
      "93358cd4be0d46e696ea8dd07ed1fbca",
      "ada783addf63491b93cd53ca207ba22c",
      "c59e69ad98a04fd4bf76c41d04b7701e",
      "a86fba34813b47d89c85334814278aa0",
      "1ef580394627496288ebaa1baa2782c8",
      "161b86b6c3f744688bfca8f67cdda809",
      "13a50e1fda63487682f84532d75f96ea",
      "60fc83eddef549828832d35f7f96f723",
      "5303a559400e4146921795953ac97e3c",
      "ff4f7e1f8caf426b9602a2a16d130433",
      "de009bc3ec0e43199dcf03d65f6e1ea5",
      "6cb03863222643faa66c794004dff174",
      "ec2e70a20407432fb260a93818883fb9",
      "8ce6b0ed744d420c9a3b242347a126f9",
      "6ad43681f6574d61a3f14c139d6bbc39",
      "3b59a0b29c2e4272972753efecbee386",
      "6a4f7c32a56c487a9a17cd4b875593e2",
      "1930c7b64c4447f9a9ddb3b8bad8764e",
      "4447b70c7ecf4408aa1a00f0f88046eb",
      "bc274a5101a94c5ba01f8a0f6f45cd30",
      "55b84bbec71a4090b137823b28a6b0bf",
      "6d3ffe21f6f743a0a2d76a6ab70865d8",
      "6a5e76088a40489db205ca3d85e530ea",
      "ab83db246a654c9587813df1965c8423",
      "1ac43d2b9d274d8c90ad7bd8f568a952",
      "4abb2f5c3ea3443bbe5fa9350b19ab90",
      "bc2b6b7ffa3e44bd9e06e57d99f8592c",
      "0b550d73f7d14d24acaab876b5356b15",
      "b90654029e4c42de98609958f17a21ce",
      "aa499ba0199647c2a2574ad171cd3d24",
      "8be8feb1ef7a497883ebf49ccbb2ab81",
      "10dd10f05f754896947bf6ec9348bbf2",
      "74085233f7fe4816aec4c03c3d66b547",
      "63e5d027797647c495d2b7ee43ba62ed",
      "9cd16f6274574edf9b0beb2002ceec51",
      "bcfaa8ecdbe9428c949bf4204161ce55",
      "0f17b3ddacb149a7bafddad9aed56a94",
      "f9bf7b8c86744cd99fac1c87b5798702",
      "636344a668814f26ac0db117e2c27c3c",
      "0d97c9d73f934d30ad41a524dd0b50a5",
      "a85c7fede9fe46549be73529e0542e93",
      "4ccfbce2d0c5484ea9cf1b81836db4e0",
      "fc4d23071ee847cb8929a2f8d06d7ad7"
     ]
    },
    "id": "QNcLf9PbWkpx",
    "outputId": "c3104eae-c47c-4d14-8f53-2f9bd461273d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "908b4d4b98a94990a274cc05f2c6de53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3ae0768136449e4a528419dc6d74799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de009bc3ec0e43199dcf03d65f6e1ea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d3ffe21f6f743a0a2d76a6ab70865d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74085233f7fe4816aec4c03c3d66b547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/371M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.2725\n",
      "Epoch 2 completado. Loss promedio: 0.1988\n",
      "Epoch 3 completado. Loss promedio: 0.1686\n",
      "Epoch 4 completado. Loss promedio: 0.1467\n",
      "Epoch 5 completado. Loss promedio: 0.1287\n",
      "Epoch 6 completado. Loss promedio: 0.1143\n",
      "Epoch 7 completado. Loss promedio: 0.1002\n",
      "Epoch 8 completado. Loss promedio: 0.0887\n",
      "Epoch 9 completado. Loss promedio: 0.0776\n",
      "Epoch 10 completado. Loss promedio: 0.0683\n",
      "Epoch 11 completado. Loss promedio: 0.0602\n",
      "Epoch 12 completado. Loss promedio: 0.0516\n",
      "Epoch 13 completado. Loss promedio: 0.0445\n",
      "Epoch 14 completado. Loss promedio: 0.0388\n",
      "Epoch 15 completado. Loss promedio: 0.0346\n",
      "📈 MCAUC (referencia, mismo set de entrenamiento): 0.9993652272846125\n",
      "✅ Archivo generado para Kaggle: pred_genres_text_DeBERTa_full.csv\n"
     ]
    }
   ],
   "source": [
    "# 🛠️ Instalar librerías necesarias\n",
    "!pip install transformers scikit-learn torch pandas --quiet\n",
    "\n",
    "# 📚 Importar librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1. Cargar y preparar datos\n",
    "df = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', index_col=0)\n",
    "df['genres'] = df['genres'].apply(ast.literal_eval)\n",
    "df[\"input_text\"] = df[\"title\"] + \" (\" + df[\"year\"].astype(str) + \"): \" + df[\"plot\"]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['genres'])\n",
    "\n",
    "# 2. Tokenizador y Dataset\n",
    "model_name = \"microsoft/deberta-v3-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 3. Usar todo el dataset para entrenamiento\n",
    "X_all, y_all = df['input_text'], y\n",
    "full_dataset = MovieDataset(X_all, y_all)\n",
    "full_loader = DataLoader(full_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# 4. Modelo DeBERTa v3 Multilabel\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # usar el token CLS\n",
    "        x = self.dropout(cls_output)\n",
    "        return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BERTClassifier(num_labels=y.shape[1]).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 5. Entrenamiento\n",
    "model.train()\n",
    "for epoch in range(15):\n",
    "    total_loss = 0\n",
    "    for batch in full_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(full_loader):.4f}\")\n",
    "\n",
    "# 6. Evaluación (solo referencia)\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in full_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].cpu().numpy()\n",
    "        outputs = model(input_ids, attention_mask).cpu().numpy()\n",
    "        all_preds.append(outputs)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "y_pred = np.vstack(all_preds)\n",
    "y_true = np.vstack(all_labels)\n",
    "print(\"📈 MCAUC (referencia, mismo set de entrenamiento):\", roc_auc_score(y_true, y_pred, average='macro'))\n",
    "\n",
    "# 7. Predicción final para Kaggle y exportación\n",
    "dataTesting = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTesting.zip', index_col=0)\n",
    "dataTesting[\"input_text\"] = dataTesting[\"title\"] + \" (\" + dataTesting[\"year\"].astype(str) + \"): \" + dataTesting[\"plot\"]\n",
    "test_enc = tokenizer(list(dataTesting[\"input_text\"]), truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
    "test_dataset = DataLoader(torch.utils.data.TensorDataset(test_enc['input_ids'], test_enc['attention_mask']), batch_size=32)\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for input_ids_batch, attn_mask_batch in test_dataset:\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        attn_mask_batch = attn_mask_batch.to(device)\n",
    "        pred_batch = model(input_ids_batch, attn_mask_batch).cpu().numpy()\n",
    "        preds.append(pred_batch)\n",
    "\n",
    "y_pred_test_final = np.vstack(preds)\n",
    "cols = ['p_' + genre for genre in mlb.classes_]\n",
    "res = pd.DataFrame(y_pred_test_final, index=dataTesting.index, columns=cols)\n",
    "res.to_csv('pred_genres_text_DeBERTa_full.csv', index_label='ID')\n",
    "print(\"✅ Archivo generado para Kaggle: pred_genres_text_DeBERTa_full.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544,
     "referenced_widgets": [
      "d8c95971b8064c90be3dbbc2811080da",
      "5ee935e60ee94e689b65884b6a17e349",
      "8866862a489d4c169d0960e7adc7dbdd",
      "cc595313992d43b390632d73535bf8ed",
      "3644ddcba47a46ffa77f249c7bfc94c6",
      "86f56b3069cd41318e1e5d2295c1ee80",
      "fb8675b3eba94700832ad63e3c86fdc0",
      "27a1577fada0496cbc429bceb5b57116",
      "42f4f8bbda4d48c491d235d456247f52",
      "5676c5ea2a6f491e9f19bfd64b156806",
      "f41501595a7240b49b18a2a820c2a175",
      "ce0a97e7bd05460c9b05e0421bbee111",
      "67245e4d382b4d19800e2534edb867ac",
      "ebdec55b2a5b40f09fa633e7b7e2b547",
      "1b962be0d4554bbe8e2bb9c55c41fed6",
      "d3fd49dbbc3349febd134a5627355767",
      "fed16d76ceb64fbc8deaf793f7fba208",
      "373ba6d8f41e438dbf4eb7fe38d7c769",
      "a927924826684c059b8b1c664567ece2",
      "bbff077ef9ab47b88ea752745c313f17",
      "684160159c1c424395e19527339e36b8",
      "78822f8c1926466698e8e052b640a732",
      "876691a0f6e741dd9674ef897c721661",
      "2bbdde2e1d4e4d8cbe750c4cc69a8f81",
      "e67f0596d414449d9957b35ece2e0009",
      "d8eb8d3492044707801a4ae8498d3801",
      "2545d8f0032947e68427c606a407ea5e",
      "973e7c09d0a04b24a5c1a0803a685cba",
      "10a267fab94441c4b51bd430392dadce",
      "6a11fe1bbc7e46dca7169a98b5031c43",
      "475847e41a384cba91155fb13065197e",
      "72f31655b4bf4460a5501b0f30ea84b0",
      "06cdb2940dd547c08e6a84bf3b45d586",
      "6828d16923d44b648f6cb0279584b5ec",
      "ca4d34289e5048cba2429c6cabbddb15",
      "f26a0e8ca58c4f4ba7357eaca7d40b3c",
      "396ef48efb234f3c8eb97f91b2baac19",
      "76e290fbe1d54cd18729db8c76118eaa",
      "7d694047219a4d3ba7a19be4419ec779",
      "bc1cf8a0ef8f4942bceaf5344a6cc7c3",
      "880540815d8347e0a2bae68d2ecd73d1",
      "dfc905a881c24fe5a10d82241066e36a",
      "145804e7cc684f599e901e87bf75c1cf",
      "187359842ef54b8da25535b094efde42",
      "1a48e16084f44abb962fbfae3b671dae",
      "ae9c9b15c88c4411af65b26ea7ff5053",
      "201ab12e3d574e8ab042755b1757c95c",
      "3f701eef5e08495e82068660c4199dac",
      "d2e1536cc7b14b2abb41e97fd9a1457c",
      "e9ac4b51f4d442648e6af17b5a4e816f",
      "9c46e7e375194658ac56e75dfc683454",
      "c5b1f60f97df4a1d9b798e5d5f0d4c51",
      "2cf2d07e9ce6491895a4e00e6a10b0ea",
      "61f5592d0ac54e029838f95bedc52beb",
      "ab79120c3b0747209c9c024c451f66a4"
     ]
    },
    "id": "-5FIHxyQCvcn",
    "outputId": "ca70eecf-ba23-46fd-dcd0-b609ae1eebe7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c95971b8064c90be3dbbc2811080da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce0a97e7bd05460c9b05e0421bbee111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/578 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "876691a0f6e741dd9674ef897c721661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6828d16923d44b648f6cb0279584b5ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/286M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a48e16084f44abb962fbfae3b671dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/286M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.2873\n",
      "Epoch 2 completado. Loss promedio: 0.2062\n",
      "Epoch 3 completado. Loss promedio: 0.1776\n",
      "Epoch 4 completado. Loss promedio: 0.1559\n",
      "Epoch 5 completado. Loss promedio: 0.1387\n",
      "Epoch 6 completado. Loss promedio: 0.1241\n",
      "Epoch 7 completado. Loss promedio: 0.1110\n",
      "Epoch 8 completado. Loss promedio: 0.0983\n",
      "Epoch 9 completado. Loss promedio: 0.0873\n",
      "Epoch 10 completado. Loss promedio: 0.0776\n",
      "Epoch 11 completado. Loss promedio: 0.0691\n",
      "Epoch 12 completado. Loss promedio: 0.0606\n",
      "Epoch 13 completado. Loss promedio: 0.0549\n",
      "Epoch 14 completado. Loss promedio: 0.0473\n",
      "Epoch 15 completado. Loss promedio: 0.0417\n",
      "📈 MCAUC (referencia, mismo set de entrenamiento): 0.9985977470867393\n",
      "✅ Archivo generado para Kaggle: pred_genres_text_DeBERTa.csv\n"
     ]
    }
   ],
   "source": [
    "# ✅ Instalar librerías necesarias\n",
    "!pip install transformers scikit-learn torch pandas --quiet\n",
    "\n",
    "# 📚 Importar librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1. Cargar y preparar datos\n",
    "df = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', index_col=0)\n",
    "df['genres'] = df['genres'].apply(ast.literal_eval)\n",
    "df[\"input_text\"] = df[\"title\"] + \" (\" + df[\"year\"].astype(str) + \"): \" + df[\"plot\"]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['genres'])\n",
    "\n",
    "# 2. Tokenizador y Dataset\n",
    "model_name = \"microsoft/deberta-v3-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 3. Usar todo el dataset para entrenamiento y validación\n",
    "X_all, y_all = df['input_text'], y\n",
    "full_dataset = MovieDataset(X_all, y_all)\n",
    "full_loader = DataLoader(full_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# 4. Modelo DeBERTa Multilabel\n",
    "class DeBERTaClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.deberta = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.deberta.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Reemplazamos pooler_output por CLS token\n",
    "        x = self.dropout(outputs.last_hidden_state[:, 0, :])\n",
    "        return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DeBERTaClassifier(num_labels=y.shape[1]).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 5. Entrenamiento\n",
    "model.train()\n",
    "for epoch in range(15):\n",
    "    total_loss = 0\n",
    "    for batch in full_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(full_loader):.4f}\")\n",
    "\n",
    "# 6. Evaluación sobre los mismos datos (solo referencia interna)\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in full_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].cpu().numpy()\n",
    "        outputs = model(input_ids, attention_mask).cpu().numpy()\n",
    "        all_preds.append(outputs)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "y_pred = np.vstack(all_preds)\n",
    "y_true = np.vstack(all_labels)\n",
    "print(\"📈 MCAUC (referencia, mismo set de entrenamiento):\", roc_auc_score(y_true, y_pred, average='macro'))\n",
    "\n",
    "# 7. Predicción final para Kaggle y exportación\n",
    "dataTesting = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTesting.zip', index_col=0)\n",
    "dataTesting[\"input_text\"] = dataTesting[\"title\"] + \" (\" + dataTesting[\"year\"].astype(str) + \"): \" + dataTesting[\"plot\"]\n",
    "test_enc = tokenizer(list(dataTesting[\"input_text\"]), truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
    "test_dataset = DataLoader(torch.utils.data.TensorDataset(test_enc['input_ids'], test_enc['attention_mask']), batch_size=32)\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for input_ids_batch, attn_mask_batch in test_dataset:\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        attn_mask_batch = attn_mask_batch.to(device)\n",
    "        pred_batch = model(input_ids_batch, attn_mask_batch).cpu().numpy()\n",
    "        preds.append(pred_batch)\n",
    "\n",
    "y_pred_test_final = np.vstack(preds)\n",
    "cols = ['p_' + genre for genre in mlb.classes_]\n",
    "res = pd.DataFrame(y_pred_test_final, index=dataTesting.index, columns=cols)\n",
    "res.to_csv('pred_genres_text_DeBERTa.csv', index_label='ID')\n",
    "print(\"✅ Archivo generado para Kaggle: pred_genres_text_DeBERTa.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "eddf88a3d06e433b97482320f37adbc9",
      "b991319c4a974d4cac9cc92e4b0f47aa",
      "b12ee1fe1cbf40dfad4b955b788e4ec9",
      "2b4a223df6b84cfebdc8876bdd39672d",
      "93ca04394a4b4a6e9b0febe85577cd01",
      "33fe511321434ed8b774c8000a0d91e8",
      "d3d98025c7bf49c1aa2bce4db91d9245",
      "401d14608a3f468b98613d5944c45721",
      "e55bac1556be4fb194be1796b9d47429",
      "14054248f2cf4fe5aea708202451338b",
      "81a0c8b693ba4dc8a3050e388bd033ec",
      "df805d1572744a5cbe971c0688983c02",
      "1cb7d3a28b0d4a6884a0c1b9c45d8b66",
      "2cc599ec93954af1b90de117ccfe4a2f",
      "9b9aeb95292245109f0b7bb64d84bcff",
      "e10e91cb7bd94f9caedff8d78ad6b30a",
      "08c544362f9a4046857b180987d4147c",
      "aa507a4dc9da46ec986bead54f462d24",
      "534538f9e3354bd98f27b084dad5b258",
      "812bc4f1cf6743f4bbf83acc2c895038",
      "b39535e817cc4b5087b510337398fceb",
      "943fc1a3b010407f9eedbe1d485b9952",
      "807bb6b4b6564653b85780ba44a7df28",
      "7844102f015f42a982c46f74a7cd23a5",
      "142d3c7687444ec39a86ad356818fd4c",
      "75affb0a298549608872db0dabe906cf",
      "9cefccb14e0242178c7a2ff30f0c3320",
      "8b4bb44aea00455b8f557d5b4b075ff1",
      "39f749b0f8a0416c9cfe6ae90cd3e03b",
      "087473848b01455982be298b54352241",
      "b0651349458a440d8d761eca512e64bf",
      "21ba3571a070429c9dfa17ad7917b350",
      "73f11fdb46ed458e8ef52a810f45e77f",
      "09743020d07c49cf8dd12342e9158d71",
      "c71f9267e11b41949651f04dfeb10b0e",
      "f50d3d7ba7be4aa8860dc2db4c42e144",
      "358588ec77224dcaa7876603e455ea07",
      "ffe131dcc874401c8caad3c801004188",
      "b8f1098a6b274c36a0d7a2c884c64896",
      "bbe554d0214945cf9ed7b0c94128407f",
      "bfccc01c226247e18eab54c11488cacc",
      "35206a0c48da4e3caf5bd2e89cad7dfe",
      "d0d76f04f6bb4a0d8076a0d0922ecaad",
      "ffc33e37375d44428aad16d98c36ed0f",
      "1d934d0abf6f4d89bbeb3ddbaaaa7537",
      "7b0537d986a746269f7b7b8fd1f171c6",
      "5db26e1e22b94e14920f459c45b206b5",
      "5ac992d24fd24bd8ab390f3fa959eda1",
      "96f34e92b4d747ebb50ce3b99fce6f78",
      "0f3ed4e8c3a440fa866312dde95d50c2",
      "79de1b02fcea486d9f97a14d2f33784c",
      "7eecd8d80c504fa4a779cdedd9087744",
      "821ddfa5f24542839edb13888fe7aba7",
      "b303e5dc8d68465fb9351541f2b8d740",
      "060abe8a079e422b999825a2ee21e582"
     ]
    },
    "id": "xBHjTopQdnOR",
    "outputId": "5ace5671-4e11-4a41-f28f-f4ad48e33447"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eddf88a3d06e433b97482320f37adbc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df805d1572744a5cbe971c0688983c02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "807bb6b4b6564653b85780ba44a7df28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09743020d07c49cf8dd12342e9158d71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d934d0abf6f4d89bbeb3ddbaaaa7537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.3216\n",
      "📈 MCAUC Epoch 1: 0.79803\n",
      "Epoch 2 completado. Loss promedio: 0.2249\n",
      "📈 MCAUC Epoch 2: 0.86380\n",
      "Epoch 3 completado. Loss promedio: 0.1825\n",
      "📈 MCAUC Epoch 3: 0.89083\n",
      "Epoch 4 completado. Loss promedio: 0.1514\n",
      "📈 MCAUC Epoch 4: 0.89456\n",
      "Epoch 5 completado. Loss promedio: 0.1273\n",
      "📈 MCAUC Epoch 5: 0.89729\n",
      "Epoch 6 completado. Loss promedio: 0.1072\n",
      "📈 MCAUC Epoch 6: 0.89474\n",
      "Epoch 7 completado. Loss promedio: 0.0915\n",
      "📈 MCAUC Epoch 7: 0.89810\n",
      "Epoch 8 completado. Loss promedio: 0.0781\n",
      "📈 MCAUC Epoch 8: 0.90008\n",
      "Epoch 9 completado. Loss promedio: 0.0663\n",
      "📈 MCAUC Epoch 9: 0.90120\n",
      "Epoch 10 completado. Loss promedio: 0.0570\n",
      "📈 MCAUC Epoch 10: 0.89981\n",
      "Epoch 11 completado. Loss promedio: 0.0487\n",
      "📈 MCAUC Epoch 11: 0.89945\n",
      "Epoch 12 completado. Loss promedio: 0.0425\n",
      "📈 MCAUC Epoch 12: 0.90124\n",
      "Epoch 13 completado. Loss promedio: 0.0368\n",
      "📈 MCAUC Epoch 13: 0.90163\n",
      "Epoch 14 completado. Loss promedio: 0.0313\n",
      "📈 MCAUC Epoch 14: 0.89975\n",
      "Epoch 15 completado. Loss promedio: 0.0272\n",
      "📈 MCAUC Epoch 15: 0.90040\n",
      "Epoch 16 completado. Loss promedio: 0.0237\n",
      "📈 MCAUC Epoch 16: 0.90063\n",
      "Epoch 17 completado. Loss promedio: 0.0209\n",
      "📈 MCAUC Epoch 17: 0.90160\n",
      "Epoch 18 completado. Loss promedio: 0.0189\n",
      "📈 MCAUC Epoch 18: 0.89850\n",
      "Epoch 19 completado. Loss promedio: 0.0165\n",
      "📈 MCAUC Epoch 19: 0.90309\n",
      "Epoch 20 completado. Loss promedio: 0.0149\n",
      "📈 MCAUC Epoch 20: 0.90268\n",
      "Epoch 21 completado. Loss promedio: 0.0133\n",
      "📈 MCAUC Epoch 21: 0.90100\n",
      "Epoch 22 completado. Loss promedio: 0.0125\n",
      "📈 MCAUC Epoch 22: 0.90081\n",
      "Epoch 23 completado. Loss promedio: 0.0108\n",
      "📈 MCAUC Epoch 23: 0.89955\n",
      "Epoch 24 completado. Loss promedio: 0.0100\n",
      "📈 MCAUC Epoch 24: 0.89944\n",
      "Epoch 25 completado. Loss promedio: 0.0096\n",
      "📈 MCAUC Epoch 25: 0.90125\n",
      "Epoch 26 completado. Loss promedio: 0.0084\n",
      "📈 MCAUC Epoch 26: 0.90157\n",
      "Epoch 27 completado. Loss promedio: 0.0076\n",
      "📈 MCAUC Epoch 27: 0.90026\n",
      "Epoch 28 completado. Loss promedio: 0.0070\n",
      "📈 MCAUC Epoch 28: 0.89746\n",
      "Epoch 29 completado. Loss promedio: 0.0069\n",
      "📈 MCAUC Epoch 29: 0.89862\n",
      "Epoch 30 completado. Loss promedio: 0.0072\n",
      "📈 MCAUC Epoch 30: 0.89708\n"
     ]
    }
   ],
   "source": [
    "# 🛠️ Instalar librerías necesarias (si aún no están)\n",
    "# !pip install transformers scikit-learn torch pandas --quiet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1. Cargar y preparar datos\n",
    "df = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', index_col=0)\n",
    "df['genres'] = df['genres'].apply(ast.literal_eval)\n",
    "df[\"input_text\"] = df[\"title\"] + \" (\" + df[\"year\"].astype(str) + \"): \" + df[\"plot\"]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['genres'])\n",
    "\n",
    "# 2. Tokenizador y Dataset\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 3. División train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['input_text'], y, test_size=0.33, random_state=42)\n",
    "train_dataset = MovieDataset(X_train, y_train)\n",
    "test_dataset = MovieDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# 4. Modelo BERT Multilabel\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.pooler_output\n",
    "        x = self.dropout(cls_output)\n",
    "        return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BERTClassifier(num_labels=y.shape[1]).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 5. Entrenamiento y evaluación por época\n",
    "for epoch in range(30):  # Cambia aquí si quieres más o menos épocas\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Evaluación por época\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].cpu().numpy()\n",
    "            outputs = model(input_ids, attention_mask).cpu().numpy()\n",
    "            all_preds.append(outputs)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    y_pred = np.vstack(all_preds)\n",
    "    y_true = np.vstack(all_labels)\n",
    "    mauc = roc_auc_score(y_true, y_pred, average='macro')\n",
    "    print(f\"📈 MCAUC Epoch {epoch+1}: {mauc:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3goUfd2a1wzn",
    "outputId": "b146dc8e-9e25-4d9f-cbb3-e0f9b751b36a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.3276\n",
      "📈 MCAUC Epoch 1: 0.80167\n",
      "Epoch 2 completado. Loss promedio: 0.2281\n",
      "📈 MCAUC Epoch 2: 0.86653\n",
      "Epoch 3 completado. Loss promedio: 0.1850\n",
      "📈 MCAUC Epoch 3: 0.88237\n",
      "Epoch 4 completado. Loss promedio: 0.1537\n",
      "📈 MCAUC Epoch 4: 0.88970\n",
      "Epoch 5 completado. Loss promedio: 0.1287\n",
      "📈 MCAUC Epoch 5: 0.89749\n",
      "Epoch 6 completado. Loss promedio: 0.1082\n",
      "📈 MCAUC Epoch 6: 0.89461\n",
      "Epoch 7 completado. Loss promedio: 0.0922\n",
      "📈 MCAUC Epoch 7: 0.89329\n",
      "Epoch 8 completado. Loss promedio: 0.0788\n",
      "📈 MCAUC Epoch 8: 0.89797\n",
      "Epoch 9 completado. Loss promedio: 0.0674\n",
      "📈 MCAUC Epoch 9: 0.89646\n",
      "Epoch 10 completado. Loss promedio: 0.0580\n",
      "📈 MCAUC Epoch 10: 0.89943\n",
      "Epoch 11 completado. Loss promedio: 0.0502\n",
      "📈 MCAUC Epoch 11: 0.89765\n",
      "Epoch 12 completado. Loss promedio: 0.0434\n",
      "📈 MCAUC Epoch 12: 0.89913\n",
      "Epoch 13 completado. Loss promedio: 0.0379\n",
      "📈 MCAUC Epoch 13: 0.89784\n",
      "Epoch 14 completado. Loss promedio: 0.0322\n",
      "📈 MCAUC Epoch 14: 0.89948\n",
      "Epoch 15 completado. Loss promedio: 0.0281\n",
      "📈 MCAUC Epoch 15: 0.89679\n",
      "Epoch 16 completado. Loss promedio: 0.0245\n",
      "📈 MCAUC Epoch 16: 0.89974\n",
      "Epoch 17 completado. Loss promedio: 0.0216\n",
      "📈 MCAUC Epoch 17: 0.89651\n",
      "Epoch 18 completado. Loss promedio: 0.0191\n",
      "📈 MCAUC Epoch 18: 0.90219\n",
      "Epoch 19 completado. Loss promedio: 0.0166\n",
      "📈 MCAUC Epoch 19: 0.89962\n",
      "Epoch 20 completado. Loss promedio: 0.0148\n",
      "📈 MCAUC Epoch 20: 0.89900\n",
      "Epoch 21 completado. Loss promedio: 0.0133\n",
      "📈 MCAUC Epoch 21: 0.90341\n",
      "Epoch 22 completado. Loss promedio: 0.0120\n",
      "📈 MCAUC Epoch 22: 0.90371\n",
      "Epoch 23 completado. Loss promedio: 0.0107\n",
      "📈 MCAUC Epoch 23: 0.90532\n",
      "Epoch 24 completado. Loss promedio: 0.0092\n",
      "📈 MCAUC Epoch 24: 0.90340\n",
      "Epoch 25 completado. Loss promedio: 0.0091\n",
      "📈 MCAUC Epoch 25: 0.90112\n",
      "Epoch 26 completado. Loss promedio: 0.0082\n",
      "📈 MCAUC Epoch 26: 0.89882\n",
      "Epoch 27 completado. Loss promedio: 0.0076\n",
      "📈 MCAUC Epoch 27: 0.89903\n",
      "Epoch 28 completado. Loss promedio: 0.0071\n",
      "📈 MCAUC Epoch 28: 0.90067\n",
      "Epoch 29 completado. Loss promedio: 0.0064\n",
      "📈 MCAUC Epoch 29: 0.90252\n",
      "Epoch 30 completado. Loss promedio: 0.0057\n",
      "📈 MCAUC Epoch 30: 0.89937\n"
     ]
    }
   ],
   "source": [
    "# 🛠️ Instalar librerías necesarias\n",
    "# !pip install transformers scikit-learn torch pandas --quiet\n",
    "\n",
    "# 📚 Importar librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1. Cargar y preparar datos\n",
    "df = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', index_col=0)\n",
    "df['genres'] = df['genres'].apply(ast.literal_eval)\n",
    "df[\"input_text\"] = df[\"plot\"] + \" \" + df[\"title\"] + \" \" + df[\"year\"].astype(str)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['genres'])\n",
    "\n",
    "# 2. Tokenizador y Dataset\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 3. División train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['input_text'], y, test_size=0.33, random_state=42)\n",
    "train_dataset = MovieDataset(X_train, y_train)\n",
    "test_dataset = MovieDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# 4. Modelo BERT Multilabel\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.pooler_output\n",
    "        x = self.dropout(cls_output)\n",
    "        return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BERTClassifier(num_labels=y.shape[1]).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 5. Entrenamiento y evaluación por época\n",
    "for epoch in range(30):  # Cambia si deseas más o menos épocas\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Evaluación por época\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].cpu().numpy()\n",
    "            outputs = model(input_ids, attention_mask).cpu().numpy()\n",
    "            all_preds.append(outputs)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    y_pred = np.vstack(all_preds)\n",
    "    y_true = np.vstack(all_labels)\n",
    "    mauc = roc_auc_score(y_true, y_pred, average='macro')\n",
    "    print(f\"📈 MCAUC Epoch {epoch+1}: {mauc:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 708,
     "referenced_widgets": [
      "d5fd947dfd9247d5b14bfcba5039fcb6",
      "813f440d4e3840f8b275a9ea9bb9164a",
      "640cb0ffe212442d89c3981f6fe228bb",
      "cd6f8938002440699b5c3d542f782ef4",
      "de5060291a16441a8eb97f8ca6474af0",
      "8866e296c7334267937e3494240264fc",
      "eb10b8cbec0a4160a2cb740cadfce11b",
      "11dfe6182b2b47eea340e77ccaafce6b",
      "0a3db12416db489d97cefe6cfc632132",
      "250d70f6890e46e490e800a6cfc43243",
      "7388850a22fb4ece807da2c01206ce58",
      "f2142a1306994d3e9d8705d962ca54e0",
      "2b08831ee3604a36ac3a934ca6c515c3",
      "532a0a7990824da69d31570eae52d4e0",
      "545651e33ba44dd880e7c5008be217d2",
      "2d4f9d3f45664bf0abd7a91be09e4377",
      "83bf553480fe4a31aeb5c1a0936eea2c",
      "a6a6b7d8e26d47099d72c9592f6ba745",
      "6868f3d873264610828ae4c127cd0087",
      "0bddcdcbab134affbfed2bb69633c246",
      "67c3f31d7eb340fea3069b3a7ff4fa2c",
      "dd260cd66e1b4118b841c8af9339a575",
      "770039719dc44b9eabca549a7786022f",
      "b6079407fa2347418e7d29bb5b203b8e",
      "1e57af551306442893d1b4e21817cc1f",
      "ee7482c64e994b479e5a712ffaec2adf",
      "c5e9a514f43e4619b87000461f19963c",
      "51d940a8242044b3ae822bfadcd68a6c",
      "f1bf5841c7314ce3a1417d5d13b63d1d",
      "96a525578f374e70aaa56a9a79195a90",
      "523b1d6c7ff544f2b2f680142e506b02",
      "3c37ca94889845d38126ebf2fde1b77c",
      "b029b7611a7d439886f8fb40b5de93a3",
      "645eadcc8908455db7951f2dcb7e62d4",
      "76fb40b19fa44b559537892f4507de2c",
      "6d906c1cac2f4ef982563bfddea7b9c2",
      "b124ae39ac334965868fdb39edc812d1",
      "97e5c9b1484848d5a6c3f213dbbc4303",
      "792ad58edc0d4b7580efea94ad56eab5",
      "4e1db26220204701905485b4d1ec93c2",
      "0d4c1b65edfd487da11c95e3042cf90d",
      "5cbf7cb8d5814717a0b5269c73a6a0f8",
      "2ad408ffddbf4450a1242d038f7e400f",
      "5230e3fd15df4637839d8f75294ea77a",
      "971866fd9fc04120959069e215c16edc",
      "0e9a8b3011104acf8173c70594597de1",
      "f5f6bcaec8414d499f9b146ce7c62401",
      "002a6cb97fcb4f29a9ee6ebdf30be3ca",
      "0c0eced2dd1c4323ad47b64d944ac643",
      "9c05af0594ab408bbe3819f2191c31bb",
      "0df4d5f143be44fb9fabab2d73bed505",
      "a8c985bad514462c984dcdf7e8d04a7d",
      "1bbdd85209dc4460895d996ea2476793",
      "16ddbd5b312a403196c8c7350025cb1b",
      "8a7da53115f54581851eac93499ee0e4"
     ]
    },
    "id": "iGa-_tF08iuJ",
    "outputId": "c2b1a411-d6df-4274-cb73-6f8faad2cb45"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5fd947dfd9247d5b14bfcba5039fcb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2142a1306994d3e9d8705d962ca54e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/580 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770039719dc44b9eabca549a7786022f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "645eadcc8908455db7951f2dcb7e62d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "971866fd9fc04120959069e215c16edc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.2626\n",
      "📈 MCAUC Epoch 1: 0.82587\n",
      "Epoch 2 completado. Loss promedio: 0.1970\n",
      "📈 MCAUC Epoch 2: 0.90088\n",
      "Epoch 3 completado. Loss promedio: 0.1599\n",
      "📈 MCAUC Epoch 3: 0.91676\n",
      "Epoch 4 completado. Loss promedio: 0.1334\n",
      "📈 MCAUC Epoch 4: 0.91934\n",
      "Epoch 5 completado. Loss promedio: 0.1076\n",
      "📈 MCAUC Epoch 5: 0.92440\n",
      "Epoch 6 completado. Loss promedio: 0.0899\n",
      "📈 MCAUC Epoch 6: 0.92285\n",
      "Epoch 7 completado. Loss promedio: 0.0703\n",
      "📈 MCAUC Epoch 7: 0.92441\n",
      "Epoch 8 completado. Loss promedio: 0.0563\n",
      "📈 MCAUC Epoch 8: 0.92015\n",
      "Epoch 9 completado. Loss promedio: 0.0492\n",
      "📈 MCAUC Epoch 9: 0.90898\n",
      "Epoch 10 completado. Loss promedio: 0.0399\n",
      "📈 MCAUC Epoch 10: 0.91244\n"
     ]
    }
   ],
   "source": [
    "# 🛠️ Instalar librerías necesarias\n",
    "!pip install transformers scikit-learn torch pandas --quiet\n",
    "\n",
    "# 📚 Importar librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1. Cargar y preparar datos\n",
    "df = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', index_col=0)\n",
    "df['genres'] = df['genres'].apply(ast.literal_eval)\n",
    "df[\"input_text\"] = df[\"plot\"] + \" \" + df[\"title\"] + \" \" + df[\"year\"].astype(str)  # plot-título-año\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['genres'])\n",
    "\n",
    "# 2. Tokenizador y Dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 3. División train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['input_text'], y, test_size=0.33, random_state=42)\n",
    "train_dataset = MovieDataset(X_train, y_train)\n",
    "test_dataset = MovieDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# 4. Modelo DeBERTa Multilabel\n",
    "class DeBERTaClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        x = self.dropout(cls_output)\n",
    "        return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DeBERTaClassifier(num_labels=y.shape[1]).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 5. Entrenamiento y evaluación por época\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Evaluación\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].cpu().numpy()\n",
    "            outputs = model(input_ids, attention_mask).cpu().numpy()\n",
    "            all_preds.append(outputs)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    y_pred = np.vstack(all_preds)\n",
    "    y_true = np.vstack(all_labels)\n",
    "    mauc = roc_auc_score(y_true, y_pred, average='macro')\n",
    "    print(f\"📈 MCAUC Epoch {epoch+1}: {mauc:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sMk9G6nzD0Q1",
    "outputId": "0209d016-356b-4a98-898d-ff989878c4a1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.2606\n",
      "Epoch 2 completado. Loss promedio: 0.2999\n",
      "Epoch 3 completado. Loss promedio: 0.2984\n",
      "Epoch 4 completado. Loss promedio: 0.2978\n",
      "Epoch 5 completado. Loss promedio: 0.2974\n",
      "✅ Archivo generado para Kaggle: pred_genres_text_DeBERTa_full.csv\n"
     ]
    }
   ],
   "source": [
    "# 🛠️ Instalar dependencias necesarias\n",
    "!pip install transformers scikit-learn torch pandas --quiet\n",
    "\n",
    "# 📚 Importar librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "# 1. Cargar y preparar datos\n",
    "df = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', index_col=0)\n",
    "df['genres'] = df['genres'].apply(ast.literal_eval)\n",
    "df[\"input_text\"] = df[\"plot\"] + \" \" + df[\"title\"] + \" \" + df[\"year\"].astype(str)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['genres'])\n",
    "\n",
    "# 2. Tokenizador y Dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 3. Cargar dataset completo para entrenamiento\n",
    "full_dataset = MovieDataset(df['input_text'], y)\n",
    "full_loader = DataLoader(full_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# 4. Modelo DeBERTa\n",
    "class DeBERTaClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        x = self.dropout(cls_output)\n",
    "        return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "# 5. Entrenamiento\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DeBERTaClassifier(num_labels=y.shape[1]).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    for batch in full_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(full_loader):.4f}\")\n",
    "\n",
    "# 6. Predicción para Kaggle\n",
    "dataTesting = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTesting.zip', index_col=0)\n",
    "dataTesting[\"input_text\"] = dataTesting[\"plot\"] + \" \" + dataTesting[\"title\"] + \" \" + dataTesting[\"year\"].astype(str)\n",
    "test_enc = tokenizer(list(dataTesting[\"input_text\"]), truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
    "test_dataset = DataLoader(torch.utils.data.TensorDataset(test_enc['input_ids'], test_enc['attention_mask']), batch_size=32)\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for input_ids_batch, attn_mask_batch in test_dataset:\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        attn_mask_batch = attn_mask_batch.to(device)\n",
    "        pred_batch = model(input_ids_batch, attn_mask_batch).cpu().numpy()\n",
    "        preds.append(pred_batch)\n",
    "\n",
    "# 7. Guardar archivo CSV\n",
    "y_pred_test_final = np.vstack(preds)\n",
    "cols = ['p_' + genre for genre in mlb.classes_]\n",
    "res = pd.DataFrame(y_pred_test_final, index=dataTesting.index, columns=cols)\n",
    "res.to_csv('pred_genres_text_DeBERTa_full.csv', index_label='ID')\n",
    "print(\"✅ Archivo generado para Kaggle: pred_genres_text_DeBERTa_full.csv\")\n",
    "# 0.50974"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BlU6esZFUkWV",
    "outputId": "dd234d7f-e494-4281-93f7-5282e0ede89b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.2283\n",
      "Epoch 2 completado. Loss promedio: 0.1643\n",
      "Epoch 3 completado. Loss promedio: 0.1305\n",
      "Epoch 4 completado. Loss promedio: 0.1080\n",
      "Epoch 5 completado. Loss promedio: 0.0824\n",
      "✅ Archivo generado para Kaggle: pred_genres_text_DeBERTa_full.csv\n"
     ]
    }
   ],
   "source": [
    "# 🛠️ Instalar dependencias necesarias\n",
    "!pip install transformers scikit-learn torch pandas --quiet\n",
    "\n",
    "# 📚 Importar librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "# 1. Cargar y preparar datos\n",
    "df = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', index_col=0)\n",
    "df['genres'] = df['genres'].apply(ast.literal_eval)\n",
    "df[\"input_text\"] = df[\"plot\"] + \" \" + df[\"title\"] + \" \" + df[\"year\"].astype(str)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['genres'])\n",
    "\n",
    "# 2. Tokenizador y Dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 3. Cargar dataset completo para entrenamiento\n",
    "full_dataset = MovieDataset(df['input_text'], y)\n",
    "full_loader = DataLoader(full_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# 4. Modelo DeBERTa\n",
    "class DeBERTaClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        x = self.dropout(cls_output)\n",
    "        return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "# 5. Entrenamiento\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DeBERTaClassifier(num_labels=y.shape[1]).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    for batch in full_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(full_loader):.4f}\")\n",
    "\n",
    "# 6. Predicción para Kaggle\n",
    "dataTesting = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTesting.zip', index_col=0)\n",
    "dataTesting[\"input_text\"] = dataTesting[\"plot\"] + \" \" + dataTesting[\"title\"] + \" \" + dataTesting[\"year\"].astype(str)\n",
    "test_enc = tokenizer(list(dataTesting[\"input_text\"]), truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
    "test_dataset = DataLoader(torch.utils.data.TensorDataset(test_enc['input_ids'], test_enc['attention_mask']), batch_size=32)\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for input_ids_batch, attn_mask_batch in test_dataset:\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        attn_mask_batch = attn_mask_batch.to(device)\n",
    "        pred_batch = model(input_ids_batch, attn_mask_batch).cpu().numpy()\n",
    "        preds.append(pred_batch)\n",
    "\n",
    "# 7. Guardar archivo CSV\n",
    "y_pred_test_final = np.vstack(preds)\n",
    "cols = ['p_' + genre for genre in mlb.classes_]\n",
    "res = pd.DataFrame(y_pred_test_final, index=dataTesting.index, columns=cols)\n",
    "res.to_csv('pred_genres_text_DeBERTa_full.csv', index_label='ID')\n",
    "print(\"✅ Archivo generado para Kaggle: pred_genres_text_DeBERTa_full.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 987,
     "referenced_widgets": [
      "c48b250c8ad0459a868911d392b91319",
      "b9b82020394c4a2eb0f43cd11a9df1bf",
      "e8e07957e1a14c6996cf6a70193afb32",
      "fb4812ef1ead450299b02adb91e527d1",
      "38aaf81c9c0f4896af87f38e6817fe6b",
      "eb05e5bd8b57418f8e6a7b6254b2d2fd",
      "6bf9ed8a4850499cbfcc4885186e37a6",
      "b679cbce81e644558c9eea55eb7b9600",
      "25a3768ff31c461badd7606a2f70a64d",
      "12c0f8593c5a47cf937d3557b271c5cc",
      "1d3d637c575d4807abb856c4c7900cc0",
      "faea85c1903b4965942802de35fa007a",
      "bd50c3dd533f45e384a7e3cdff491222",
      "fcb78df956aa434e8ddd52d860f36478",
      "43d685daf6bf4a40b55ce0c6d8f2252b",
      "6a80af32d00f40aea434f0b36b4504b5",
      "86a9191b31fd48fc95bebe2e90ed9684",
      "c89b9f2f9aab4ca38a2e1aaee07977dd",
      "4b96959753064296a24f2f2492d83d42",
      "33a49e73668f40b697b967dd03ccc185",
      "c09feb4d734c4bba957461279f5e3b5d",
      "6204013f53cc4c998b1862f9142467c7",
      "a05187bb049b40d6aef783615d7f898e",
      "818db824410545feac4cc76a1cd5d541",
      "5a5e28c049a94e3088e6325599b46027",
      "26b6db4a258044cfa0bb8bfb1877ca33",
      "e6ec6475c6054ddf9b08c643f9066ecd",
      "1956f0ed50f44de9bf918e8a529719af",
      "1e74938bfce84afbb977055d52a46f88",
      "45a04bb4d6eb4835aff474b92acb6963",
      "517d520c14034351942507b8fcb58914",
      "3aa2918f7a084a4aad70b1bdffb6a8ad",
      "8f00e923a79b40a1a7db64f53ff3f1ec",
      "92ba7a0ce7644849b2293a5e53ecf7b3",
      "e3a126c9c4124809b4f704f208f0bec9",
      "464b3f3823734ab1b4849585ff2eb06a",
      "6a8ab4f3277b49af8998bcce3ec18cdf",
      "85e857d3e16641a9aa91b1cac33cef3b",
      "013bb5ef6e7e4c89a1d27d3e4bea6108",
      "987188c6a4df43d5bb600b1e5fb6aaa9",
      "439215ba827e4cf28643b5984de41f18",
      "cde32476d16542e78998fa34905e3ad5",
      "8a6ae12913c44798978eab60ae9db9e3",
      "83dac93bdaa54c918aff967bfbf8bca9",
      "a92a1064b0b24b74b4d1c83ae1413970"
     ]
    },
    "id": "M51oYi3Adalg",
    "outputId": "107066a4-8390-4fc9-b0b1-8f81842029d1"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c48b250c8ad0459a868911d392b91319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faea85c1903b4965942802de35fa007a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/580 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a05187bb049b40d6aef783615d7f898e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "818db824410545feac4cc76a1cd5d541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3a126c9c4124809b4f704f208f0bec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.2346\n",
      "📈 MCAUC Epoch 1: 0.89370\n",
      "Epoch 2 completado. Loss promedio: 0.1659\n",
      "📈 MCAUC Epoch 2: 0.91494\n",
      "Epoch 3 completado. Loss promedio: 0.1318\n",
      "📈 MCAUC Epoch 3: 0.91900\n",
      "Epoch 4 completado. Loss promedio: 0.1055\n",
      "📈 MCAUC Epoch 4: 0.92504\n",
      "Epoch 5 completado. Loss promedio: 0.0842\n",
      "📈 MCAUC Epoch 5: 0.92557\n",
      "Epoch 6 completado. Loss promedio: 0.0670\n",
      "📈 MCAUC Epoch 6: 0.92384\n",
      "Epoch 7 completado. Loss promedio: 0.0523\n",
      "📈 MCAUC Epoch 7: 0.92384\n",
      "Epoch 8 completado. Loss promedio: 0.0419\n",
      "📈 MCAUC Epoch 8: 0.92177\n",
      "Epoch 9 completado. Loss promedio: 0.0343\n",
      "📈 MCAUC Epoch 9: 0.91894\n",
      "Epoch 10 completado. Loss promedio: 0.0298\n",
      "📈 MCAUC Epoch 10: 0.91991\n"
     ]
    }
   ],
   "source": [
    "# 🛠️ Instalar dependencias necesarias\n",
    "!pip install transformers scikit-learn torch pandas --quiet\n",
    "\n",
    "# 📚 Importar librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "# 1. Cargar y preparar datos\n",
    "df = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', index_col=0)\n",
    "df['genres'] = df['genres'].apply(ast.literal_eval)\n",
    "df[\"input_text\"] = df[\"plot\"] + \" \" + df[\"title\"]  # Quitamos el año\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['genres'])\n",
    "\n",
    "# 2. Tokenizador y Dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=384, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 3. División train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['input_text'], y, test_size=0.33, random_state=42)\n",
    "train_dataset = MovieDataset(X_train, y_train)\n",
    "test_dataset = MovieDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# 4. Modelo DeBERTa Multilabel\n",
    "class DeBERTaClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        x = self.dropout(cls_output)\n",
    "        return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "# 5. Entrenamiento y evaluación por época con scheduler\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DeBERTaClassifier(num_labels=y.shape[1]).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = nn.BCELoss()\n",
    "num_training_steps = len(train_loader) * 10\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Evaluación\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].cpu().numpy()\n",
    "            outputs = model(input_ids, attention_mask).cpu().numpy()\n",
    "            all_preds.append(outputs)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    y_pred = np.vstack(all_preds)\n",
    "    y_true = np.vstack(all_labels)\n",
    "    mauc = roc_auc_score(y_true, y_pred, average='macro')\n",
    "    print(f\"📈 MCAUC Epoch {epoch+1}: {mauc:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "f78181ff03c347ff9cc07a7c3be132d8",
      "eefe8111462c4dfe889414c51483c86c",
      "ebf7cfa96522451fbdb9601a07d905a9",
      "c9894a34908a4934a2676ad8bcd7adf2",
      "33e888ddb4a248058586ac89a8f31d01",
      "420bc41770b647dd8b3680b724e73453",
      "ab56a0c138e347ca9452fc74b32a80fc",
      "3f79c766c132443da8c683c5a5a33633",
      "047c5b2024064055bd1828ad0f9451b9",
      "df2e076dcd7045a9ba7b312623006888",
      "09cc39f9426b4f28afbdf1a3e0f8fb1a",
      "5d4c05e8df744758be661273df37abf4",
      "f786f3230c744d9181c92a7963415343",
      "08e192fe88d147b18cda8545cd88f7dc",
      "3b32b9978cf142ffbc238044796bfc09",
      "89f1d53360be481d8beb74567d2a1e11",
      "395ff0e3a2fe4804898a3eab44cbb0fa",
      "5b694f4228ad4dc188ba7398ca1f5a7f",
      "d3c17c29056240d184dd80c34c7b6cc4",
      "f4e8330dab0042b9a393a5dd0d2ec53d",
      "c266a47e45e04223a894db7356a860ba",
      "460b9e0b63e747eda51b76079b12891a",
      "90d485ed00b84b808c66bfe5469b9bc5",
      "2115d1e6d3df40ad87df9311f0ff26e9",
      "ad7d7725488145a6bbdc460ca3d7af3a",
      "db6b1704f3f84e6480b585d9d350f53f",
      "b1237c0d4c274982a961494d7a66d82e",
      "d93db406950f48ee82f4e5b338e345cd",
      "07953a2ee5554c41b39a99f50048e45c",
      "a67f96d5efa74d7fb39d57f30d358a06",
      "e3dadf3602444267aecc4264af395fb8",
      "4fa2c369a855499fad460b29551448d8",
      "56c92e8537f74608897fcfb371b4721f",
      "34b39e17323c415eacfbe8a96b7d106c",
      "473ffaa1dd3647f287f4bbfcd9558774",
      "77abc84c74ef49cfacdb40e4cc543f70",
      "61090e9763d04eb8976bbaa691778330",
      "25570448723c489cae300307a70401d2",
      "8808893056054806bf6cd9dda7fa9822",
      "1b1e3fc3b0e94d4886ad2f21c6635971",
      "4fcba16d8ee74235a121cb6c92a63115",
      "8dbbd2de7ce64ee8a4c3ac4b03e63edb",
      "a86ae5b9134340afb1e140c70e37e907",
      "c2ff1fba40534467b7216cb907d3aa0b",
      "45590423b5e64b35bfa624d0c7da7cc7",
      "6bf1ce1633034640906cc9c6b7f7d1dd",
      "ca60937a892b404d968801b78232094b",
      "c925ac2b8c5f4c8c873327b0602fe003",
      "42e61a851a1c443cb3d69b9f702f61f0",
      "18cc98324fa646bda03b20be4edadd49",
      "6c3403e4c915426385ca99286f617d3d",
      "173ba4bd49a64c4baf0dc2c82df9461b",
      "51f8ae2cf26f44178dbba42a646952d8",
      "bd4f8a3d067246e1a97fe21d0fcb8722",
      "93485b6b69454257a59c60bdae787883"
     ]
    },
    "id": "Rd-SKaiX-1WL",
    "outputId": "48dd0432-aa0d-429a-9dc9-2c9de8c3a329"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m102.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f78181ff03c347ff9cc07a7c3be132d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d4c05e8df744758be661273df37abf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/580 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90d485ed00b84b808c66bfe5469b9bc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34b39e17323c415eacfbe8a96b7d106c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45590423b5e64b35bfa624d0c7da7cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.2409\n",
      "📈 MCAUC Epoch 1: 0.87442\n",
      "Epoch 2 completado. Loss promedio: 0.1740\n",
      "📈 MCAUC Epoch 2: 0.91414\n",
      "Epoch 3 completado. Loss promedio: 0.1408\n",
      "📈 MCAUC Epoch 3: 0.92479\n",
      "Epoch 4 completado. Loss promedio: 0.1153\n",
      "📈 MCAUC Epoch 4: 0.93015\n",
      "Epoch 5 completado. Loss promedio: 0.0933\n",
      "📈 MCAUC Epoch 5: 0.93173\n",
      "Epoch 6 completado. Loss promedio: 0.0743\n",
      "📈 MCAUC Epoch 6: 0.93141\n",
      "Epoch 7 completado. Loss promedio: 0.0599\n",
      "📈 MCAUC Epoch 7: 0.93036\n",
      "Epoch 8 completado. Loss promedio: 0.0480\n",
      "📈 MCAUC Epoch 8: 0.92719\n",
      "Epoch 9 completado. Loss promedio: 0.0398\n",
      "📈 MCAUC Epoch 9: 0.92762\n",
      "Epoch 10 completado. Loss promedio: 0.0345\n",
      "📈 MCAUC Epoch 10: 0.92668\n"
     ]
    }
   ],
   "source": [
    "# 🛠️ Instalar dependencias necesarias\n",
    "!pip install transformers scikit-learn torch pandas --quiet\n",
    "\n",
    "# 📚 Importar librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "# 1. Cargar y preparar datos\n",
    "df = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', index_col=0)\n",
    "df['genres'] = df['genres'].apply(ast.literal_eval)\n",
    "df[\"input_text\"] = df[\"plot\"] + \" \" + df[\"title\"] + \" \" + df[\"year\"].astype(str)  # ✅ AÑADIMOS EL AÑO\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['genres'])\n",
    "\n",
    "# 2. Tokenizador y Dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=384, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 3. División train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['input_text'], y, test_size=0.33, random_state=42)\n",
    "train_dataset = MovieDataset(X_train, y_train)\n",
    "test_dataset = MovieDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# 4. Modelo DeBERTa Multilabel\n",
    "class DeBERTaClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        x = self.dropout(cls_output)\n",
    "        return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "# 5. Entrenamiento y evaluación por época\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DeBERTaClassifier(num_labels=y.shape[1]).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = nn.BCELoss()\n",
    "num_training_steps = len(train_loader) * 10\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Evaluación\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].cpu().numpy()\n",
    "            outputs = model(input_ids, attention_mask).cpu().numpy()\n",
    "            all_preds.append(outputs)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    y_pred = np.vstack(all_preds)\n",
    "    y_true = np.vstack(all_labels)\n",
    "    mauc = roc_auc_score(y_true, y_pred, average='macro')\n",
    "    print(f\"📈 MCAUC Epoch {epoch+1}: {mauc:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "535e1fe014aa4b8699e32bf94900fe13",
      "a2e8a26e9bbc496c86651adc6f405f5b",
      "5c1400f443c049509915f0c02c16c9ab",
      "14541fbe64c549d7bc201305d04badb4",
      "de8b530fe3eb4b03b181d123631b3f3f",
      "1ad2c0778d124111b56d35bab9c454ce",
      "0509f010443541b0867458f9d40382f2",
      "4faf522a2fa0465cb5358a27661a0ac1",
      "5f5120c8935c44a08bde5f7477fbe19d",
      "09d4a537aecb4de1bd2a29766f40c297",
      "83139bbaf54445be81d9a46bf48ea34d",
      "01eea31c6fb440149758fa2b484be354",
      "a73b9a756f47477fa207cb70948fdc59",
      "68ef6b1e84a84b7c8b49a30cad0f57c0",
      "344ccac8843a484bac2c1decac22c7f6",
      "397289f60d2a48d9bb6d926076041c41",
      "2346fc8b2573465dbeae8181ba5977db",
      "75ec644a31884ddc947713600086131a",
      "9b67da1b1bb346b5876c5da086dd7e49",
      "e285e80f3687487cbc902b8b843401d3",
      "9470e066d1f24a81ad9cebfc2b9c3423",
      "62dd398cf80849d981885982fb688bc1",
      "69f5be2966024259af1250fc0a7b036d",
      "edfc541bf1b64659adb8c19184ca6192",
      "84a27b3982ba44e4a541a9d4aee5d10b",
      "1e367a04130944d5828396995a64d3ae",
      "462e895720c546b5b0d187cbaed2fff9",
      "736ba1a1f323445eba3d7804bfe456f7",
      "0e7727ed1f304b66999fb8bad823f584",
      "7c2c5de1ba8e4cab8886a5944509bd39",
      "89777a690f45433c86da14d3b23746fb",
      "63c4f891f6bc4e81a4800bef42aa99d6",
      "681eb3a193984486a9cee3b26548b6c4",
      "4b5923c27ae8498b91fbd0f3e645850c",
      "43c50de5b8f34601b42d2e2435232b67",
      "829d4451f84a4277a4f4cb7a03ab928b",
      "06f1078c696e4aa7ade2ab4f8e15958f",
      "1d37c5c4f83240ef9b0643652ab2bd08",
      "9c4c88c86a89476c97af02b81c2c0f16",
      "dc9b2fbf49b14b9583c0d6d61461ab06",
      "c9d995fe3e5c4ba79dc8a8f12eab4776",
      "24d9b6f8a0694bc982126b2abf8133db",
      "68c774631b354f1fa488661b6091b050",
      "f186207110ab462794850559066594ea",
      "25e1e03846ec49f1af7698c8a80a64f8",
      "ee7d165f58ab490699ed79b016a98af7",
      "31b32606260143b297644cd1f960848f",
      "56a39e6e530349ec93d1b0dc7574e965",
      "0aca0f1d371842e6a4e89519ee8153b2",
      "6a13c1e25aa54a4f890d2d1d95b97a18",
      "90ff320521dd41169b3429b143afc3d7",
      "b1299165f172403ca6cab230e56134c9",
      "baf7a2968b0d4d348ff05a8c08245140",
      "2c0ad3b2c0634f3391cd4230810e040d",
      "82320885161749d1b48fd32c1ca96354"
     ]
    },
    "id": "51I8j83MPeOQ",
    "outputId": "f41025a4-6f39-45b8-85e0-f097bea28ca5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m126.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m109.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "535e1fe014aa4b8699e32bf94900fe13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01eea31c6fb440149758fa2b484be354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/580 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f5be2966024259af1250fc0a7b036d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b5923c27ae8498b91fbd0f3e645850c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e1e03846ec49f1af7698c8a80a64f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.2341\n",
      "📈 MCAUC Epoch 1: 0.90520\n",
      "Epoch 2 completado. Loss promedio: 0.1628\n",
      "📈 MCAUC Epoch 2: 0.92606\n",
      "Epoch 3 completado. Loss promedio: 0.1271\n",
      "📈 MCAUC Epoch 3: 0.93374\n",
      "Epoch 4 completado. Loss promedio: 0.1015\n",
      "📈 MCAUC Epoch 4: 0.93489\n",
      "Epoch 5 completado. Loss promedio: 0.0801\n",
      "📈 MCAUC Epoch 5: 0.93564\n",
      "Epoch 6 completado. Loss promedio: 0.0627\n",
      "📈 MCAUC Epoch 6: 0.93609\n",
      "Epoch 7 completado. Loss promedio: 0.0487\n",
      "📈 MCAUC Epoch 7: 0.93307\n",
      "Epoch 8 completado. Loss promedio: 0.0389\n",
      "📈 MCAUC Epoch 8: 0.93205\n",
      "Epoch 9 completado. Loss promedio: 0.0319\n",
      "📈 MCAUC Epoch 9: 0.92942\n",
      "Epoch 10 completado. Loss promedio: 0.0274\n",
      "📈 MCAUC Epoch 10: 0.92947\n"
     ]
    }
   ],
   "source": [
    "# 🛠️ Instalar dependencias necesarias\n",
    "!pip install transformers scikit-learn torch pandas --quiet\n",
    "\n",
    "# 📚 Importar librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "# 1. Cargar y preparar datos\n",
    "df = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', index_col=0)\n",
    "df['genres'] = df['genres'].apply(ast.literal_eval)\n",
    "df[\"input_text\"] = df[\"title\"] + \" (\" + df[\"year\"].astype(str) + \"): \" + df[\"plot\"]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['genres'])\n",
    "\n",
    "# 2. Tokenizador y Dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=384, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 3. División train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['input_text'], y, test_size=0.33, random_state=42)\n",
    "train_dataset = MovieDataset(X_train, y_train)\n",
    "test_dataset = MovieDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# 4. Modelo DeBERTa Multilabel\n",
    "class DeBERTaClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        x = self.dropout(cls_output)\n",
    "        return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "# 5. Entrenamiento y evaluación por época\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DeBERTaClassifier(num_labels=y.shape[1]).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = nn.BCELoss()\n",
    "num_training_steps = len(train_loader) * 10\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Evaluación\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].cpu().numpy()\n",
    "            outputs = model(input_ids, attention_mask).cpu().numpy()\n",
    "            all_preds.append(outputs)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    y_pred = np.vstack(all_preds)\n",
    "    y_true = np.vstack(all_labels)\n",
    "    mauc = roc_auc_score(y_true, y_pred, average='macro')\n",
    "    print(f\"📈 MCAUC Epoch {epoch+1}: {mauc:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kvaIUBQ5pTdK",
    "outputId": "6f914580-bbc7-4281-8c1b-f06f0dcbdddf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.2320\n",
      "Epoch 2 completado. Loss promedio: 0.1610\n",
      "Epoch 3 completado. Loss promedio: 0.1276\n",
      "Epoch 4 completado. Loss promedio: 0.1035\n",
      "Epoch 5 completado. Loss promedio: 0.0828\n",
      "Epoch 6 completado. Loss promedio: 0.0655\n",
      "✅ Archivo final generado: pred_genres_DeBERTaV3_Ep6.csv\n"
     ]
    }
   ],
   "source": [
    "# 🛠️ Instalar dependencias necesarias\n",
    "!pip install transformers scikit-learn torch pandas --quiet\n",
    "\n",
    "# 📚 Importar librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "# 1. Cargar y preparar datos\n",
    "df = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', index_col=0)\n",
    "df['genres'] = df['genres'].apply(ast.literal_eval)\n",
    "df[\"input_text\"] = df[\"title\"] + \" (\" + df[\"year\"].astype(str) + \"): \" + df[\"plot\"]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['genres'])\n",
    "\n",
    "# 2. Tokenizador y Dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=384, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 3. Dataset completo para entrenamiento\n",
    "full_dataset = MovieDataset(df['input_text'], y)\n",
    "full_loader = DataLoader(full_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# 4. Modelo DeBERTa v3 Large\n",
    "class DeBERTaClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        x = self.dropout(cls_output)\n",
    "        return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "# 5. Entrenamiento (usando solo hasta época 6 como la mejor)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DeBERTaClassifier(num_labels=y.shape[1]).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(6):  # Solo hasta la mejor época encontrada\n",
    "    total_loss = 0\n",
    "    for batch in full_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(full_loader):.4f}\")\n",
    "\n",
    "# 6. Predicción para Kaggle\n",
    "dataTesting = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTesting.zip', index_col=0)\n",
    "dataTesting[\"input_text\"] = dataTesting[\"title\"] + \" (\" + dataTesting[\"year\"].astype(str) + \"): \" + dataTesting[\"plot\"]\n",
    "test_enc = tokenizer(list(dataTesting[\"input_text\"]), truncation=True, padding=True, max_length=384, return_tensors='pt')\n",
    "test_dataset = DataLoader(torch.utils.data.TensorDataset(test_enc['input_ids'], test_enc['attention_mask']), batch_size=32)\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for input_ids_batch, attn_mask_batch in test_dataset:\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        attn_mask_batch = attn_mask_batch.to(device)\n",
    "        pred_batch = model(input_ids_batch, attn_mask_batch).cpu().numpy()\n",
    "        preds.append(pred_batch)\n",
    "\n",
    "# 7. Guardar archivo CSV\n",
    "y_pred_test_final = np.vstack(preds)\n",
    "cols = ['p_' + genre for genre in mlb.classes_]\n",
    "res = pd.DataFrame(y_pred_test_final, index=dataTesting.index, columns=cols)\n",
    "res.to_csv('pred_genres_DeBERTaV3_Ep6.csv', index_label='ID')\n",
    "print(\"✅ Archivo final generado: pred_genres_DeBERTaV3_Ep6.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "f052b958a2e943988c02f1b481b1c442",
      "09cccefd71c14f199145f86a365043fe",
      "ce7bd04261ba44a8a7b3661d290e6e5f",
      "6f0c5b945d5549d6b747d3984ed6acc4",
      "fabee4d709d44a3a8c4c14ecce46f372",
      "78c738d8af5a4f22a72f4c587bc48ff9",
      "3139816ab8fe447cbe9bcf41f96da7c4",
      "27b90724612a436eb11927a158e81c27",
      "a5011ce710334d50b5a960d722b84790",
      "530509799bde4c22acd8ae396e5e51dc",
      "976ea1140fc34ca297a6715b5d0c219f",
      "6287633011b24480bf555246a41473d2",
      "6cbafc25e12c41a49476d8c5e113171a",
      "8d379fa648014f4cb2cb54ef315f7060",
      "a4ad7f6b5cef4f79bab5d5258a0b64fb",
      "7231dd34a42142efbdb2f2417f42d365",
      "939b9833f82143ae95b76977786c0e7e",
      "51919e95808e4fa281a068c38086148e",
      "75a056309ad84d0cbd12c66e0f916236",
      "6fc96c5e6d3444dd9695fd4bb34552db",
      "fb3000d571c04fe8a18f8e7f9309bfd9",
      "7f006f4201774aaf991dd118a62e4804",
      "f1e118022d124f73b329132b429ccb21",
      "4a702166a8f148778e0e47619ec570a4",
      "753806a0eb894651a602cbbac72a0203",
      "ed6b2f7eefe14f5ead5b382bace36cf8",
      "dc19e7cbdcf24b929fd5bfc767f729dc",
      "9bbcf4e911f04113b0bd3caf760c413e",
      "b34f4804611543de9022b6d87aaa3613",
      "2e9e33983c754f1b9394bd8a0552c3b4",
      "8084363342464bbe8de5d68b82d6f300",
      "a00d439dd8ce4468b7ac457c457ec1c7",
      "3cbef6e2fe2d489f9b564cbba145e462",
      "67b03cd3503742d5a9db60fa333d2d55",
      "348e39ca00c945adbf5825a1a128a452",
      "1f7473bd5b684a498940cb6b90760982",
      "3bf6fe2fa68647a38a409f62451b820e",
      "2b82927017a349e0a30fe1c2800fa33b",
      "8261cf28a2b64337a3d92b389faf0863",
      "24615e8c3cef437cb663ab0bee81dd3e",
      "989b2802d9b445b4887127b97f9a2183",
      "5218ddf7d9d64dde848119688ea6cd74",
      "ed77def552cf4c1faa4fe7b1647d42dd",
      "84bfb36410064f0aaf9d8fc34d2e1ce5",
      "4d28d1b10fb84b418865b4b0e56bad3e",
      "0ba7a9eefaea4d339bd4aa45133b0ff7",
      "d0e362a305f147f6b1116edfb5f6c9c1",
      "eb13517ad9a94cc2972a303eb8f3f8d1",
      "a890d602d4084f95bfb7ccc0df598a2d",
      "a2bbd26321fc41a08f7bb3623c509896",
      "bd390d83a9e142f880ceff6fee1cc811",
      "8c2b16c81a4547ce8ef7899441a2c861",
      "898a58e3de3748bc821cbf739a097c8a",
      "32be5a13c1614b7584790cd72953511c",
      "1e5fac074f0b4c75acbfb9d04894039c"
     ]
    },
    "id": "8n3tJsG5_wz8",
    "outputId": "8cbed134-5db1-4c37-91c6-6327788f3235"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m124.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m108.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f052b958a2e943988c02f1b481b1c442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6287633011b24480bf555246a41473d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/580 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e118022d124f73b329132b429ccb21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67b03cd3503742d5a9db60fa333d2d55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d28d1b10fb84b418865b4b0e56bad3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.2373\n",
      "📈 MCAUC Epoch 1: 0.89268\n",
      "Epoch 2 completado. Loss promedio: 0.1681\n",
      "📈 MCAUC Epoch 2: 0.92031\n",
      "Epoch 3 completado. Loss promedio: 0.1326\n",
      "📈 MCAUC Epoch 3: 0.92968\n",
      "Epoch 4 completado. Loss promedio: 0.1072\n",
      "📈 MCAUC Epoch 4: 0.93321\n",
      "Epoch 5 completado. Loss promedio: 0.0857\n",
      "📈 MCAUC Epoch 5: 0.93186\n",
      "Epoch 6 completado. Loss promedio: 0.0682\n",
      "📈 MCAUC Epoch 6: 0.93215\n",
      "Epoch 7 completado. Loss promedio: 0.0542\n",
      "📈 MCAUC Epoch 7: 0.93208\n",
      "Epoch 8 completado. Loss promedio: 0.0432\n",
      "📈 MCAUC Epoch 8: 0.93058\n",
      "Epoch 9 completado. Loss promedio: 0.0356\n",
      "📈 MCAUC Epoch 9: 0.92905\n",
      "Epoch 10 completado. Loss promedio: 0.0308\n",
      "📈 MCAUC Epoch 10: 0.92973\n"
     ]
    }
   ],
   "source": [
    "# 🛠️ Instalar dependencias necesarias\n",
    "!pip install transformers scikit-learn torch pandas --quiet\n",
    "\n",
    "# 📚 Importar librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "# 1. Cargar y preparar datos\n",
    "df = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', index_col=0)\n",
    "df['genres'] = df['genres'].apply(ast.literal_eval)\n",
    "df[\"input_text\"] = df[\"title\"] + \" (\" + df[\"year\"].astype(str) + \"): \" + df[\"plot\"]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['genres'])\n",
    "\n",
    "# 2. Tokenizador y Dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=384, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 3. División train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['input_text'], y, test_size=0.33, random_state=42)\n",
    "train_dataset = MovieDataset(X_train, y_train)\n",
    "test_dataset = MovieDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# 4. Modelo DeBERTa Multilabel\n",
    "class DeBERTaClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        x = self.dropout(cls_output)\n",
    "        return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "# 5. Entrenamiento y evaluación por época\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DeBERTaClassifier(num_labels=y.shape[1]).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = nn.BCELoss()\n",
    "num_training_steps = len(train_loader) * 10\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Evaluación\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].cpu().numpy()\n",
    "            outputs = model(input_ids, attention_mask).cpu().numpy()\n",
    "            all_preds.append(outputs)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    y_pred = np.vstack(all_preds)\n",
    "    y_true = np.vstack(all_labels)\n",
    "    mauc = roc_auc_score(y_true, y_pred, average='macro')\n",
    "    print(f\"📈 MCAUC Epoch {epoch+1}: {mauc:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "id": "yCxsneJvHXLy",
    "outputId": "d87f72ff-b134-473d-8bb9-c6c6784873e1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.2369\n",
      "📈 MCAUC Epoch 1: 0.88839\n",
      "Epoch 2 completado. Loss promedio: 0.1645\n",
      "📈 MCAUC Epoch 2: 0.92617\n",
      "Epoch 3 completado. Loss promedio: 0.1294\n",
      "📈 MCAUC Epoch 3: 0.93254\n",
      "Epoch 4 completado. Loss promedio: 0.1032\n",
      "📈 MCAUC Epoch 4: 0.93456\n",
      "Epoch 5 completado. Loss promedio: 0.0820\n",
      "📈 MCAUC Epoch 5: 0.93378\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3e77cf2d45f2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(train_loader):.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 🛠️ Instalar dependencias necesarias\n",
    "!pip install transformers scikit-learn torch pandas --quiet\n",
    "\n",
    "# 📚 Importar librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "# 1. Cargar y preparar datos\n",
    "df = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', index_col=0)\n",
    "df['genres'] = df['genres'].apply(ast.literal_eval)\n",
    "\n",
    "# 🧠 Agregar variable de década\n",
    "df['decade'] = (df['year'] // 10 * 10).astype(str) + \"s\"\n",
    "\n",
    "# 🎯 Formato input_text con década incluida\n",
    "df[\"input_text\"] = df[\"title\"] + \" (\" + df[\"year\"].astype(str) + \", \" + df[\"decade\"] + \"): \" + df[\"plot\"]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['genres'])\n",
    "\n",
    "# 2. Tokenizador y Dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=384, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 3. División train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['input_text'], y, test_size=0.33, random_state=42)\n",
    "train_dataset = MovieDataset(X_train, y_train)\n",
    "test_dataset = MovieDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# 4. Modelo DeBERTa Multilabel\n",
    "class DeBERTaClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        x = self.dropout(cls_output)\n",
    "        return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "# 5. Entrenamiento y evaluación por época\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DeBERTaClassifier(num_labels=y.shape[1]).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = nn.BCELoss()\n",
    "num_training_steps = len(train_loader) * 10\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Evaluación\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].cpu().numpy()\n",
    "            outputs = model(input_ids, attention_mask).cpu().numpy()\n",
    "            all_preds.append(outputs)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    y_pred = np.vstack(all_preds)\n",
    "    y_true = np.vstack(all_labels)\n",
    "    mauc = roc_auc_score(y_true, y_pred, average='macro')\n",
    "    print(f\"📈 MCAUC Epoch {epoch+1}: {mauc:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xQ31JLrjXXQf",
    "outputId": "8486c303-3d73-4ad0-8c5b-0eb03b7cc402"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.2364\n",
      "📈 MCAUC Epoch 1: 0.89618\n",
      "Epoch 2 completado. Loss promedio: 0.1664\n",
      "📈 MCAUC Epoch 2: 0.92118\n",
      "Epoch 3 completado. Loss promedio: 0.1310\n",
      "📈 MCAUC Epoch 3: 0.92906\n",
      "Epoch 4 completado. Loss promedio: 0.1045\n",
      "📈 MCAUC Epoch 4: 0.93452\n",
      "Epoch 5 completado. Loss promedio: 0.0832\n",
      "📈 MCAUC Epoch 5: 0.93465\n",
      "Epoch 6 completado. Loss promedio: 0.0656\n",
      "📈 MCAUC Epoch 6: 0.93314\n"
     ]
    }
   ],
   "source": [
    "# 🛠️ Instalar dependencias necesarias\n",
    "!pip install transformers scikit-learn torch pandas --quiet\n",
    "\n",
    "# 📚 Importar librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "# 1. Cargar y preparar datos\n",
    "df = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', index_col=0)\n",
    "df['genres'] = df['genres'].apply(ast.literal_eval)\n",
    "\n",
    "# 🧠 Agregar variable de década\n",
    "df['decade'] = (df['year'] // 10 * 10).astype(str) + \"s\"\n",
    "\n",
    "# 🎯 Formato input_text con década incluida\n",
    "df[\"input_text\"] = df[\"title\"] + \" (\" + df[\"decade\"] + \"): \" + df[\"plot\"]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['genres'])\n",
    "\n",
    "# 2. Tokenizador y Dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=384, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 3. División train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['input_text'], y, test_size=0.33, random_state=42)\n",
    "train_dataset = MovieDataset(X_train, y_train)\n",
    "test_dataset = MovieDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# 4. Modelo DeBERTa Multilabel\n",
    "class DeBERTaClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        x = self.dropout(cls_output)\n",
    "        return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "# 5. Entrenamiento y evaluación por época\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DeBERTaClassifier(num_labels=y.shape[1]).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = nn.BCELoss()\n",
    "num_training_steps = len(train_loader) * 10\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Evaluación\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].cpu().numpy()\n",
    "            outputs = model(input_ids, attention_mask).cpu().numpy()\n",
    "            all_preds.append(outputs)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    y_pred = np.vstack(all_preds)\n",
    "    y_true = np.vstack(all_labels)\n",
    "    mauc = roc_auc_score(y_true, y_pred, average='macro')\n",
    "    print(f\"📈 MCAUC Epoch {epoch+1}: {mauc:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "eacb2344ceba4fefb9c87af2cd4ff4d1",
      "d73d7b7502af4221bced49aa846fccbd",
      "ba359cda725c4ac9a0c6e1ce7419c3b5",
      "7da6a033341b46aa818faf2ca9cba83f",
      "1b1cd762cde840ee87a1592dabd99055",
      "a11ae41dc208430abef35a41b0329d5e",
      "6e593ac3cdb14f2e9e8008d8d0491ccc",
      "faf161ed57b749e29b495208c21f380c",
      "66e25500b2fb41d99e72bf5e7dff1614",
      "223a0411d1604143a774c03524296d34",
      "7118d01dd1254501b879cab969ce853c",
      "0ddbdb58792d445c9cbcba43a55bb68c",
      "5189c9495fc64effab046e8f8aa07eee",
      "a85c400c90df462b8127935fb64aa960",
      "1799befddae54d5b837937bd58dfdd92",
      "9613766699024a55800b94501b87fe0c",
      "fefb093003b74649963e911e28b2b0b9",
      "120921bf4f7c457fb3cc84f9a497c69a",
      "6d5f47ec56ee4daf87ef13bd99d156f0",
      "e4961316dcbd4153b4399f7012bccf0d",
      "885a3f7a60eb4d70ad38e4dc9d7232ab",
      "eb9af8ade01b4abcb70f5db658c41d1d",
      "7d67efe63929498c874577f83dc33254",
      "a69917711e2643c8b1cf9d1d72563cb3",
      "f6d218a18d494ff4bb711ed6e8f5febe",
      "3e94c7f9a63e42338d59cb37a9ab8836",
      "4dc13ead15914bf083a1ce803e061993",
      "8a0840be79684a2288a29ec587c591f7",
      "ad019c89dce2429d93c915a35efebc0d",
      "2d31d7ffc89640909226f471f15825a3",
      "73bcaef5d91e408b8bd5e3969c536d00",
      "184e915e79284c1588f1a66293be04c0",
      "cc122be1bdbf4606b52cfcf551445d62",
      "8d08dde4803343b7b6e001eb76ae6b2a",
      "619d990bf4c14e8ba528cbcc32419317",
      "46ffeabf6463448c8c8a7d9e21bf23af",
      "317b5a16dec145a8a76ff262cfe30ab6",
      "4138535c1f91448da0a88edd568d21a8",
      "4dd4eac3f86a43c9858e3d8c9b9c51fd",
      "562c1a11f99e4559a30ef7d614c3d26b",
      "0d5614070baf4472afdd807def73d57f",
      "c322d1606378491ca3f37f01cd0827b8",
      "3d2966c6c8ec4e76ab1d900ab717cbd0",
      "e47cd14a56cb4671a8f04e3ef048da19",
      "9c56ee684dde4561a5ad4973d2be7600",
      "0aa24f49212e4ec88129ea3473515f45",
      "6d49ba27df34464cae7316939cda809a",
      "bbe01462f3904abd910f2b3bd97dc347",
      "29a523f2996b4673a887ce03f2966b18",
      "3b27c99ba22d4b79915e31bded930ce0",
      "d55f8c640f98458aa04fb546ad23ad61",
      "13e0df2c1a2048cb8d5f4bca5a493668",
      "7be38bfcf9734765bfa39b42f7181f98",
      "9b76525c6bac45d78552453a794325c0",
      "0fd0a84374a34179a293a8720c88ee44"
     ]
    },
    "id": "7d9M4DY1uW7z",
    "outputId": "76d559db-9e19-4c20-936f-3a2b4ad20bbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eacb2344ceba4fefb9c87af2cd4ff4d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ddbdb58792d445c9cbcba43a55bb68c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/580 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d67efe63929498c874577f83dc33254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d08dde4803343b7b6e001eb76ae6b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c56ee684dde4561a5ad4973d2be7600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.2405\n",
      "📈 MCAUC Epoch 1: 0.89219\n",
      "Epoch 2 completado. Loss promedio: 0.1732\n",
      "📈 MCAUC Epoch 2: 0.91379\n",
      "Epoch 3 completado. Loss promedio: 0.1390\n",
      "📈 MCAUC Epoch 3: 0.92348\n",
      "Epoch 4 completado. Loss promedio: 0.1133\n",
      "📈 MCAUC Epoch 4: 0.93100\n",
      "Epoch 5 completado. Loss promedio: 0.0912\n",
      "📈 MCAUC Epoch 5: 0.93510\n",
      "Epoch 6 completado. Loss promedio: 0.0730\n",
      "📈 MCAUC Epoch 6: 0.93194\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4c8360c2bb0d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(train_loader):.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 🛠️ Instalar dependencias necesarias\n",
    "!pip install transformers scikit-learn torch pandas --quiet\n",
    "\n",
    "# 📚 Importar librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "# 1. Cargar y preparar datos\n",
    "df = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', index_col=0)\n",
    "df['genres'] = df['genres'].apply(ast.literal_eval)\n",
    "\n",
    "# 🔧 Ingeniería de variables: KMeans sobre TF-IDF del plot\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "X_tfidf = vectorizer.fit_transform(df['plot'])\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "df['plot_cluster'] = kmeans.fit_predict(X_tfidf)\n",
    "\n",
    "# Formato original + nuevo cluster agregado como texto\n",
    "df[\"input_text\"] = df[\"title\"] + \" (\" + df[\"year\"].astype(str) + \"): \" + df[\"plot\"] + \" Cluster\" + df[\"plot_cluster\"].astype(str)\n",
    "\n",
    "# 2. Etiquetas multilabel\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['genres'])\n",
    "\n",
    "# 3. Tokenizador y Dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=384, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 4. División train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['input_text'], y, test_size=0.33, random_state=42)\n",
    "train_dataset = MovieDataset(X_train, y_train)\n",
    "test_dataset = MovieDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# 5. Modelo DeBERTa\n",
    "class DeBERTaClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        x = self.dropout(cls_output)\n",
    "        return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "# 6. Entrenamiento y evaluación por época\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DeBERTaClassifier(num_labels=y.shape[1]).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = nn.BCELoss()\n",
    "num_training_steps = len(train_loader) * 10\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].cpu().numpy()\n",
    "            outputs = model(input_ids, attention_mask).cpu().numpy()\n",
    "            all_preds.append(outputs)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    y_pred = np.vstack(all_preds)\n",
    "    y_true = np.vstack(all_labels)\n",
    "    mauc = roc_auc_score(y_true, y_pred, average='macro')\n",
    "    print(f\"📈 MCAUC Epoch {epoch+1}: {mauc:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kqLjZ7vpQAEs",
    "outputId": "68374209-2149-4d78-9183-ef2460166db7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.2248\n",
      "Epoch 2 completado. Loss promedio: 0.1551\n",
      "Epoch 3 completado. Loss promedio: 0.1232\n",
      "Epoch 4 completado. Loss promedio: 0.0980\n",
      "Epoch 5 completado. Loss promedio: 0.0772\n",
      "Epoch 6 completado. Loss promedio: 0.0601\n",
      "✅ Archivo final generado: pred_genres_DeBERTaV3_Ep6.csv\n"
     ]
    }
   ],
   "source": [
    "# 🛠️ Instalar dependencias necesarias\n",
    "!pip install transformers scikit-learn torch pandas --quiet\n",
    "\n",
    "# 📚 Importar librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "# 1. Cargar y preparar datos\n",
    "df = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', index_col=0)\n",
    "df['genres'] = df['genres'].apply(ast.literal_eval)\n",
    "df[\"input_text\"] = df[\"title\"] + \" (\" + df[\"year\"].astype(str) + \"): \" + df[\"plot\"]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['genres'])\n",
    "\n",
    "# 2. Tokenizador y Dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=384, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 3. Dataset completo para entrenamiento\n",
    "full_dataset = MovieDataset(df['input_text'], y)\n",
    "full_loader = DataLoader(full_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# 4. Modelo DeBERTa v3 Large\n",
    "class DeBERTaClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        x = self.dropout(cls_output)\n",
    "        return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "# 5. Entrenamiento (usando solo hasta época 6 como la mejor)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DeBERTaClassifier(num_labels=y.shape[1]).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(6):  # Solo hasta la mejor época encontrada\n",
    "    total_loss = 0\n",
    "    for batch in full_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(full_loader):.4f}\")\n",
    "\n",
    "# 6. Predicción para Kaggle\n",
    "dataTesting = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTesting.zip', index_col=0)\n",
    "dataTesting[\"input_text\"] = dataTesting[\"title\"] + \" (\" + dataTesting[\"year\"].astype(str) + \"): \" + dataTesting[\"plot\"]\n",
    "test_enc = tokenizer(list(dataTesting[\"input_text\"]), truncation=True, padding=True, max_length=384, return_tensors='pt')\n",
    "test_dataset = DataLoader(torch.utils.data.TensorDataset(test_enc['input_ids'], test_enc['attention_mask']), batch_size=32)\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for input_ids_batch, attn_mask_batch in test_dataset:\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        attn_mask_batch = attn_mask_batch.to(device)\n",
    "        pred_batch = model(input_ids_batch, attn_mask_batch).cpu().numpy()\n",
    "        preds.append(pred_batch)\n",
    "\n",
    "# 7. Guardar archivo CSV\n",
    "y_pred_test_final = np.vstack(preds)\n",
    "cols = ['p_' + genre for genre in mlb.classes_]\n",
    "res = pd.DataFrame(y_pred_test_final, index=dataTesting.index, columns=cols)\n",
    "res.to_csv('pred_genres_DeBERTaV3_Ep6.csv', index_label='ID')\n",
    "print(\"✅ Archivo final generado: pred_genres_DeBERTaV3_Ep6.csv\")\n",
    "# Podemos probarlo con menos epocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 745,
     "referenced_widgets": [
      "3abf7fc0022b4a0c9eb8847e8337a6d2",
      "26c77429d1cf4a438af4674cfe5f8fc7",
      "6cee15532276430da7b0ec11819467b8",
      "f1ce95442b024ce99de90427d3ca6f5c",
      "012b79347b7a4dfe9aa0b86a20f4ae30",
      "e7b90cb7299f4c41b5b72fdd7b61d6fa",
      "5ebb8e31ab584804aec462d40dc27269",
      "f9f85e76dcb749bab8936b0ef373765f",
      "9ef1a259a5aa4ee285fc3088f8f5edc5",
      "e8bd451046c14b329f53f4f958c7d7bf",
      "a35f046bb84b4d4db4f026d360407513",
      "f41bffbe0f7f43de97dc61b123ae2268",
      "e20085e2e1f149768084f84fab7c7935",
      "5440ad6a102b4ebebbbc2c4ca3e65c29",
      "6d01021a1da949da8617809a2f7a624f",
      "098c97ceedf5426bb9ec3dc22827c2fd",
      "5f659808e01f4dc8a1b3e9f67fd3fd7d",
      "b9785f43312841b1ba601a1f3d4d3589",
      "d58dc9337f0d40a88726657e00958983",
      "57eb03de73aa4e7d907ff6608d76d782",
      "50c22b72478a4a0aba6ec0d67853a066",
      "5f99e4762c45412f97d74e5bd2c8bf89",
      "2973e1ab1bc64d1c8a5f9fc25591f725",
      "1446e2114d544cc6b03efea6806c1e49",
      "a966ab262ca14d6eb1645d795d1abdd1",
      "03aa5eaa8dc34f21a31d5d79a447d273",
      "94314c14067548f6b087539d4c5216d2",
      "48812eb1ebfc4e618cdfb5eac0f84dcd",
      "15d8315fdec04dd295a72471cff49767",
      "f12c9e3f99dc4eb59ad4d5302d31aa8f",
      "5ee3ba4256bc489490434580ec65cacd",
      "4c5e7c0c3c4b4d00a152086fac5cb8be",
      "3bc305bed67145ac990ed4770ae7b0da",
      "ca19f316bfba4256bfea82b75501d945",
      "f835aaef3af74de7bfbdabf7b7a6a6a5",
      "5a25adeac9cf45a89272f6c2be2cb337",
      "1642bd5cc13b41c2a6e6aa3536a60213",
      "4f7a00a12c1e423aaa096deb670d3dd2",
      "981e581ddbd2471d937db54cf3509702",
      "adcaf62b90cd4707b1c3b2c3afece3fe",
      "03a2b6cd123a47609ef338836ff72747",
      "5b0bf5e4857c4909b88f9281c6989f67",
      "19985d39dee445698b11d100ac1dc340",
      "ff884ec96cf240f4a5119a2223616e9f",
      "e079113001c74801a6b3ab78f0bd41ec",
      "21e2d6223f3c4795b830d2543fd05dc3",
      "240b4502c56e4df59f16c57e4cbbe4a0",
      "3d7bb9917cef4d398fc59e239f70f994",
      "ea5c72cd43144cbb8084811785e9f456",
      "4aeaed53b54f4cce8c554983d4f3d1ac",
      "31a5f690f54e4f549938a1b52ec87bcc",
      "e5dfebf7561b4c8b8777bec9647a931e",
      "bbed91f0434e48b3b2cd843472a72029",
      "7ae47b3a671e415ca3397cd0d9fd2ddf",
      "e898a42ddeb34b68a20064cb088dce7f"
     ]
    },
    "id": "huQBAEzZeT87",
    "outputId": "222a4339-030a-44e9-f3c5-c3a6fc600fbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m102.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3abf7fc0022b4a0c9eb8847e8337a6d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f41bffbe0f7f43de97dc61b123ae2268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/580 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2973e1ab1bc64d1c8a5f9fc25591f725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca19f316bfba4256bfea82b75501d945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e079113001c74801a6b3ab78f0bd41ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.2192\n",
      "Epoch 2 completado. Loss promedio: 0.1540\n",
      "Epoch 3 completado. Loss promedio: 0.1210\n",
      "Epoch 4 completado. Loss promedio: 0.0964\n",
      "Epoch 5 completado. Loss promedio: 0.0757\n",
      "✅ Archivo final generado: pred_genres_DeBERTaV3_Ep5.csv\n"
     ]
    }
   ],
   "source": [
    "# 🛠️ Instalar dependencias necesarias\n",
    "!pip install transformers scikit-learn torch pandas --quiet\n",
    "\n",
    "# 📚 Importar librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "# 1. Cargar y preparar datos\n",
    "df = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', index_col=0)\n",
    "df['genres'] = df['genres'].apply(ast.literal_eval)\n",
    "df[\"input_text\"] = df[\"title\"] + \" (\" + df[\"year\"].astype(str) + \"): \" + df[\"plot\"]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['genres'])\n",
    "\n",
    "# 2. Tokenizador y Dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=384, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 3. Dataset completo para entrenamiento\n",
    "full_dataset = MovieDataset(df['input_text'], y)\n",
    "full_loader = DataLoader(full_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# 4. Modelo DeBERTa v3 Large\n",
    "class DeBERTaClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        x = self.dropout(cls_output)\n",
    "        return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "# 5. Entrenamiento (usando solo hasta época 6 como la mejor)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DeBERTaClassifier(num_labels=y.shape[1]).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(5):  # Solo hasta la mejor época encontrada\n",
    "    total_loss = 0\n",
    "    for batch in full_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(full_loader):.4f}\")\n",
    "\n",
    "# 6. Predicción para Kaggle\n",
    "dataTesting = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTesting.zip', index_col=0)\n",
    "dataTesting[\"input_text\"] = dataTesting[\"title\"] + \" (\" + dataTesting[\"year\"].astype(str) + \"): \" + dataTesting[\"plot\"]\n",
    "test_enc = tokenizer(list(dataTesting[\"input_text\"]), truncation=True, padding=True, max_length=384, return_tensors='pt')\n",
    "test_dataset = DataLoader(torch.utils.data.TensorDataset(test_enc['input_ids'], test_enc['attention_mask']), batch_size=32)\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for input_ids_batch, attn_mask_batch in test_dataset:\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        attn_mask_batch = attn_mask_batch.to(device)\n",
    "        pred_batch = model(input_ids_batch, attn_mask_batch).cpu().numpy()\n",
    "        preds.append(pred_batch)\n",
    "\n",
    "# 7. Guardar archivo CSV\n",
    "y_pred_test_final = np.vstack(preds)\n",
    "cols = ['p_' + genre for genre in mlb.classes_]\n",
    "res = pd.DataFrame(y_pred_test_final, index=dataTesting.index, columns=cols)\n",
    "res.to_csv('pred_genres_DeBERTaV3_Ep6.csv', index_label='ID')\n",
    "print(\"✅ Archivo final generado: pred_genres_DeBERTaV3_Ep5.csv\")\n",
    "# Podemos probarlo con menos epocas 0.93820"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 727,
     "referenced_widgets": [
      "784154db1d464acea6f4565989283b04",
      "be43408e0b5a418e8b0ba8f0b0515b22",
      "d9f0c9198f9f4020b86aaf256de47c97",
      "bb3b671501834139a3b3de2e720c1f7b",
      "ec1b28a4d2dd4689a1d0b16eb3c3bf34",
      "17f1f2bb52da4d5a9b165b4dd04e9bc2",
      "b47021f2943c4f77adaa2430be18fd25",
      "26e707668cea48cca00067a57232c9b2",
      "4da70ac8520044fcbdba9b8820bc3db0",
      "b38e8164c0434ad8bd75631e9a7a1059",
      "7567e82ee80c46eba1148d5f04a58b44",
      "bd1d139ca3c348c09acf866e8a22933e",
      "e612c10d809349478bd00b7d1eab737a",
      "12af6ec5fe154c36bf6d0534b613c5dd",
      "a3eac4f95fe84c5cba758122183ef805",
      "2075a81d07fb44b7bc4392b229e3b753",
      "b122b193e65f4ecca6aaefe1042c3e1b",
      "446da2bac82a4f6fb9bb66a0da39939e",
      "a2eeebd2318449c09fc4c374b9fcbd6c",
      "c8f969f9b82641c4b8f21376c7452825",
      "0c02337589ac4097a59e90dbe378d3e5",
      "e9f62f5132794d23a6652383e7791509",
      "807389a8ab54425ba4e6c94989d54da6",
      "23bc70ffef1a410cad058aa1316a0bdc",
      "e979642cf8eb4fa7866a66a219b48661",
      "69c2884fa7ba44b983add9d6faa205ab",
      "2cb4f01154834119bd30ef304d81eab4",
      "ef5930f876e34d8a9c888c36d7552441",
      "0022e5d209e84859b38836bcd7d8e31d",
      "10b88a777ed24dd7a181c4034a5f592b",
      "96dc616199e5479c87ccfd356b0054bd",
      "43d4a0fc69e548f9b03f709ce8b9c7cd",
      "7858cd86bab94352b9acdbefc7e9dba3",
      "6197580e41c347ee9bbf66fa93ece059",
      "2892f862cd64495586052530c500cdb1",
      "cc46e24c159b476b9da9f4cca342bae1",
      "20786e6f4a9c49c8997cbe126f61c887",
      "9298b501d5a4411b9bfe6a32f7079179",
      "0bb8e1265c894ecc842c95df514cb085",
      "2ecbc7083c294555ac24805db3d35706",
      "41ab5b65afb9496e80d10f14ad256d6a",
      "a56d791867864ce0a4356953763a5075",
      "5fff59859a6b481aa82f495ffc0e6c09",
      "b9a3c7c9d5824e9a9c1b51b456851488",
      "34b00d7bae6143f890718059457c2bcf",
      "25aeab50426d42cb88eff00755695455",
      "f2f33b1dd7a84af88a2ab1445968187c",
      "72500b06bb484bacb5f45836f30c1b13",
      "181349c6a8d74b878d290765b009c019",
      "f1956db690cd46e094281155a45d2518",
      "6f0eb74631494fdea8d52b1e3389a29b",
      "37710b9ca44d467bb71ad535404c0ea3",
      "bddbd38d1cf043888823db5dfffb6a03",
      "79760241dbb7451b9790bbaceac6311b",
      "2a579ced3a5a4239821252663b044778"
     ]
    },
    "id": "sJWDiDCIoazk",
    "outputId": "3f90b936-ae4c-4a21-cfaa-6d79f7cfbb97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m126.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m102.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "784154db1d464acea6f4565989283b04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd1d139ca3c348c09acf866e8a22933e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/580 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "807389a8ab54425ba4e6c94989d54da6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6197580e41c347ee9bbf66fa93ece059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34b00d7bae6143f890718059457c2bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.2189\n",
      "Epoch 2 completado. Loss promedio: 0.1552\n",
      "Epoch 3 completado. Loss promedio: 0.1234\n",
      "Epoch 4 completado. Loss promedio: 0.0994\n",
      "✅ Archivo final generado: pred_genres_DeBERTaV3_Ep4.csv\n"
     ]
    }
   ],
   "source": [
    "# 🛠️ Instalar dependencias necesarias\n",
    "!pip install transformers scikit-learn torch pandas --quiet\n",
    "\n",
    "# 📚 Importar librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "# 1. Cargar y preparar datos\n",
    "df = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', index_col=0)\n",
    "df['genres'] = df['genres'].apply(ast.literal_eval)\n",
    "df[\"input_text\"] = df[\"title\"] + \" (\" + df[\"year\"].astype(str) + \"): \" + df[\"plot\"]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['genres'])\n",
    "\n",
    "# 2. Tokenizador y Dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=384, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 3. Dataset completo para entrenamiento\n",
    "full_dataset = MovieDataset(df['input_text'], y)\n",
    "full_loader = DataLoader(full_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# 4. Modelo DeBERTa v3 Large\n",
    "class DeBERTaClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        x = self.dropout(cls_output)\n",
    "        return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "# 5. Entrenamiento (usando solo hasta época 6 como la mejor)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DeBERTaClassifier(num_labels=y.shape[1]).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(4):  # Solo hasta la mejor época encontrada\n",
    "    total_loss = 0\n",
    "    for batch in full_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(full_loader):.4f}\")\n",
    "\n",
    "# 6. Predicción para Kaggle\n",
    "dataTesting = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTesting.zip', index_col=0)\n",
    "dataTesting[\"input_text\"] = dataTesting[\"title\"] + \" (\" + dataTesting[\"year\"].astype(str) + \"): \" + dataTesting[\"plot\"]\n",
    "test_enc = tokenizer(list(dataTesting[\"input_text\"]), truncation=True, padding=True, max_length=384, return_tensors='pt')\n",
    "test_dataset = DataLoader(torch.utils.data.TensorDataset(test_enc['input_ids'], test_enc['attention_mask']), batch_size=32)\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for input_ids_batch, attn_mask_batch in test_dataset:\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        attn_mask_batch = attn_mask_batch.to(device)\n",
    "        pred_batch = model(input_ids_batch, attn_mask_batch).cpu().numpy()\n",
    "        preds.append(pred_batch)\n",
    "\n",
    "# 7. Guardar archivo CSV\n",
    "y_pred_test_final = np.vstack(preds)\n",
    "cols = ['p_' + genre for genre in mlb.classes_]\n",
    "res = pd.DataFrame(y_pred_test_final, index=dataTesting.index, columns=cols)\n",
    "res.to_csv('pred_genres_DeBERTaV3_Ep4.csv', index_label='ID')\n",
    "print(\"✅ Archivo final generado: pred_genres_DeBERTaV3_Ep4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 745,
     "referenced_widgets": [
      "e643fc86087144ce825688141d862248",
      "e920e1e37d9e48caa6225af69cfde9ae",
      "cb1a4bbcee614f9b815c6b42e2ad9f1e",
      "bd2245c87ca44b549368e13fc6fef1c0",
      "b847f6175ac940d1af2e99c4ffe7b552",
      "0e4a722ec97e4827addb63f56f5c23ba",
      "69783cb8c98549eca7229775e75cf085",
      "aeaf13492d984c7c83325259729d9f5c",
      "d6e035648fd74589a6f435b0d3f1bed6",
      "6d3ef4e0ce5346c1bc567f2dabd031ae",
      "91334a1d1e014e2594d611a9123aa390",
      "29f675752efe4162a5bd9f426842f1fc",
      "6152154f0677444f9e3f14c354b5d32e",
      "e5825941a1254117b914590d4d191d84",
      "3d9738d49b714b5dbea6c54f1f1e90ae",
      "0659f14456bd472eb72f0717166b2e72",
      "470cb9fbb1614f63b317903768dc9fc0",
      "8f105f8439484cafb7b8f65df5adae3f",
      "ea43053dc28346b3b919c454d504de26",
      "744a71762930450b9c7cf4e964fb0a55",
      "2ad172ba0e4243f2a4237ff959927d16",
      "8f81347a59e24df4a2c8d8b5586eb3f9",
      "c4e611ad57584c56afcb0d93e9c6ac35",
      "31591440f431472d9dec1d1d92db76b7",
      "d295f8a7848f4ef2b52d2c1c158ed29d",
      "7bfa539c76614e1ba7fb61428829d3e8",
      "2ad1b37d5c1141e6ab108d760a399618",
      "ad58b7da871e4e4b81751d9f151fa0aa",
      "b5e7650dce02483ab80b29a99162acc9",
      "3e62e6fca6e64eb2828b230a7e755e69",
      "6421e79aef4a4a58ac1dee5484680f45",
      "2d090f9d02d04595981959692171cbd3",
      "6a542d6db90c4640baff78cb2b3033ff",
      "d6dc9a976a79453eb7191fe64b6b0e43",
      "a7f145582567425a9b69a9c5418edd39",
      "043ddca00a6b4da8b8287a9d7f664acf",
      "cabc9278785f463d9754ed0af2b63543",
      "e29a87841b174ed292bb015e0ae5ce33",
      "dee1aeb2697a434d99c18ea87cc80b4f",
      "0f776eb437c540ddbf5e08aa955f2e4b",
      "565e052c12cd4ba0a085acfb97d4cee8",
      "6c91e5943bda48adb8ab4a11a77c71d1",
      "44f6bfb16e054c36a10e42d9d95255f2",
      "16c05425fafd43b2bcb0f446fb110e58",
      "5732c3cdc18c43e683bddbc2782baf54",
      "19ee24aa7da64afca35a4ae34bd21df1",
      "60b5cdd5242144aa89fc36d9f7e1e74f",
      "4bdb3556d8ed48edaf98387da1428618",
      "6a3d49d3b71844d8939410765cd44ac5",
      "355d2512f78e4df6932b61ac823e56cf",
      "338c31bc1f6c42b4ae2732c2c1cde8bb",
      "3838b55ec5fc4d9890c08ac8ef24b523",
      "b97886f454474c7fa80619f2017928c9",
      "b6b474d895a14380968de7edae14d7cb",
      "8bd2cacd135f45549ddb8f547a267156"
     ]
    },
    "id": "071dM8zg-MkW",
    "outputId": "8a86239d-ce8c-4f71-f53a-d22ac2a9aa0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e643fc86087144ce825688141d862248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f675752efe4162a5bd9f426842f1fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/580 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e611ad57584c56afcb0d93e9c6ac35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6dc9a976a79453eb7191fe64b6b0e43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5732c3cdc18c43e683bddbc2782baf54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.2223\n",
      "Epoch 2 completado. Loss promedio: 0.1541\n",
      "Epoch 3 completado. Loss promedio: 0.1224\n",
      "Epoch 4 completado. Loss promedio: 0.0982\n",
      "Epoch 5 completado. Loss promedio: 0.0777\n",
      "✅ Archivo final generado: pred_genres_DeBERTaV3_Ep5.csv\n"
     ]
    }
   ],
   "source": [
    "# 🛠️ Instalar dependencias necesarias\n",
    "!pip install transformers scikit-learn torch pandas --quiet\n",
    "\n",
    "# 📚 Importar librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "# 1. Cargar y preparar datos\n",
    "df = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', index_col=0)\n",
    "df['genres'] = df['genres'].apply(ast.literal_eval)\n",
    "df[\"input_text\"] = df[\"title\"] + \" (\" + df[\"year\"].astype(str) + \"): \" + df[\"plot\"]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['genres'])\n",
    "\n",
    "# 2. Tokenizador y Dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=384, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 3. Dataset completo para entrenamiento\n",
    "full_dataset = MovieDataset(df['input_text'], y)\n",
    "full_loader = DataLoader(full_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# 4. Modelo DeBERTa v3 Large\n",
    "class DeBERTaClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        x = self.dropout(cls_output)\n",
    "        return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "# 5. Entrenamiento (usando solo hasta época 6 como la mejor)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DeBERTaClassifier(num_labels=y.shape[1]).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(5):  # Solo hasta la mejor época encontrada\n",
    "    total_loss = 0\n",
    "    for batch in full_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(full_loader):.4f}\")\n",
    "\n",
    "# 6. Predicción para Kaggle\n",
    "dataTesting = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTesting.zip', index_col=0)\n",
    "dataTesting[\"input_text\"] = dataTesting[\"title\"] + \" (\" + dataTesting[\"year\"].astype(str) + \"): \" + dataTesting[\"plot\"]\n",
    "test_enc = tokenizer(list(dataTesting[\"input_text\"]), truncation=True, padding=True, max_length=384, return_tensors='pt')\n",
    "test_dataset = DataLoader(torch.utils.data.TensorDataset(test_enc['input_ids'], test_enc['attention_mask']), batch_size=32)\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for input_ids_batch, attn_mask_batch in test_dataset:\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        attn_mask_batch = attn_mask_batch.to(device)\n",
    "        pred_batch = model(input_ids_batch, attn_mask_batch).cpu().numpy()\n",
    "        preds.append(pred_batch)\n",
    "\n",
    "# 7. Guardar archivo CSV\n",
    "y_pred_test_final = np.vstack(preds)\n",
    "cols = ['p_' + genre for genre in mlb.classes_]\n",
    "res = pd.DataFrame(y_pred_test_final, index=dataTesting.index, columns=cols)\n",
    "res.to_csv('pred_genres_DeBERTaV3_Ep5.csv', index_label='ID')\n",
    "print(\"✅ Archivo final generado: pred_genres_DeBERTaV3_Ep5.csv\")\n",
    "# 5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PfgsSXXAVsRY",
    "outputId": "9346e771-f6bb-47ae-bf11-6c5945850c7e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.2195\n",
      "Epoch 2 completado. Loss promedio: 0.1546\n",
      "Epoch 3 completado. Loss promedio: 0.1233\n",
      "Epoch 4 completado. Loss promedio: 0.0992\n",
      "Epoch 5 completado. Loss promedio: 0.0793\n",
      "Epoch 6 completado. Loss promedio: 0.0619\n",
      "✅ Archivo final generado: pred_genres_DeBERTaV3_Ep6.csv\n"
     ]
    }
   ],
   "source": [
    "# 🛠️ Instalar dependencias necesarias\n",
    "!pip install transformers scikit-learn torch pandas --quiet\n",
    "\n",
    "# 📚 Importar librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "# 1. Cargar y preparar datos\n",
    "df = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', index_col=0)\n",
    "df['genres'] = df['genres'].apply(ast.literal_eval)\n",
    "df[\"input_text\"] = df[\"title\"] + \" (\" + df[\"year\"].astype(str) + \"): \" + df[\"plot\"]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['genres'])\n",
    "\n",
    "# 2. Tokenizador y Dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=384, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 3. Dataset completo para entrenamiento\n",
    "full_dataset = MovieDataset(df['input_text'], y)\n",
    "full_loader = DataLoader(full_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# 4. Modelo DeBERTa v3 Large\n",
    "class DeBERTaClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        x = self.dropout(cls_output)\n",
    "        return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "# 5. Entrenamiento (usando solo hasta época 6 como la mejor)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DeBERTaClassifier(num_labels=y.shape[1]).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(6):  # Solo hasta la mejor época encontrada\n",
    "    total_loss = 0\n",
    "    for batch in full_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(full_loader):.4f}\")\n",
    "\n",
    "# 6. Predicción para Kaggle\n",
    "dataTesting = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTesting.zip', index_col=0)\n",
    "dataTesting[\"input_text\"] = dataTesting[\"title\"] + \" (\" + dataTesting[\"year\"].astype(str) + \"): \" + dataTesting[\"plot\"]\n",
    "test_enc = tokenizer(list(dataTesting[\"input_text\"]), truncation=True, padding=True, max_length=384, return_tensors='pt')\n",
    "test_dataset = DataLoader(torch.utils.data.TensorDataset(test_enc['input_ids'], test_enc['attention_mask']), batch_size=32)\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for input_ids_batch, attn_mask_batch in test_dataset:\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        attn_mask_batch = attn_mask_batch.to(device)\n",
    "        pred_batch = model(input_ids_batch, attn_mask_batch).cpu().numpy()\n",
    "        preds.append(pred_batch)\n",
    "\n",
    "# 7. Guardar archivo CSV\n",
    "y_pred_test_final = np.vstack(preds)\n",
    "cols = ['p_' + genre for genre in mlb.classes_]\n",
    "res = pd.DataFrame(y_pred_test_final, index=dataTesting.index, columns=cols)\n",
    "res.to_csv('pred_genres_DeBERTaV3_Ep6.csv', index_label='ID')\n",
    "print(\"✅ Archivo final generado: pred_genres_DeBERTaV3_Ep6.csv\")\n",
    "# 6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-69i-0FcU1c2",
    "outputId": "7d73be90-312e-4f1b-e6cd-8076aba4cd71"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.2193\n",
      "Epoch 2 completado. Loss promedio: 0.1541\n",
      "Epoch 3 completado. Loss promedio: 0.1222\n",
      "Epoch 4 completado. Loss promedio: 0.0985\n",
      "Epoch 5 completado. Loss promedio: 0.0772\n",
      "Epoch 6 completado. Loss promedio: 0.0604\n",
      "✅ Archivo final generado: pred_genres_DeBERTaV3_Ep6.csv\n"
     ]
    }
   ],
   "source": [
    "# 🛠️ Instalar dependencias necesarias\n",
    "!pip install transformers scikit-learn torch pandas --quiet\n",
    "\n",
    "# 📚 Importar librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "# 1. Cargar y preparar datos\n",
    "df = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', index_col=0)\n",
    "df['genres'] = df['genres'].apply(ast.literal_eval)\n",
    "df[\"input_text\"] = df[\"title\"] + \" (\" + df[\"year\"].astype(str) + \"): \" + df[\"plot\"]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['genres'])\n",
    "\n",
    "# 2. Tokenizador y Dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=384, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 3. Dataset completo para entrenamiento\n",
    "full_dataset = MovieDataset(df['input_text'], y)\n",
    "full_loader = DataLoader(full_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# 4. Modelo DeBERTa v3 Large\n",
    "class DeBERTaClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        x = self.dropout(cls_output)\n",
    "        return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "# 5. Entrenamiento (usando solo hasta época 6 como la mejor)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DeBERTaClassifier(num_labels=y.shape[1]).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(6):  # Solo hasta la mejor época encontrada\n",
    "    total_loss = 0\n",
    "    for batch in full_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(full_loader):.4f}\")\n",
    "\n",
    "# 6. Predicción para Kaggle\n",
    "dataTesting = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTesting.zip', index_col=0)\n",
    "dataTesting[\"input_text\"] = dataTesting[\"title\"] + \" (\" + dataTesting[\"year\"].astype(str) + \"): \" + dataTesting[\"plot\"]\n",
    "test_enc = tokenizer(list(dataTesting[\"input_text\"]), truncation=True, padding=True, max_length=384, return_tensors='pt')\n",
    "test_dataset = DataLoader(torch.utils.data.TensorDataset(test_enc['input_ids'], test_enc['attention_mask']), batch_size=32)\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for input_ids_batch, attn_mask_batch in test_dataset:\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        attn_mask_batch = attn_mask_batch.to(device)\n",
    "        pred_batch = model(input_ids_batch, attn_mask_batch).cpu().numpy()\n",
    "        preds.append(pred_batch)\n",
    "\n",
    "# 7. Guardar archivo CSV\n",
    "y_pred_test_final = np.vstack(preds)\n",
    "cols = ['p_' + genre for genre in mlb.classes_]\n",
    "res = pd.DataFrame(y_pred_test_final, index=dataTesting.index, columns=cols)\n",
    "res.to_csv('pred_genres_DeBERTaV3_Ep6.csv', index_label='ID')\n",
    "print(\"✅ Archivo final generado: pred_genres_DeBERTaV3_Ep6.csv\")\n",
    "# 6.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0hhAheNY1azP",
    "outputId": "ba67c6e3-cde8-4d71-eac6-abf4a3fd1161"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.2218\n",
      "Epoch 2 completado. Loss promedio: 0.1564\n",
      "Epoch 3 completado. Loss promedio: 0.1239\n",
      "Epoch 4 completado. Loss promedio: 0.0997\n",
      "Epoch 5 completado. Loss promedio: 0.0793\n",
      "✅ Archivo final generado: pred_genres_DeBERTaV3_Ep5.csv\n"
     ]
    }
   ],
   "source": [
    "# 🛠️ Instalar dependencias necesarias\n",
    "!pip install transformers scikit-learn torch pandas --quiet\n",
    "\n",
    "# 📚 Importar librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "# 1. Cargar y preparar datos\n",
    "df = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', index_col=0)\n",
    "df['genres'] = df['genres'].apply(ast.literal_eval)\n",
    "df[\"input_text\"] = df[\"title\"] + \" (\" + df[\"year\"].astype(str) + \"): \" + df[\"plot\"]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['genres'])\n",
    "\n",
    "# 2. Tokenizador y Dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=384, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 3. Dataset completo para entrenamiento\n",
    "full_dataset = MovieDataset(df['input_text'], y)\n",
    "full_loader = DataLoader(full_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# 4. Modelo DeBERTa v3 Large\n",
    "class DeBERTaClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        x = self.dropout(cls_output)\n",
    "        return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "# 5. Entrenamiento (usando solo hasta época 6 como la mejor)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DeBERTaClassifier(num_labels=y.shape[1]).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(5):  # Solo hasta la mejor época encontrada\n",
    "    total_loss = 0\n",
    "    for batch in full_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(full_loader):.4f}\")\n",
    "\n",
    "# 6. Predicción para Kaggle\n",
    "dataTesting = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTesting.zip', index_col=0)\n",
    "dataTesting[\"input_text\"] = dataTesting[\"title\"] + \" (\" + dataTesting[\"year\"].astype(str) + \"): \" + dataTesting[\"plot\"]\n",
    "test_enc = tokenizer(list(dataTesting[\"input_text\"]), truncation=True, padding=True, max_length=384, return_tensors='pt')\n",
    "test_dataset = DataLoader(torch.utils.data.TensorDataset(test_enc['input_ids'], test_enc['attention_mask']), batch_size=32)\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for input_ids_batch, attn_mask_batch in test_dataset:\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        attn_mask_batch = attn_mask_batch.to(device)\n",
    "        pred_batch = model(input_ids_batch, attn_mask_batch).cpu().numpy()\n",
    "        preds.append(pred_batch)\n",
    "\n",
    "# 7. Guardar archivo CSV\n",
    "y_pred_test_final = np.vstack(preds)\n",
    "cols = ['p_' + genre for genre in mlb.classes_]\n",
    "res = pd.DataFrame(y_pred_test_final, index=dataTesting.index, columns=cols)\n",
    "res.to_csv('pred_genres_DeBERTaV3_Ep5.csv', index_label='ID')\n",
    "print(\"✅ Archivo final generado: pred_genres_DeBERTaV3_Ep5.csv\")\n",
    "# 5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 635,
     "referenced_widgets": [
      "d079d05af856498abfc0daaaed8e7fba",
      "5e6e78433a7447bab4f807068c5007fb",
      "4ac9acc226244183a85befd2143b3cfd",
      "20ab2e9c609a46cf80262d58cefa4736",
      "7254430b19404850ad251804b76c75b3",
      "f442e9bd7f35486dab6f00e94084dac8",
      "75643430c73144348d5bb2947adcf082",
      "a6ee9814272c42f2a554362481d18401",
      "1ea34e01f4174804b1144aa00f1c7451",
      "3c5d495d42e749839171bb2583c2b0cf",
      "d9e80cc67f8b43b5bd90bead5a47b818",
      "9f25a04cf55940a585614478a207ca80",
      "09ebf9ee0569416e8be6fb22f93ca0a6",
      "c8df5131ccc9464bbe13e964bc47670e",
      "4c1647a67d6d453ca8ef9342381be098",
      "5494311c82f547b7b832b80aae9dda5d",
      "23c7506ce936489f9af98ba8354860b1",
      "02561b68d14748e483c8acf08a54ba88",
      "fcc2d14767164f2583900faf227d624f",
      "4cffa295a73142edb6e5ff194538abff",
      "3400a32dc5d0476a83bf18f7f3339a65",
      "a7d84f7f07014150a9b49e0f558a6123",
      "0ffb70c2332141c882024968b2f3531d",
      "2b464186d72343c99c46fbe4e81a27e7",
      "2bbebdff2d534f599a3f666e97776fe4",
      "c9ec7fa0fcb74dceb4a3b8a9b2095931",
      "064ae1ebee8f4b8692a1a8c3e2078474",
      "971c474e23594d2688f7d28e26cc1be6",
      "693e50ad49624fe2b68bd8f87d2300f5",
      "6c563eb3401d4258be8ac6ee3cfff4a1",
      "f963cfb43af849f09edebc7c7459a6e4",
      "387846c8c023428ea2bce27c38891539",
      "ea699a1c88c24ee3bfc0efce7564b72d",
      "e487e84e0578493f841c50e58b459c36",
      "056dd255937f43fd869089886be93fea",
      "7a844924f8904a3cb1573a95072a9bfb",
      "23fec8bc30ea4ad48fa43e4b0c6a785b",
      "49d3150b138d46d2bfb49f9f4871813d",
      "bfbfa9b3d4a64641ac9c20f8f6fe2528",
      "9094066c05b64f979b88981ed2dcffa2",
      "8dbd175ab79245469e8eda3b0dc65127",
      "9e1ad69e3efb440fb4a42633ddd7b102",
      "4bef341658224b79bc2ee7785a3140b9",
      "393a2123fda94bb9821a2245ea357562",
      "00103d1c832c48e592a189cbd80c2212",
      "24686bc1aec9455d8be9c07a11d87795",
      "767c1109868d41af817ba7fa293851f8",
      "9005955ef805446a9ae93b1376c73f71",
      "7b9be6f35e104b67aa055a29b423d794",
      "76f7100cc5f64073ae63530f41874e65",
      "ddb4cbfd11ff4185a257fb11040121f2",
      "d07d1441440340ffb2edd648b5bfbebc",
      "1e5e869411df41e7bedde4bded5fee3f",
      "450c6ae6943a41b9bc55cb1a53653ffa",
      "c9a13d8377cd47a4a596acf132d7b55b"
     ]
    },
    "id": "iIvCzQg9FWsU",
    "outputId": "3453abc7-3798-41c3-eb42-871ee1421e07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m113.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d079d05af856498abfc0daaaed8e7fba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f25a04cf55940a585614478a207ca80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/580 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ffb70c2332141c882024968b2f3531d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e487e84e0578493f841c50e58b459c36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00103d1c832c48e592a189cbd80c2212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.2225\n",
      "Epoch 2 completado. Loss promedio: 0.1552\n",
      "Epoch 3 completado. Loss promedio: 0.1237\n",
      "Epoch 4 completado. Loss promedio: 0.0987\n",
      "Epoch 5 completado. Loss promedio: 0.0774\n",
      "✅ Archivo final generado: pred_genres_DeBERTaV3_Ep5.csv\n"
     ]
    }
   ],
   "source": [
    "# 🛠️ Instalar dependencias necesarias\n",
    "!pip install transformers scikit-learn torch pandas --quiet\n",
    "\n",
    "# 📚 Importar librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "# 1. Cargar y preparar datos\n",
    "df = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', index_col=0)\n",
    "df['genres'] = df['genres'].apply(ast.literal_eval)\n",
    "df[\"input_text\"] = df[\"title\"] + \" (\" + df[\"year\"].astype(str) + \"): \" + df[\"plot\"]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['genres'])\n",
    "\n",
    "# 2. Tokenizador y Dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=384, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 3. Dataset completo para entrenamiento\n",
    "full_dataset = MovieDataset(df['input_text'], y)\n",
    "full_loader = DataLoader(full_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# 4. Modelo DeBERTa v3 Large\n",
    "class DeBERTaClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        x = self.dropout(cls_output)\n",
    "        return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "# 5. Entrenamiento (usando solo hasta época 6 como la mejor)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DeBERTaClassifier(num_labels=y.shape[1]).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(5):  # Solo hasta la mejor época encontrada\n",
    "    total_loss = 0\n",
    "    for batch in full_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(full_loader):.4f}\")\n",
    "\n",
    "# 6. Predicción para Kaggle\n",
    "dataTesting = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTesting.zip', index_col=0)\n",
    "dataTesting[\"input_text\"] = dataTesting[\"title\"] + \" (\" + dataTesting[\"year\"].astype(str) + \"): \" + dataTesting[\"plot\"]\n",
    "test_enc = tokenizer(list(dataTesting[\"input_text\"]), truncation=True, padding=True, max_length=384, return_tensors='pt')\n",
    "test_dataset = DataLoader(torch.utils.data.TensorDataset(test_enc['input_ids'], test_enc['attention_mask']), batch_size=32)\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for input_ids_batch, attn_mask_batch in test_dataset:\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        attn_mask_batch = attn_mask_batch.to(device)\n",
    "        pred_batch = model(input_ids_batch, attn_mask_batch).cpu().numpy()\n",
    "        preds.append(pred_batch)\n",
    "\n",
    "# 7. Guardar archivo CSV\n",
    "y_pred_test_final = np.vstack(preds)\n",
    "cols = ['p_' + genre for genre in mlb.classes_]\n",
    "res = pd.DataFrame(y_pred_test_final, index=dataTesting.index, columns=cols)\n",
    "res.to_csv('pred_genres_DeBERTaV3_Ep5.csv', index_label='ID')\n",
    "print(\"✅ Archivo final generado: pred_genres_DeBERTaV3_Ep5.csv\")\n",
    "# 5.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 763,
     "referenced_widgets": [
      "04b9875366db4f85bdbd9a41e9bf4893",
      "1fa61fb6e52542c089a1957b40f41304",
      "999a113fa5e547b884ca3652d06d1b5e",
      "736f8b187bac45e388737146079ad6e1",
      "a7c8b7aa47a447d89257d8f235615c01",
      "1a5d8f3984bb4ac28833da8a8f2471bd",
      "68beb83c77344cb6b79d293c3d5025d0",
      "3a00bb33140e4c859ba4560fb33dd9e9",
      "437047b0231d45ce830e98055cdccaec",
      "5679d6ad84b740549763cfd7bc01eea6",
      "bd1245f67b504e1e9f839315e48a7512",
      "c6d4024c061c4eb5893bb85bd5fcf5e5",
      "d9d15c8a519447a5975e9c9271e91881",
      "2eac5127b3b344f3be85dd1875703683",
      "050c2f58a8494116bc395def7648e540",
      "5c1ea906e2844faea0c744322e018ba5",
      "1d15633ce835458390b13ac5fa18a440",
      "0c855ab5d030401a8024b3d2efc909c9",
      "b79aa18c742c4838b138f691dbcde1b3",
      "20b41a94fa2d47218b07df4aea95cc97",
      "524826c4099c44a1915578540712dd2d",
      "33ffd14911dc4a7eb984cdfee1bc2ccc",
      "0fc6a7d6a9d0491182ef1436e66dcbeb",
      "52fd731ff5bb4a5ca8a7b8a2e7ad9b07",
      "16ea7b2fb82c4aa48d71ff00cedb9e49",
      "a3e311ca0a5b4dbe85055856749afbfe",
      "a9a3af849314481bb3cf690e364c429e",
      "ff8f6c58373848c9a2a87eaa4bbceae1",
      "9c7b3bac126f43f491e29c4a32787360",
      "62a1510e44f54ea5b0f80d09ed17fa98",
      "3cbffaed4c3b4e72aba3e539dab187d9",
      "3f1f0482a96f45a2a6639b67ef6b06b8",
      "68b4d59cba194f3580b867688916c4f7",
      "c8daa25aa9534207a3bec5cc9bffc9d7",
      "f92464528af24b6a9a3d9f977ae53583",
      "6bae40a044d045a3905a46399935a20d",
      "0c8da0df6b7e4c0aada17b309745a8f9",
      "69e59a9a7d73428e858a1a331ec21639",
      "0b491796c39e485a826724fae309e7b2",
      "6d2313a2ae184a29a68e5fad312de7ce",
      "aef6188da15f46b68fa77babe1a1d320",
      "df4e33302d7941c787b6d759383ce859",
      "2c6b235ac31749928054cff25ce06e66",
      "24506c885b444123b46f3ecf0d8f41bf",
      "df238cccf0d64aeb9f481726ede1cfbe",
      "cb5975e6c1f94e668153313482000da8",
      "c2be497c9a2240209c21d58b77c0c9db",
      "01f0ae1919814313a79e1c865a2df0c5",
      "9c95d5dd94ba49d6a2c85970a96570a5",
      "3046ea5992b44435aa8ae02751767c44",
      "b22329a7c2d3448d9afedfbb68a35ac7",
      "68490e6bb16e458aba832bb45e97d105",
      "dc5b504539674348a4b23afa4a0182ad",
      "6e0fc8767cf2457f9865af141e19007d",
      "069f5a31cb4442e28d2b11dd1b9cee29"
     ]
    },
    "id": "GQSH3qQIISIR",
    "outputId": "f3395e8e-90c1-47c5-893c-ab47a62bf2fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04b9875366db4f85bdbd9a41e9bf4893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d4024c061c4eb5893bb85bd5fcf5e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/580 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fc6a7d6a9d0491182ef1436e66dcbeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8daa25aa9534207a3bec5cc9bffc9d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df238cccf0d64aeb9f481726ede1cfbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.2241\n",
      "Epoch 2 completado. Loss promedio: 0.1602\n",
      "Epoch 3 completado. Loss promedio: 0.1281\n",
      "Epoch 4 completado. Loss promedio: 0.1032\n",
      "Epoch 5 completado. Loss promedio: 0.0821\n",
      "Epoch 6 completado. Loss promedio: 0.0643\n",
      "✅ Archivo final generado: pred_genres_DeBERTaV3_Ep6.csv\n"
     ]
    }
   ],
   "source": [
    "# 🛠️ Instalar dependencias necesarias\n",
    "!pip install transformers scikit-learn torch pandas --quiet\n",
    "\n",
    "# 📚 Importar librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "# 1. Cargar y preparar datos\n",
    "df = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', index_col=0)\n",
    "df['genres'] = df['genres'].apply(ast.literal_eval)\n",
    "df[\"input_text\"] = df[\"title\"] + \" (\" + df[\"year\"].astype(str) + \"): \" + df[\"plot\"]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['genres'])\n",
    "\n",
    "# 2. Tokenizador y Dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=384, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 3. Dataset completo para entrenamiento\n",
    "full_dataset = MovieDataset(df['input_text'], y)\n",
    "full_loader = DataLoader(full_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# 4. Modelo DeBERTa v3 Large\n",
    "class DeBERTaClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        x = self.dropout(cls_output)\n",
    "        return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "# 5. Entrenamiento (usando solo hasta época 6 como la mejor)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DeBERTaClassifier(num_labels=y.shape[1]).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(6):  # Solo hasta la mejor época encontrada\n",
    "    total_loss = 0\n",
    "    for batch in full_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(full_loader):.4f}\")\n",
    "\n",
    "# 6. Predicción para Kaggle\n",
    "dataTesting = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTesting.zip', index_col=0)\n",
    "dataTesting[\"input_text\"] = dataTesting[\"title\"] + \" (\" + dataTesting[\"year\"].astype(str) + \"): \" + dataTesting[\"plot\"]\n",
    "test_enc = tokenizer(list(dataTesting[\"input_text\"]), truncation=True, padding=True, max_length=384, return_tensors='pt')\n",
    "test_dataset = DataLoader(torch.utils.data.TensorDataset(test_enc['input_ids'], test_enc['attention_mask']), batch_size=32)\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for input_ids_batch, attn_mask_batch in test_dataset:\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        attn_mask_batch = attn_mask_batch.to(device)\n",
    "        pred_batch = model(input_ids_batch, attn_mask_batch).cpu().numpy()\n",
    "        preds.append(pred_batch)\n",
    "\n",
    "# 7. Guardar archivo CSV\n",
    "y_pred_test_final = np.vstack(preds)\n",
    "cols = ['p_' + genre for genre in mlb.classes_]\n",
    "res = pd.DataFrame(y_pred_test_final, index=dataTesting.index, columns=cols)\n",
    "res.to_csv('pred_genres_DeBERTaV3_Ep5.csv', index_label='ID')\n",
    "print(\"✅ Archivo final generado: pred_genres_DeBERTaV3_Ep6.csv\")\n",
    "# 6.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Qtr13kbfPmx",
    "outputId": "da42427c-3914-49e2-d431-c7c6ebf3a664"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completado. Loss promedio: 0.2232\n",
      "Epoch 2 completado. Loss promedio: 0.1594\n",
      "Epoch 3 completado. Loss promedio: 0.1285\n",
      "Epoch 4 completado. Loss promedio: 0.1066\n",
      "Epoch 5 completado. Loss promedio: 0.0863\n",
      "✅ Archivo final generado: pred_genres_DeBERTaV3_Ep5.csv\n"
     ]
    }
   ],
   "source": [
    "# 🛠️ Instalar dependencias necesarias\n",
    "!pip install transformers scikit-learn torch pandas --quiet\n",
    "\n",
    "# 📚 Importar librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "# 1. Cargar y preparar datos\n",
    "df = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', index_col=0)\n",
    "df['genres'] = df['genres'].apply(ast.literal_eval)\n",
    "df[\"input_text\"] = df[\"title\"] + \" (\" + df[\"year\"].astype(str) + \"): \" + df[\"plot\"]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['genres'])\n",
    "\n",
    "# 2. Tokenizador y Dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=384, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 3. Dataset completo para entrenamiento\n",
    "full_dataset = MovieDataset(df['input_text'], y)\n",
    "full_loader = DataLoader(full_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# 4. Modelo DeBERTa v3 Large\n",
    "class DeBERTaClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        x = self.dropout(cls_output)\n",
    "        return torch.sigmoid(self.classifier(x))\n",
    "\n",
    "# 5. Entrenamiento (usando solo hasta época 6 como la mejor)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DeBERTaClassifier(num_labels=y.shape[1]).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(5):  # Solo hasta la mejor época encontrada\n",
    "    total_loss = 0\n",
    "    for batch in full_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} completado. Loss promedio: {total_loss / len(full_loader):.4f}\")\n",
    "\n",
    "# 6. Predicción para Kaggle\n",
    "dataTesting = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTesting.zip', index_col=0)\n",
    "dataTesting[\"input_text\"] = dataTesting[\"title\"] + \" (\" + dataTesting[\"year\"].astype(str) + \"): \" + dataTesting[\"plot\"]\n",
    "test_enc = tokenizer(list(dataTesting[\"input_text\"]), truncation=True, padding=True, max_length=384, return_tensors='pt')\n",
    "test_dataset = DataLoader(torch.utils.data.TensorDataset(test_enc['input_ids'], test_enc['attention_mask']), batch_size=32)\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for input_ids_batch, attn_mask_batch in test_dataset:\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        attn_mask_batch = attn_mask_batch.to(device)\n",
    "        pred_batch = model(input_ids_batch, attn_mask_batch).cpu().numpy()\n",
    "        preds.append(pred_batch)\n",
    "\n",
    "# 7. Guardar archivo CSV\n",
    "y_pred_test_final = np.vstack(preds)\n",
    "cols = ['p_' + genre for genre in mlb.classes_]\n",
    "res = pd.DataFrame(y_pred_test_final, index=dataTesting.index, columns=cols)\n",
    "res.to_csv('pred_genres_DeBERTaV3_Ep5.csv', index_label='ID')\n",
    "print(\"✅ Archivo final generado: pred_genres_DeBERTaV3_Ep5.csv\")\n",
    "# 5.19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dWkUTAcp3Mss"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "updated": true
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
